{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# SVHN Projektarbeit",
   "id": "cf13ccb0f5657075"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1. Aufgabe (Task to solve)\n",
    "\n",
    "In diesem Projekt soll ein Modell darauf trainiert werden, **Ziffern (0–9)** auf realen Straßenaufnahmen zu erkennen.\n",
    "Es handelt sich um ein **multiklassiges Bildklassifikationsproblem**.\n",
    "Das Modell soll jedem Eingabebild die wahrscheinlichste Klasse $0,1,\\dots,9$ zuordnen.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Datensatz (Data used)\n",
    "\n",
    "Für das Projekt wird der **Street View House Numbers (SVHN)** Datensatz verwendet.\n",
    "\n",
    "### 2.1 Datentyp\n",
    "- Farbige **RGB-Bilder**\n",
    "- Auflösung: **32 × 32 Pixel**\n",
    "- Realweltaufnahmen von Hausnummern\n",
    "- Jede Ziffer liegt als einzelnes Bild vor\n",
    "\n",
    "### 2.2 Größe des Datensatzes\n",
    "- **Training Set:** 73.257 Bilder\n",
    "- **Test Set:** 26.032 Bilder\n",
    "- **Additional Set:** 531.131 Bilder (optional)\n",
    "\n",
    "### 2.3 Labels\n",
    "- Labelbereich: **1–10**, wobei **10 die Ziffer 0** repräsentiert\n",
    "- Labels werden vor Training entsprechend korrigiert\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Modellarchitektur (Model architecture)\n",
    "\n",
    "Es wird ein **Convolutional Neural Network (CNN)** verwendet, da CNNs besonders geeignet sind für Bilddaten aufgrund lokaler Mustererkennung.\n",
    "\n",
    "Die geplante Architektur:\n",
    "\n",
    "1. **Convolution Layer** (5×5, 3 → 10 Kanäle)\n",
    "2. **Max-Pooling Layer** (2×2)\n",
    "3. **ReLU-Aktivierung**\n",
    "4. **Convolution Layer** (5×5, 10 → 20 Kanäle)\n",
    "5. **Dropout2D**\n",
    "6. **Max-Pooling Layer** (2×2)\n",
    "7. **ReLU-Aktivierung**\n",
    "8. **Flatten Layer**\n",
    "9. **Fully Connected Layer** (auf 50 Neuronen)\n",
    "10. **ReLU**\n",
    "11. **Fully Connected Layer** (auf 10 Klassen)\n",
    "12. **Log-Softmax-Ausgabe**\n",
    "\n",
    "Dieses CNN extrahiert zuerst lokale Merkmale (Kanten, Formen), reduziert die räumlichen Dimensionen mittels Pooling und klassifiziert anschließend die resultierenden Feature Maps.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Basiseinstellungen für Training und Test (Basic training/testing setup)\n",
    "\n",
    "### 4.1 Preprocessing\n",
    "- Normalisierung der Pixelwerte auf Bereich \\([0,1]\\)\n",
    "- Normalisierung jedes RGB-Kanals: Mittelwert 0.5, Standardabweichung 0.5\n",
    "\n",
    "### 4.2 Hyperparameter\n",
    "| Parameter | Wert |\n",
    "|----------|------|\n",
    "| Optimizer | SGD |\n",
    "| Learning Rate | 0.01 |\n",
    "| Momentum | 0.5 |\n",
    "| Batch Size (Train) | 64 |\n",
    "| Batch Size (Test) | 1000 |\n",
    "| Epochen | 3 (Baseline) |\n",
    "| Weight Initialization | Kaiming/He |\n",
    "\n",
    "### 4.3 Loss Function\n",
    "- **Negative Log Likelihood Loss (NLLLoss)**\n",
    "- Wird mit **LogSoftmax** kombiniert\n",
    "\n",
    "### 4.4 Trainingspipeline\n",
    "1. Laden der Trainingsbatches\n",
    "2. Forward Pass durch das CNN\n",
    "3. Berechnung des NLL-Loss\n",
    "4. Backpropagation\n",
    "5. Update der Gewichte mit SGD\n",
    "6. Wiederholung über mehrere Epochen\n",
    "\n",
    "### 4.5 Evaluation\n",
    "- Test-Loss\n",
    "- Test-Accuracy\n",
    "- Ausgabe der Modellvorhersagen auf Beispielbildern\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Ziel des Projektes\n",
    "\n",
    "- Aufbau eines funktionierenden CNNs für das SVHN-Dataset\n",
    "- Durchführung **vier unterschiedlicher Experimente** (Hyperparameter- und Architekturvergleiche)\n",
    "- Analyse und Visualisierung der Trainingsergebnisse\n",
    "- Erkennen des Einflusses verschiedener Einstellungen wie:\n",
    "  - Learning Rate\n",
    "  - Optimizer\n",
    "  - Netzwerkarchitektur\n",
    "  - Batch Size\n",
    "  - Weight Initialization\n",
    "\n",
    "Ziel ist es, ein besseres Verständnis für das Verhalten neuronaler Netze und deren Hyperparameter in realen Bildklassifikationsproblemen zu entwickeln.\n"
   ],
   "id": "7368b1615a0f95c1"
  },
  {
   "cell_type": "markdown",
   "id": "234fcf1afc74a162",
   "metadata": {},
   "source": [
    "# Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40074f299749155",
   "metadata": {},
   "source": [
    "## wichtige Pakete einlesen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f112f3bba625c2f5",
   "metadata": {},
   "source": "gegebenenfalls müssen die Pakete (v.a. im Jupyter Hub) zunächst installiert werden, bevor sie importiert werden können"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T12:49:11.642718Z",
     "start_time": "2025-11-28T12:49:10.018475Z"
    }
   },
   "cell_type": "code",
   "source": [
    "!pip install scipy\n",
    "!pip install torchvision\n",
    "!pip install torch\n",
    "!pip install matplotlib\n",
    "!pip install scikit-learn\n",
    "!pip install torchsummary"
   ],
   "id": "549a97c30782ccf5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scipy in ./.venv/lib/python3.9/site-packages (1.13.1)\r\n",
      "Requirement already satisfied: numpy<2.3,>=1.22.4 in ./.venv/lib/python3.9/site-packages (from scipy) (2.0.2)\r\n",
      "Requirement already satisfied: torchvision in ./.venv/lib/python3.9/site-packages (0.23.0)\r\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.9/site-packages (from torchvision) (2.0.2)\r\n",
      "Requirement already satisfied: torch==2.8.0 in ./.venv/lib/python3.9/site-packages (from torchvision) (2.8.0)\r\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in ./.venv/lib/python3.9/site-packages (from torchvision) (11.3.0)\r\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.9/site-packages (from torch==2.8.0->torchvision) (3.19.1)\r\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in ./.venv/lib/python3.9/site-packages (from torch==2.8.0->torchvision) (4.15.0)\r\n",
      "Requirement already satisfied: sympy>=1.13.3 in ./.venv/lib/python3.9/site-packages (from torch==2.8.0->torchvision) (1.14.0)\r\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.9/site-packages (from torch==2.8.0->torchvision) (3.2.1)\r\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.9/site-packages (from torch==2.8.0->torchvision) (3.1.6)\r\n",
      "Requirement already satisfied: fsspec in ./.venv/lib/python3.9/site-packages (from torch==2.8.0->torchvision) (2025.10.0)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.9/site-packages (from sympy>=1.13.3->torch==2.8.0->torchvision) (1.3.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.9/site-packages (from jinja2->torch==2.8.0->torchvision) (3.0.3)\r\n",
      "Requirement already satisfied: torch in ./.venv/lib/python3.9/site-packages (2.8.0)\r\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.9/site-packages (from torch) (3.19.1)\r\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in ./.venv/lib/python3.9/site-packages (from torch) (4.15.0)\r\n",
      "Requirement already satisfied: sympy>=1.13.3 in ./.venv/lib/python3.9/site-packages (from torch) (1.14.0)\r\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.9/site-packages (from torch) (3.2.1)\r\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.9/site-packages (from torch) (3.1.6)\r\n",
      "Requirement already satisfied: fsspec in ./.venv/lib/python3.9/site-packages (from torch) (2025.10.0)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.9/site-packages (from sympy>=1.13.3->torch) (1.3.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.9/site-packages (from jinja2->torch) (3.0.3)\r\n"
     ]
    }
   ],
   "execution_count": 111
  },
  {
   "cell_type": "code",
   "id": "fbc121e30a2defb3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-26T09:26:07.394537Z",
     "start_time": "2025-11-26T09:26:07.391349Z"
    }
   },
   "source": [
    "import torch                # PyTorch: das Hauptframework für Deep Learning (Tensors, Autograd, Modelle, Training)\n",
    "import torchvision          # TorchVision: Bibliothek mit Standard-Datasets, Modellen und Bildtransformationen\n",
    "import torch.nn as nn       # nn = Neural Network Module: für das Erstellen von Layern (z.B. Conv2d, Linear, ReLU)\n",
    "import torch.nn.functional as F  # Funktionale API: für direkte Nutzung von Aktivierungsfunktionen, Losses, etc. (z.B. F.relu)\n",
    "import torch.optim as optim       # Optimizer (z.B. SGD, Adam) für das Training von Modellen\n",
    "import matplotlib.pyplot as plt   # Matplotlib: zum Plotten von Bildern, Trainingskurven oder Loss/Accuracy\n",
    "import random                     # Python Standardbibliothek: für zufällige Zahlen, Shuffling, Seed-Kontrolle\n",
    "import scipy.io                   # SciPy I/O: um .mat-Dateien (MATLAB-Format) zu laden, z.B. deine train/test Daten\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "                                  # Dataset: Basisklasse für eigene Datensätze\n",
    "                                  # DataLoader: erleichtert Batch-Verarbeitung, Shuffling und Parallelisierung beim Training\n",
    "import torchvision.transforms as transforms\n",
    "                                  # Transformations für Bilder: z.B. Normalisierung, Resizing, RandomCrop, ToTensor\n",
    "import os                         # Zugriff auf Betriebssystemfunktionen (Dateien, Verzeichnisse)\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from torchsummary import summary\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Daten",
   "id": "f6a1edabc7d934fd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Daten über torchvision",
   "id": "4a239986587698a6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Für unseren Datensatz gibt es zwei Wege die Daten sinnvoll im Notebook zu hinterlegen. Tatsächlich gibt es diesen Datensatz im Paket torchvision und kann direkt per Code heruntergeladen werden. Das ist vor allem dann sinnvoll, wenn der Datensatz auf dem lokalen Gerät nicht zur Verfügung steht. Es bietet aber außerdem den Vorteil, dass man damit besser über github arbeiten kann, da dort die Daten über 100MB nicht hochgeladen werden können. Zur Vollständigkeit ist aber dennoch der Code zum einlesen der Daten darunter.",
   "id": "547250f3e7f3bdcd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-26T09:31:05.150467Z",
     "start_time": "2025-11-26T09:26:11.909185Z"
    }
   },
   "cell_type": "code",
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Train-Set\n",
    "train_dataset = datasets.SVHN(\n",
    "    root=\"data\",\n",
    "    split='train',      # 'train' = train_32x32.mat\n",
    "    download=True,      # lädt automatisch herunter, wenn nicht vorhanden\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "# Test-Set\n",
    "test_dataset = datasets.SVHN(\n",
    "    root=\"data\",\n",
    "    split='test',       # 'test' = test_32x32.mat\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader  = DataLoader(test_dataset,  batch_size=1000, shuffle=False)\n"
   ],
   "id": "2e3d8bb0352fee3e",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n",
      "100.0%\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Daten über seine eigene os",
   "id": "563d3745bb55d93a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "In dieser Zelle wird eine **PyTorch Dataset-Klasse** für den SVHN-Datensatz definiert.\n",
    "\n",
    "- **Zweck:** Ermöglicht es, `.mat`-Dateien mit Bildern und Labels einfach in PyTorch zu laden und in Batches zu verarbeiten.\n",
    "- **Funktionen:**\n",
    "  - `__init__`: Lädt die Daten aus der `.mat`-Datei und speichert optionale Transformationen.\n",
    "  - `__len__`: Gibt die Anzahl der Bilder zurück (wichtig für DataLoader).\n",
    "  - `__getitem__`: Liefert ein einzelnes Bild und Label als Tensor, normalisiert die Pixel und wendet Transformationen an.\n",
    "- **Besonderheiten:**\n",
    "  - Pixelwerte werden von [0,255] auf [0,1] normalisiert.\n",
    "  - SVHN Label „10“ wird in `0` umgewandelt.\n",
    "  - PyTorch erwartet die Kanal-Reihenfolge `(C,H,W)`, daher wird das Bild umgeformt.\n"
   ],
   "id": "672bdb5d0b72a3e0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T19:16:40.676553Z",
     "start_time": "2025-11-29T19:16:40.671457Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class SVHNDataset(Dataset):\n",
    "    # Wir erben von torch.utils.data.Dataset, damit PyTorch den DataLoader nutzen kann\n",
    "\n",
    "    def __init__(self, mat_file, transform=None):\n",
    "        # Konstruktor: Lädt die .mat-Datei und initialisiert Variablen\n",
    "        data = scipy.io.loadmat(mat_file)   # scipy.io.loadmat: lädt MATLAB-Dateien\n",
    "        self.X = data['X']                  # X enthält die Bilder: shape (32, 32, 3, N)\n",
    "        self.y = data['y'].flatten()        # y enthält Labels: shape (N,). flatten() macht aus Spaltenvektor 1D-Array\n",
    "        self.transform = transform          # Transformationsobjekt für Bildvorverarbeitung (optional)\n",
    "\n",
    "        # SVHN-Spezialfall: Label \"10\" bedeutet Ziffer \"0\"\n",
    "        self.y[self.y == 10] = 0\n",
    "\n",
    "    def __len__(self):\n",
    "        # Gibt die Anzahl der Samples zurück, damit DataLoader weiß, wie viele es gibt\n",
    "        return self.X.shape[3]              # N = Anzahl Bilder\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Liefert ein einzelnes Sample (Bild + Label) für Index idx\n",
    "        img = self.X[:,:,:,idx]             # Bild: (32,32,3)\n",
    "        # torch.tensor: konvertiert numpy-array zu Tensor\n",
    "        # permute(2,0,1): PyTorch erwartet Kanal zuerst (C,H,W) statt H,W,C\n",
    "        # float()/255.0: normalisiert Pixelwerte von [0,255] auf [0,1]\n",
    "        img = torch.tensor(img).permute(2,0,1).float()/255.0\n",
    "        label = int(self.y[idx])            # Label als Integer\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)       # Falls Transform gesetzt, anwenden (z.B. RandomCrop, Normalize)\n",
    "\n",
    "        return img, label                     # Rückgabe: Tensorbild + Label\n"
   ],
   "id": "f67558b545203f4d",
   "outputs": [],
   "execution_count": 116
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "transform = transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))\n",
    "\n",
    "train_dataset = SVHNDataset('train_32x32.mat', transform=transform)\n",
    "test_dataset  = SVHNDataset('test_32x32.mat',  transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader  = DataLoader(test_dataset,  batch_size=1000, shuffle=False)\n"
   ],
   "id": "269c0f764e9b78c5"
  },
  {
   "cell_type": "markdown",
   "id": "827c5d27301f398c",
   "metadata": {},
   "source": "### Beispiel der Trainingsdaten\n"
  },
  {
   "cell_type": "code",
   "id": "b02d1d7c32ac5c9c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-26T09:32:36.707586Z",
     "start_time": "2025-11-26T09:32:36.190841Z"
    }
   },
   "source": [
    "# Iterator über den Test-DataLoader erstellen\n",
    "examples = enumerate(test_loader)\n",
    "\n",
    "# Den ersten Batch aus dem Test-DataLoader holen\n",
    "batch_idx, (example_data, example_targets) = next(examples)\n",
    "\n",
    "# Neue Figur für die Plots erstellen\n",
    "fig = plt.figure()\n",
    "\n",
    "# 6 Bilder aus dem Batch plotten\n",
    "for i in range(6):\n",
    "    # Unterteilt die Figur in 2x3 Raster, i+1 = aktuelle Position\n",
    "    plt.subplot(2, 3, i+1)\n",
    "    plt.tight_layout()  # Sorgt dafür, dass Plots sich nicht überlappen\n",
    "    # Zeige das Bild an; example_data[i][0] = 1. Kanal (Graustufe)\n",
    "    plt.imshow(example_data[i][0], cmap='gray', interpolation='none')\n",
    "    # Titel mit Ground-Truth-Label\n",
    "    plt.title(\"Ground Truth: {}\".format(example_targets[i]))\n",
    "    # Achsenbeschriftungen entfernen\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "\n",
    "# Alle geplotteten Bilder anzeigen\n",
    "plt.show()\n"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 6 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## klassisches neuronales Netz",
   "id": "159182b8463df21d"
  },
  {
   "cell_type": "markdown",
   "id": "54a8cbb0d3e384d",
   "metadata": {},
   "source": [
    "### Definition des CNN-Modells\n",
    "\n",
    "In dieser Zelle wird die Klasse `Net` definiert, die ein Convolutional Neural Network (CNN) implementiert.\n",
    "\n",
    "- **Zweck:** Definiert die Architektur des neuronalen Netzes für Bilderkennung.\n",
    "- **Aufbau:**\n",
    "  1. Convolutional Layer (Feature Extraction)\n",
    "  2. Max-Pooling (Reduzierung der Dimensionen)\n",
    "  3. ReLU (Nichtlinearität)\n",
    "  4. Dropout (Regularisierung)\n",
    "  5. Flatten (Umwandlung in 1D für Fully Connected Layer)\n",
    "  6. Fully Connected Layer(s) (klassische Klassifizierung)\n",
    "  7. Softmax (Wahrscheinlichkeitsausgabe für Klassen)\n",
    "- **TODO-Liste im Kommentar:** Zeigt, welche Layer du für ein vollständiges CNN einbauen solltest.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "7c922dbecff6e9ff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-26T09:33:25.682741Z",
     "start_time": "2025-11-26T09:33:25.675855Z"
    }
   },
   "source": [
    "# TODO Change this class to implement\n",
    "# 1. A valid convolution with kernel size 5, 1 input channel and 10 output channels\n",
    "# 2. A max pooling operation over a 2x2 area\n",
    "# 3. A Relu\n",
    "# 4. A valid convolution with kernel size 5, 10 input channels and 20 output channels\n",
    "# 5. A 2D Dropout layer\n",
    "# 6. A max pooling operation over a 2x2 area\n",
    "# 7. A relu\n",
    "# 8. A flattening operation\n",
    "# 9. A fully connected layer mapping from (whatever dimensions we are at-- find out using .shape) to 50\n",
    "# 10. A ReLU\n",
    "# 11. A fully connected layer mapping from 50 to 10 dimensions\n",
    "# 12. A softmax function.\n",
    "\n",
    "# Replace this class which implements a minimal network (which still does okay)\n",
    "\n",
    "# CNN-Klasse definieren\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        # --- Erster Convolutional Layer ---\n",
    "        # nn.Conv2d: 2D Convolution\n",
    "        # 3 Input-Kanäle (RGB), 10 Output-Kanäle (Features), Kernelgröße 5x5\n",
    "        self.conv1 = nn.Conv2d(3, 10, kernel_size=5)  # TODO: ggf. auf 1 Input-Kanal anpassen für Graustufen\n",
    "\n",
    "        # Dropout für die Convolutionen (reduziert Overfitting)\n",
    "        self.drop = nn.Dropout2d()\n",
    "\n",
    "        # Fully Connected Layer\n",
    "        # nn.Linear: verbindet alle Eingangseinheiten mit allen Ausgangseinheiten\n",
    "        # 1960 Eingangsdimensionen → 1000 Ausgangsdimensionen (muss evtl. an tatsächliche Flatten-Größe angepasst werden)\n",
    "        self.fc1 = nn.Linear(1960, 1000)\n",
    "\n",
    "    # --- Forward-Pass ---\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)               # 1. Convolution\n",
    "        x = self.drop(x)                # 2. Dropout\n",
    "        x = F.max_pool2d(x, 2)          # 3. Max-Pooling über 2x2\n",
    "        x = F.relu(x)                   # 4. ReLU-Aktivierung\n",
    "        x = x.flatten(1)                # 5. Flattening: CxHxW → 1D für Fully Connected Layer\n",
    "        x = self.fc1(x)                 # 6. Fully Connected Layer\n",
    "        x = F.log_softmax(x)            # 7. Log-Softmax für Klassenausgabe\n",
    "        return x                        # Rückgabe: Log-Wahrscheinlichkeiten pro Klasse"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "id": "153a6d1b806aa4d5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-26T09:33:28.250815Z",
     "start_time": "2025-11-26T09:33:28.248108Z"
    }
   },
   "source": [
    "# He initialization of weights\n",
    "def weights_init(layer_in):\n",
    "  if isinstance(layer_in, nn.Linear):\n",
    "    nn.init.kaiming_uniform_(layer_in.weight)\n",
    "    layer_in.bias.data.fill_(0.0)"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "id": "e9e28f00bbe1a2",
   "metadata": {},
   "source": [
    "### Initialisierung des Modells, der Gewichte und des Optimizers\n",
    "\n",
    "In dieser Zelle wird das neuronale Netz erstellt, seine Gewichte initialisiert und der Optimizer definiert.\n",
    "\n",
    "- `model = Net()`: Erstellt eine Instanz des zuvor definierten CNN-Modells.\n",
    "- `model.apply(weights_init)`: Wendet eine eigene Initialisierungsfunktion (`weights_init`) auf alle Layer des Modells an.\n",
    "  Dies sorgt dafür, dass die Anfangsgewichte der Layer sinnvoll gesetzt werden, was das Lernen stabiler und schneller macht.\n",
    "- `optim.SGD(...)`: Erstellt einen Stochastic Gradient Descent Optimizer.\n",
    "  - `model.parameters()`: Übergibt die trainierbaren Parameter des Modells.\n",
    "  - `lr=0.01`: Lernrate.\n",
    "  - `momentum=0.5`: Verbessert die Konvergenzgeschwindigkeit, indem vorherige Gradienten berücksichtigt werden.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "c2e06fa5ac0111d5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-26T11:12:39.587408Z",
     "start_time": "2025-11-26T11:12:39.569632Z"
    }
   },
   "source": [
    "# Erstellt eine Instanz des CNN-Modells (Klasse Net aus vorheriger Zelle)\n",
    "model = Net()\n",
    "\n",
    "# Initialisiert die Gewichte aller Layer mit einer benutzerdefinierten Funktion weights_init\n",
    "# Dadurch starten die Gewichte nicht zufällig schlecht, was Training verbessern kann\n",
    "model.apply(weights_init)\n",
    "\n",
    "# Definiert den Optimizer: Stochastic Gradient Descent\n",
    "# model.parameters(): alle trainierbaren Parameter des Modells\n",
    "# lr=0.01: Lernrate\n",
    "# momentum=0.5: hilft, \"Schwankungen\" im Gradienten auszugleichen\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n"
   ],
   "outputs": [],
   "execution_count": 37
  },
  {
   "cell_type": "markdown",
   "id": "43767df867a534a",
   "metadata": {},
   "source": [
    "### Trainingsroutine\n",
    "\n",
    "In dieser Zelle wird die Trainingsfunktion `train(epoch)` definiert.\n",
    "Sie führt einen kompletten Trainingsdurchlauf (Epoch) über den gesamten Trainingsdatensatz aus.\n",
    "\n",
    "Ablauf:\n",
    "1. `model.train()`: Schaltet das Modell in den Trainingsmodus (aktiviert z.B. Dropout).\n",
    "2. Schleife über alle Batches im `train_loader`.\n",
    "3. `optimizer.zero_grad()`: Setzt Gradienten des vorherigen Backpropagation-Schritts zurück.\n",
    "4. `output = model(data)`: Modellvorhersage für den aktuellen Batch.\n",
    "5. `loss = F.nll_loss(...)`: Berechnet die Negative Log-Likelihood Loss (passend für `log_softmax`).\n",
    "6. `loss.backward()`: Backpropagation – berechnet Gradienten.\n",
    "7. `optimizer.step()`: Aktualisiert die Modellparameter.\n",
    "8. Alle 10 Batches wird der aktuelle Trainingsfortschritt (Loss) ausgegeben.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "7f77e0e2fc3fec3d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-26T11:12:42.421287Z",
     "start_time": "2025-11-26T11:12:42.416429Z"
    }
   },
   "source": [
    "# Haupt-Trainingsroutine\n",
    "def train(epoch):\n",
    "    model.train()  # Modell in den Trainingsmodus setzen (Dropout aktiv, BatchNorm aktiviert)\n",
    "\n",
    "    # Schleife über alle Batches im Training DataLoader\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "\n",
    "        optimizer.zero_grad()       # Vor jedem Schritt Gradienten zurücksetzen\n",
    "\n",
    "        output = model(data)        # Modellvorhersage für aktuellen Batch\n",
    "        loss = F.nll_loss(output, target)  # Loss berechnen (für log_softmax geeignet)\n",
    "\n",
    "        loss.backward()             # Backpropagation: Gradienten berechnen\n",
    "        optimizer.step()            # Parameter-Update mittels Optimizer\n",
    "\n",
    "        # Fortschrittsanzeige alle 10 Batches\n",
    "        if batch_idx % 10 == 0:\n",
    "            print(\n",
    "                'Train Epoch: {} [{}/{}]\\tLoss: {:.6f}'.format(\n",
    "                    epoch,                          # Aktuelle Epoche\n",
    "                    batch_idx * len(data),          # Anzahl verarbeiteter Trainingsbeispiele\n",
    "                    len(train_loader.dataset),      # Gesamtzahl der Trainingsdaten\n",
    "                    loss.item()                     # Aktueller Loss-Wert\n",
    "                )\n",
    "            )\n"
   ],
   "outputs": [],
   "execution_count": 38
  },
  {
   "cell_type": "markdown",
   "id": "8123efde2fd30a84",
   "metadata": {},
   "source": [
    "### Test-/Evaluierungsroutine\n",
    "\n",
    "In dieser Zelle wird die Funktion `test()` definiert, die das Modell auf den Testdaten auswertet.\n",
    "\n",
    "Ablauf:\n",
    "1. `model.eval()`: Setzt das Modell in den Evaluierungsmodus\n",
    "   (Dropout deaktiviert, BatchNorm verwendet Durchschnittswerte).\n",
    "2. `torch.no_grad()`: Deaktiviert Gradientenberechnung – spart Zeit und Speicher.\n",
    "3. Schleife über `test_loader`:\n",
    "   - Modellvorhersagen berechnen\n",
    "   - Loss aufsummieren (für Durchschnitt am Ende)\n",
    "   - Vorhersagen mit den Zielwerten vergleichen → Accuracy\n",
    "4. Nach der Schleife:\n",
    "   - Durchschnitts-Loss berechnen\n",
    "   - Genauigkeit berechnen und ausgeben\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "43b154fbc5b9d301",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-26T11:12:45.534804Z",
     "start_time": "2025-11-26T11:12:45.529710Z"
    }
   },
   "source": [
    "# Testfunktion, um das Modell auf den Testdaten auszuwerten\n",
    "def test():\n",
    "    model.eval()      # Modell in den Evaluierungsmodus setzen (Dropout aus)\n",
    "    test_loss = 0     # Summe der Loss-Werte\n",
    "    correct = 0       # Anzahl korrekter Vorhersagen\n",
    "\n",
    "    # Im Testmodus keine Gradienten berechnen → schneller und spart Speicher\n",
    "    with torch.no_grad():\n",
    "        # Schleife über alle Test-Batches\n",
    "        for data, target in test_loader:\n",
    "\n",
    "            output = model(data)  # Modellvorhersage\n",
    "\n",
    "            # Addiere den Loss des Batches (size_average=False = Summe statt Mittelwert)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()\n",
    "\n",
    "            # Wähle die Klasse mit der höchsten Wahrscheinlichkeit\n",
    "            pred = output.data.max(1, keepdim=True)[1]\n",
    "\n",
    "            # Vergleiche Vorhersagen mit dem Ground-Truth\n",
    "            correct += pred.eq(target.data.view_as(pred)).sum()\n",
    "\n",
    "    # Durchschnittlichen Loss berechnen\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    # Ergebnisse ausgeben\n",
    "    print(\n",
    "        '\\nTest set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "            test_loss,\n",
    "            correct,\n",
    "            len(test_loader.dataset),\n",
    "            100. * correct / len(test_loader.dataset)\n",
    "        )\n",
    "    )"
   ],
   "outputs": [],
   "execution_count": 39
  },
  {
   "cell_type": "markdown",
   "id": "bfe5f4c34938c25a",
   "metadata": {},
   "source": [
    "### Ausführen des Trainings und Testens über mehrere Epochen\n",
    "\n",
    "In dieser Zelle wird zuerst die Anfangsleistung des Modells auf den Testdaten berechnet („untrainiertes Modell“).\n",
    "Anschließend wird das Modell über mehrere Epochen trainiert. Nach jeder Epoche wird erneut getestet, um den Fortschritt zu messen.\n",
    "\n",
    "Ablauf:\n",
    "1. `test()`: Bewertung des Modells vor dem Training (Baseline).\n",
    "2. `n_epochs = 3`: Anzahl der Trainingsdurchläufe (Epochen).\n",
    "3. Schleife über alle Epochen:\n",
    "   - `train(epoch)`: Trainingsdurchlauf über alle Trainingsdaten.\n",
    "   - `test()`: Bewertung der Modellleistung nach dieser Epoche.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "10649218547097f6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-26T11:13:32.095184Z",
     "start_time": "2025-11-26T11:12:48.819057Z"
    }
   },
   "source": [
    "# Anfangsleistung des Modells testen (ungelerntes Modell)\n",
    "test()\n",
    "\n",
    "# Anzahl der Trainings-Epochen festlegen\n",
    "n_epochs = 3\n",
    "\n",
    "# Trainings-/Testschleife über mehrere Epochen\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    train(epoch)  # Modell trainieren\n",
    "    test()        # Modell nach der Epoche testen\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gm/d7swcmj57md2th2_r0b315x80000gn/T/ipykernel_38549/299424523.py:43: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  x = F.log_softmax(x)            # 7. Log-Softmax für Klassenausgabe\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 7.0260, Accuracy: 0/26032 (0%)\n",
      "\n",
      "Train Epoch: 1 [0/73257]\tLoss: 7.019948\n",
      "Train Epoch: 1 [1280/73257]\tLoss: 5.422011\n",
      "Train Epoch: 1 [2560/73257]\tLoss: 4.442428\n",
      "Train Epoch: 1 [3840/73257]\tLoss: 3.591133\n",
      "Train Epoch: 1 [5120/73257]\tLoss: 3.523211\n",
      "Train Epoch: 1 [6400/73257]\tLoss: 3.417609\n",
      "Train Epoch: 1 [7680/73257]\tLoss: 3.326477\n",
      "Train Epoch: 1 [8960/73257]\tLoss: 3.509353\n",
      "Train Epoch: 1 [10240/73257]\tLoss: 3.236801\n",
      "Train Epoch: 1 [11520/73257]\tLoss: 3.747551\n",
      "Train Epoch: 1 [12800/73257]\tLoss: 3.316441\n",
      "Train Epoch: 1 [14080/73257]\tLoss: 3.606859\n",
      "Train Epoch: 1 [15360/73257]\tLoss: 4.066888\n",
      "Train Epoch: 1 [16640/73257]\tLoss: 3.446515\n",
      "Train Epoch: 1 [17920/73257]\tLoss: 3.195255\n",
      "Train Epoch: 1 [19200/73257]\tLoss: 3.382719\n",
      "Train Epoch: 1 [20480/73257]\tLoss: 3.588111\n",
      "Train Epoch: 1 [21760/73257]\tLoss: 3.586449\n",
      "Train Epoch: 1 [23040/73257]\tLoss: 3.592638\n",
      "Train Epoch: 1 [24320/73257]\tLoss: 3.387845\n",
      "Train Epoch: 1 [25600/73257]\tLoss: 3.451303\n",
      "Train Epoch: 1 [26880/73257]\tLoss: 3.441940\n",
      "Train Epoch: 1 [28160/73257]\tLoss: 3.403947\n",
      "Train Epoch: 1 [29440/73257]\tLoss: 3.728967\n",
      "Train Epoch: 1 [30720/73257]\tLoss: 3.324754\n",
      "Train Epoch: 1 [32000/73257]\tLoss: 3.320905\n",
      "Train Epoch: 1 [33280/73257]\tLoss: 3.549397\n",
      "Train Epoch: 1 [34560/73257]\tLoss: 3.556833\n",
      "Train Epoch: 1 [35840/73257]\tLoss: 3.009922\n",
      "Train Epoch: 1 [37120/73257]\tLoss: 3.370533\n",
      "Train Epoch: 1 [38400/73257]\tLoss: 3.257215\n",
      "Train Epoch: 1 [39680/73257]\tLoss: 3.671174\n",
      "Train Epoch: 1 [40960/73257]\tLoss: 3.555156\n",
      "Train Epoch: 1 [42240/73257]\tLoss: 3.634991\n",
      "Train Epoch: 1 [43520/73257]\tLoss: 3.520216\n",
      "Train Epoch: 1 [44800/73257]\tLoss: 3.634040\n",
      "Train Epoch: 1 [46080/73257]\tLoss: 3.272236\n",
      "Train Epoch: 1 [47360/73257]\tLoss: 3.690800\n",
      "Train Epoch: 1 [48640/73257]\tLoss: 3.312031\n",
      "Train Epoch: 1 [49920/73257]\tLoss: 3.491589\n",
      "Train Epoch: 1 [51200/73257]\tLoss: 3.136804\n",
      "Train Epoch: 1 [52480/73257]\tLoss: 3.089536\n",
      "Train Epoch: 1 [53760/73257]\tLoss: 3.394505\n",
      "Train Epoch: 1 [55040/73257]\tLoss: 3.363781\n",
      "Train Epoch: 1 [56320/73257]\tLoss: 3.582965\n",
      "Train Epoch: 1 [57600/73257]\tLoss: 3.461028\n",
      "Train Epoch: 1 [58880/73257]\tLoss: 3.462510\n",
      "Train Epoch: 1 [60160/73257]\tLoss: 3.627084\n",
      "Train Epoch: 1 [61440/73257]\tLoss: 3.072854\n",
      "Train Epoch: 1 [62720/73257]\tLoss: 2.979641\n",
      "Train Epoch: 1 [64000/73257]\tLoss: 3.111130\n",
      "Train Epoch: 1 [65280/73257]\tLoss: 2.851865\n",
      "Train Epoch: 1 [66560/73257]\tLoss: 2.768888\n",
      "Train Epoch: 1 [67840/73257]\tLoss: 3.053923\n",
      "Train Epoch: 1 [69120/73257]\tLoss: 2.982071\n",
      "Train Epoch: 1 [70400/73257]\tLoss: 2.764462\n",
      "Train Epoch: 1 [71680/73257]\tLoss: 2.509985\n",
      "Train Epoch: 1 [72960/73257]\tLoss: 2.873079\n",
      "\n",
      "Test set: Avg. loss: 2.3266, Accuracy: 5097/26032 (20%)\n",
      "\n",
      "Train Epoch: 2 [0/73257]\tLoss: 2.642552\n",
      "Train Epoch: 2 [1280/73257]\tLoss: 3.083095\n",
      "Train Epoch: 2 [2560/73257]\tLoss: 2.940811\n",
      "Train Epoch: 2 [3840/73257]\tLoss: 2.630381\n",
      "Train Epoch: 2 [5120/73257]\tLoss: 3.134398\n",
      "Train Epoch: 2 [6400/73257]\tLoss: 2.646534\n",
      "Train Epoch: 2 [7680/73257]\tLoss: 2.889861\n",
      "Train Epoch: 2 [8960/73257]\tLoss: 2.663751\n",
      "Train Epoch: 2 [10240/73257]\tLoss: 2.957978\n",
      "Train Epoch: 2 [11520/73257]\tLoss: 2.888613\n",
      "Train Epoch: 2 [12800/73257]\tLoss: 2.899235\n",
      "Train Epoch: 2 [14080/73257]\tLoss: 2.651624\n",
      "Train Epoch: 2 [15360/73257]\tLoss: 2.881016\n",
      "Train Epoch: 2 [16640/73257]\tLoss: 2.877301\n",
      "Train Epoch: 2 [17920/73257]\tLoss: 2.773690\n",
      "Train Epoch: 2 [19200/73257]\tLoss: 2.850177\n",
      "Train Epoch: 2 [20480/73257]\tLoss: 2.705559\n",
      "Train Epoch: 2 [21760/73257]\tLoss: 2.639607\n",
      "Train Epoch: 2 [23040/73257]\tLoss: 3.039027\n",
      "Train Epoch: 2 [24320/73257]\tLoss: 2.954003\n",
      "Train Epoch: 2 [25600/73257]\tLoss: 2.552933\n",
      "Train Epoch: 2 [26880/73257]\tLoss: 2.861610\n",
      "Train Epoch: 2 [28160/73257]\tLoss: 2.971979\n",
      "Train Epoch: 2 [29440/73257]\tLoss: 2.743769\n",
      "Train Epoch: 2 [30720/73257]\tLoss: 2.968084\n",
      "Train Epoch: 2 [32000/73257]\tLoss: 2.767715\n",
      "Train Epoch: 2 [33280/73257]\tLoss: 3.138464\n",
      "Train Epoch: 2 [34560/73257]\tLoss: 2.736064\n",
      "Train Epoch: 2 [35840/73257]\tLoss: 2.783772\n",
      "Train Epoch: 2 [37120/73257]\tLoss: 2.750077\n",
      "Train Epoch: 2 [38400/73257]\tLoss: 2.880088\n",
      "Train Epoch: 2 [39680/73257]\tLoss: 2.622209\n",
      "Train Epoch: 2 [40960/73257]\tLoss: 2.651929\n",
      "Train Epoch: 2 [42240/73257]\tLoss: 2.752656\n",
      "Train Epoch: 2 [43520/73257]\tLoss: 2.800812\n",
      "Train Epoch: 2 [44800/73257]\tLoss: 2.862595\n",
      "Train Epoch: 2 [46080/73257]\tLoss: 2.878808\n",
      "Train Epoch: 2 [47360/73257]\tLoss: 2.615080\n",
      "Train Epoch: 2 [48640/73257]\tLoss: 2.548252\n",
      "Train Epoch: 2 [49920/73257]\tLoss: 2.646057\n",
      "Train Epoch: 2 [51200/73257]\tLoss: 2.808646\n",
      "Train Epoch: 2 [52480/73257]\tLoss: 3.013874\n",
      "Train Epoch: 2 [53760/73257]\tLoss: 2.880257\n",
      "Train Epoch: 2 [55040/73257]\tLoss: 2.828686\n",
      "Train Epoch: 2 [56320/73257]\tLoss: 2.708411\n",
      "Train Epoch: 2 [57600/73257]\tLoss: 2.678521\n",
      "Train Epoch: 2 [58880/73257]\tLoss: 2.730254\n",
      "Train Epoch: 2 [60160/73257]\tLoss: 2.800993\n",
      "Train Epoch: 2 [61440/73257]\tLoss: 2.695388\n",
      "Train Epoch: 2 [62720/73257]\tLoss: 2.934298\n",
      "Train Epoch: 2 [64000/73257]\tLoss: 2.836193\n",
      "Train Epoch: 2 [65280/73257]\tLoss: 2.823063\n",
      "Train Epoch: 2 [66560/73257]\tLoss: 2.620929\n",
      "Train Epoch: 2 [67840/73257]\tLoss: 2.753327\n",
      "Train Epoch: 2 [69120/73257]\tLoss: 2.690872\n",
      "Train Epoch: 2 [70400/73257]\tLoss: 2.560905\n",
      "Train Epoch: 2 [71680/73257]\tLoss: 2.550229\n",
      "Train Epoch: 2 [72960/73257]\tLoss: 2.737444\n",
      "\n",
      "Test set: Avg. loss: 2.2117, Accuracy: 5163/26032 (20%)\n",
      "\n",
      "Train Epoch: 3 [0/73257]\tLoss: 2.639792\n",
      "Train Epoch: 3 [1280/73257]\tLoss: 2.884874\n",
      "Train Epoch: 3 [2560/73257]\tLoss: 2.695630\n",
      "Train Epoch: 3 [3840/73257]\tLoss: 2.761204\n",
      "Train Epoch: 3 [5120/73257]\tLoss: 2.658911\n",
      "Train Epoch: 3 [6400/73257]\tLoss: 2.475491\n",
      "Train Epoch: 3 [7680/73257]\tLoss: 2.788910\n",
      "Train Epoch: 3 [8960/73257]\tLoss: 2.659894\n",
      "Train Epoch: 3 [10240/73257]\tLoss: 2.875117\n",
      "Train Epoch: 3 [11520/73257]\tLoss: 2.863993\n",
      "Train Epoch: 3 [12800/73257]\tLoss: 2.975267\n",
      "Train Epoch: 3 [14080/73257]\tLoss: 2.965313\n",
      "Train Epoch: 3 [15360/73257]\tLoss: 2.784371\n",
      "Train Epoch: 3 [16640/73257]\tLoss: 2.682542\n",
      "Train Epoch: 3 [17920/73257]\tLoss: 2.656728\n",
      "Train Epoch: 3 [19200/73257]\tLoss: 2.659656\n",
      "Train Epoch: 3 [20480/73257]\tLoss: 2.725478\n",
      "Train Epoch: 3 [21760/73257]\tLoss: 2.874536\n",
      "Train Epoch: 3 [23040/73257]\tLoss: 2.487536\n",
      "Train Epoch: 3 [24320/73257]\tLoss: 2.820583\n",
      "Train Epoch: 3 [25600/73257]\tLoss: 2.862849\n",
      "Train Epoch: 3 [26880/73257]\tLoss: 2.738981\n",
      "Train Epoch: 3 [28160/73257]\tLoss: 2.628475\n",
      "Train Epoch: 3 [29440/73257]\tLoss: 2.669019\n",
      "Train Epoch: 3 [30720/73257]\tLoss: 2.909695\n",
      "Train Epoch: 3 [32000/73257]\tLoss: 2.577963\n",
      "Train Epoch: 3 [33280/73257]\tLoss: 2.738708\n",
      "Train Epoch: 3 [34560/73257]\tLoss: 2.651720\n",
      "Train Epoch: 3 [35840/73257]\tLoss: 2.779266\n",
      "Train Epoch: 3 [37120/73257]\tLoss: 2.724792\n",
      "Train Epoch: 3 [38400/73257]\tLoss: 2.710169\n",
      "Train Epoch: 3 [39680/73257]\tLoss: 2.821268\n",
      "Train Epoch: 3 [40960/73257]\tLoss: 2.589869\n",
      "Train Epoch: 3 [42240/73257]\tLoss: 2.556251\n",
      "Train Epoch: 3 [43520/73257]\tLoss: 2.612860\n",
      "Train Epoch: 3 [44800/73257]\tLoss: 2.833094\n",
      "Train Epoch: 3 [46080/73257]\tLoss: 2.538879\n",
      "Train Epoch: 3 [47360/73257]\tLoss: 2.473727\n",
      "Train Epoch: 3 [48640/73257]\tLoss: 2.612411\n",
      "Train Epoch: 3 [49920/73257]\tLoss: 2.808944\n",
      "Train Epoch: 3 [51200/73257]\tLoss: 2.746826\n",
      "Train Epoch: 3 [52480/73257]\tLoss: 2.621413\n",
      "Train Epoch: 3 [53760/73257]\tLoss: 2.888114\n",
      "Train Epoch: 3 [55040/73257]\tLoss: 2.886714\n",
      "Train Epoch: 3 [56320/73257]\tLoss: 2.695531\n",
      "Train Epoch: 3 [57600/73257]\tLoss: 2.848211\n",
      "Train Epoch: 3 [58880/73257]\tLoss: 2.723468\n",
      "Train Epoch: 3 [60160/73257]\tLoss: 2.710991\n",
      "Train Epoch: 3 [61440/73257]\tLoss: 2.736977\n",
      "Train Epoch: 3 [62720/73257]\tLoss: 2.564607\n",
      "Train Epoch: 3 [64000/73257]\tLoss: 2.783994\n",
      "Train Epoch: 3 [65280/73257]\tLoss: 2.676960\n",
      "Train Epoch: 3 [66560/73257]\tLoss: 2.770494\n",
      "Train Epoch: 3 [67840/73257]\tLoss: 2.682677\n",
      "Train Epoch: 3 [69120/73257]\tLoss: 2.601593\n",
      "Train Epoch: 3 [70400/73257]\tLoss: 2.628576\n",
      "Train Epoch: 3 [71680/73257]\tLoss: 2.554036\n",
      "Train Epoch: 3 [72960/73257]\tLoss: 2.701440\n",
      "\n",
      "Test set: Avg. loss: 2.1723, Accuracy: 5619/26032 (22%)\n",
      "\n"
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "cell_type": "markdown",
   "id": "2a8defd803f7a988",
   "metadata": {},
   "source": [
    "### Vorhersagen des trainierten Modells visualisieren\n",
    "\n",
    "In dieser Zelle wird das Modell auf einige zuvor geladene Beispielbilder angewendet.\n",
    "Die Predictions (Vorhersagen) werden zusammen mit den Bildern dargestellt.\n",
    "\n",
    "Ablauf:\n",
    "1. `output = model(example_data)`: Modell erzeugt Vorhersagen für die ausgewählten Testbilder.\n",
    "2. `plt.figure()`: Neue Plot-Figur.\n",
    "3. Schleife über mehrere Bilder:\n",
    "   - Bild anzeigen (`imshow`)\n",
    "   - Modellvorhersage ermitteln (`output.data.max(...)`)\n",
    "   - Prediction als Titel ausgeben\n",
    "   - Achsen entfernen\n",
    "4. `plt.show()`: Darstellung der Ergebnisse.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "92c082b0e372bf4d",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2025-11-26T11:13:32.357492Z",
     "start_time": "2025-11-26T11:13:32.101595Z"
    }
   },
   "source": [
    "# Modellvorhersagen für die zuvor geladenen Beispielbilder berechnen\n",
    "output = model(example_data)\n",
    "\n",
    "# Neue Figur für die Plots erstellen\n",
    "fig = plt.figure()\n",
    "\n",
    "# Zeige die ersten 10 Bilder und ihre Vorhersagen\n",
    "for i in range(10):\n",
    "    plt.subplot(5, 5, i+1)  # 5x5 Raster, aktueller Plot: i+1\n",
    "    plt.tight_layout()      # Überlappungen vermeiden\n",
    "\n",
    "    # Beispielbild anzeigen (Graustufe)\n",
    "    plt.imshow(example_data[i][0], cmap='gray', interpolation='none')\n",
    "\n",
    "    # Vorhersage ermitteln:\n",
    "    # max(1) → höchste Wahrscheinlichkeit entlang der Klassenachse\n",
    "    # [1] → Index des Maximums → vorhergesagte Klasse\n",
    "    predicted_label = output.data.max(1, keepdim=True)[1][i].item()\n",
    "\n",
    "    # Titel mit der Vorhersage\n",
    "    plt.title(\"Prediction: {}\".format(predicted_label))\n",
    "\n",
    "    # Achsen entfernen\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "\n",
    "# Plots anzeigen\n",
    "plt.show()\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gm/d7swcmj57md2th2_r0b315x80000gn/T/ipykernel_38549/299424523.py:43: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  x = F.log_softmax(x)            # 7. Log-Softmax für Klassenausgabe\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 10 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    }
   ],
   "execution_count": 41
  },
  {
   "cell_type": "markdown",
   "id": "b0168238de2cf947",
   "metadata": {},
   "source": "## flexible CNN Struktur mit 5 Layern"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T18:46:55.714637Z",
     "start_time": "2025-11-29T18:46:55.702794Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def run_training(model, optimizer, train_loader, test_loader, n_epochs=3, exp_info=\"\"):\n",
    "    train_losses, test_losses, test_accuracies = [], [], []\n",
    "\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "\n",
    "        model.train()\n",
    "        for data, target in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = F.nll_loss(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        train_losses.append(loss.item())\n",
    "\n",
    "        # ===== Testen =====\n",
    "        model.eval()\n",
    "        test_loss = 0\n",
    "        correct = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for data, target in test_loader:\n",
    "                output = model(data)\n",
    "                test_loss += F.nll_loss(output, target, reduction='sum').item()\n",
    "                pred = output.argmax(dim=1)\n",
    "                correct += pred.eq(target).sum().item()\n",
    "\n",
    "        test_loss /= len(test_loader.dataset)\n",
    "        accuracy = 100. * correct / len(test_loader.dataset)\n",
    "\n",
    "        test_losses.append(test_loss)\n",
    "        test_accuracies.append(accuracy)\n",
    "\n",
    "        prefix = f\"[{exp_info}] \" if exp_info else \"\"\n",
    "        print(f\"{prefix}Epoch {epoch} | Test Acc: {accuracy:.2f}% | Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "    return model, train_losses, test_losses, test_accuracies\n"
   ],
   "id": "78c4547ec0e6dbf",
   "outputs": [],
   "execution_count": 112
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Definition\n",
    "Flexible SVHN CNN mit 1–5 Convolutional Layers\n",
    "\n",
    "Dieser Code definiert eine **flexible Convolutional Neural Network (CNN) Architektur für das SVHN-Dataset**, die zwischen 1 und 5 Convolutional Layers variieren kann.\n",
    "Die Architektur umfasst:\n",
    "- Mehrere Convolutional Layers mit ReLU-Aktivierungen\n",
    "- Max-Pooling nach bestimmten Convs\n",
    "- Spatial Dropout nach der letzten Convolution\n",
    "- Fully Connected Layers für die Klassifikation in 10 Klassen (Ziffern 0–9)\n",
    "- Log-Softmax am Output für die Nutzung mit `NLLLoss`\n"
   ],
   "id": "a99b9806a7ca9b2b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-26T11:33:51.557495Z",
     "start_time": "2025-11-26T11:33:51.551715Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class SVHNC5NN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_conv_layers=5,   # Anzahl der Convolutional Layers, flexibel 1–5\n",
    "        dropout_rate=0.3,    # Dropout-Wahrscheinlichkeit nach Convs\n",
    "        fc_hidden=50         # Anzahl der Neuronen im Fully Connected Hidden Layer\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # Sicherstellen, dass num_conv_layers im erlaubten Bereich liegt\n",
    "        assert 1 <= num_conv_layers <= 5, \"num_conv_layers muss zwischen 1 und 5 liegen\"\n",
    "        self.num_conv_layers = num_conv_layers\n",
    "\n",
    "        # --- Convolutional Layers ---\n",
    "        # Erstellen einer flexiblen Liste von Conv-Layern\n",
    "        self.convs = nn.ModuleList()\n",
    "        in_channels = 3  # RGB-Bilder haben 3 Kanäle\n",
    "        out_channels_list = [10, 20, 30, 40, 50]  # Ausgabekanäle pro Layer\n",
    "        kernel_sizes = [5, 5, 3, 3, 3]           # Kernelgrößen pro Layer\n",
    "\n",
    "        for i in range(num_conv_layers):\n",
    "            # Conv Layer erstellen und zur Liste hinzufügen\n",
    "            self.convs.append(nn.Conv2d(in_channels, out_channels_list[i], kernel_size=kernel_sizes[i]))\n",
    "            in_channels = out_channels_list[i]\n",
    "\n",
    "        # Dropout Layer nach dem letzten Conv\n",
    "        self.dropout = nn.Dropout2d(p=dropout_rate)\n",
    "\n",
    "        # --- Dummy Forward Pass ---\n",
    "        # Berechnung der Flatten-Dimension automatisch\n",
    "        dummy_input = torch.zeros(1, 3, 32, 32)  # 1 Dummy-Bild\n",
    "        x = dummy_input\n",
    "        for i, conv in enumerate(self.convs):\n",
    "            x = F.relu(conv(x))\n",
    "            # MaxPooling nach Layer 2 und 4 (optional)\n",
    "            if i in [1, 3]:\n",
    "                x = F.max_pool2d(x, 2)\n",
    "        self.flatten_dim = x.numel()  # Anzahl der Features nach Flatten\n",
    "\n",
    "        # --- Fully Connected Layers ---\n",
    "        self.fc1 = nn.Linear(self.flatten_dim, fc_hidden)  # Hidden Layer\n",
    "        self.fc2 = nn.Linear(fc_hidden, 10)               # Output Layer: 10 Klassen (Ziffern)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # --- Convolutional Forward Pass ---\n",
    "        for i, conv in enumerate(self.convs):\n",
    "            x = F.relu(conv(x))\n",
    "            if i in [1, 3]:\n",
    "                x = F.max_pool2d(x, 2)  # Max-Pooling nach Layer 2 und 4\n",
    "\n",
    "        # Dropout anwenden\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Flatten für Fully Connected Layer\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        # Fully Connected Layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        # Log-Softmax für NLLLoss\n",
    "        return F.log_softmax(x, dim=1)\n"
   ],
   "id": "4da8accf2635af6e",
   "outputs": [],
   "execution_count": 58
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T20:43:01.503203Z",
     "start_time": "2025-11-29T20:43:01.497749Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def run_training(model, optimizer, train_loader, test_loader, n_epochs=3, exp_info=\"\"):\n",
    "    \"\"\"\n",
    "    Führt Training + Testen über mehrere Epochen durch.\n",
    "    Gibt zurück:\n",
    "      - das trainierte Modell\n",
    "      - train_loss pro Epoche\n",
    "      - test_loss pro Epoche\n",
    "      - train_accuracy pro Epoche\n",
    "      - test_accuracy pro Epoche\n",
    "    \"\"\"\n",
    "    train_losses, test_losses = [], []\n",
    "    train_accuracies, test_accuracies = [], []\n",
    "\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        # ===== Training =====\n",
    "        model.train()\n",
    "        correct_train = 0\n",
    "        train_loss_epoch = 0\n",
    "\n",
    "        for data, target in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = F.nll_loss(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss_epoch += loss.item() * data.size(0)\n",
    "            pred = output.argmax(dim=1)\n",
    "            correct_train += pred.eq(target).sum().item()\n",
    "\n",
    "        train_loss_epoch /= len(train_loader.dataset)\n",
    "        train_acc_epoch = 100. * correct_train / len(train_loader.dataset)\n",
    "\n",
    "        train_losses.append(train_loss_epoch)\n",
    "        train_accuracies.append(train_acc_epoch)\n",
    "\n",
    "        # ===== Test =====\n",
    "        model.eval()\n",
    "        test_loss_epoch = 0\n",
    "        correct_test = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for data, target in test_loader:\n",
    "                output = model(data)\n",
    "                test_loss_epoch += F.nll_loss(output, target, reduction='sum').item()\n",
    "                pred = output.argmax(dim=1)\n",
    "                correct_test += pred.eq(target).sum().item()\n",
    "\n",
    "        test_loss_epoch /= len(test_loader.dataset)\n",
    "        test_acc_epoch = 100. * correct_test / len(test_loader.dataset)\n",
    "\n",
    "        test_losses.append(test_loss_epoch)\n",
    "        test_accuracies.append(test_acc_epoch)\n",
    "\n",
    "        # Ausgabe\n",
    "        prefix = f\"[{exp_info}] \" if exp_info else \"\"\n",
    "        print(f\"{prefix}Epoch {epoch} | \"\n",
    "              f\"Train Acc: {train_acc_epoch:.2f}% | Train Loss: {train_loss_epoch:.4f} | \"\n",
    "              f\"Test Acc: {test_acc_epoch:.2f}% | Test Loss: {test_loss_epoch:.4f}\")\n",
    "\n",
    "    # Alles zurückgeben, inklusive trainiertem Modell\n",
    "    return model, train_losses, test_losses, train_accuracies, test_accuracies\n"
   ],
   "id": "895c8763a9cfb382",
   "outputs": [],
   "execution_count": 121
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T20:43:07.205176Z",
     "start_time": "2025-11-29T20:43:07.192078Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ALL_MODELS = {}      # speichert die trainierten Modelle\n",
    "ALL_RESULTS = {}     # speichert Losses & Accuracies pro Modell\n",
    "EPOCHS = 10\n",
    "learning_rates = [0.2, 0.1, 0.01]\n",
    "batch_sizes = [32, 64, 128, 256]\n"
   ],
   "id": "2d8b3b899fe47c20",
   "outputs": [],
   "execution_count": 122
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 1 Layer",
   "id": "b855459468f6f726"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T21:06:14.637131Z",
     "start_time": "2025-11-29T20:43:33.533412Z"
    }
   },
   "cell_type": "code",
   "source": [
    "NUM_CONV_LAYERS = 1\n",
    "for lr in learning_rates:\n",
    "    for bs in batch_sizes:\n",
    "        print(f\"\\n===== Training MODEL: L={NUM_CONV_LAYERS}, LR={lr}, B={bs} =====\")\n",
    "\n",
    "        train_loader_exp = DataLoader(train_dataset, batch_size=bs, shuffle=True)\n",
    "\n",
    "        model = SVHNC5NN(num_conv_layers=NUM_CONV_LAYERS)\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.5)\n",
    "\n",
    "        exp_name = f\"L{NUM_CONV_LAYERS}_LR{lr}_B{bs}\"\n",
    "\n",
    "        trained_model, train_loss, test_loss, train_acc, test_acc = run_training(\n",
    "            model, optimizer, train_loader_exp, test_loader,\n",
    "            n_epochs=EPOCHS,\n",
    "            exp_info=exp_name\n",
    "        )\n",
    "\n",
    "        # Modelle & Ergebnisse speichern\n",
    "        ALL_MODELS[exp_name] = trained_model\n",
    "        ALL_RESULTS[exp_name] = {\n",
    "            \"train_loss\": train_loss,\n",
    "            \"test_loss\": test_loss,\n",
    "            \"train_acc\": train_acc,\n",
    "            \"test_acc\": test_acc,\n",
    "            \"layers\": NUM_CONV_LAYERS,\n",
    "            \"lr\": lr,\n",
    "            \"batch\": bs\n",
    "        }\n",
    "\n",
    "print(f\"\\n=== FERTIG: {NUM_CONV_LAYERS} Convolutional Layer ===\")\n"
   ],
   "id": "29d061b41b68b9ef",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Training MODEL: L=1, LR=0.2, B=32 =====\n",
      "[L1_LR0.2_B32] Epoch 1 | Train Acc: 18.85% | Train Loss: 2.2398 | Test Acc: 19.59% | Test Loss: 2.2253\n",
      "[L1_LR0.2_B32] Epoch 2 | Train Acc: 18.89% | Train Loss: 2.2393 | Test Acc: 19.59% | Test Loss: 2.2265\n",
      "[L1_LR0.2_B32] Epoch 3 | Train Acc: 18.87% | Train Loss: 2.2392 | Test Acc: 19.59% | Test Loss: 2.2248\n",
      "[L1_LR0.2_B32] Epoch 4 | Train Acc: 18.91% | Train Loss: 2.2392 | Test Acc: 19.59% | Test Loss: 2.2274\n",
      "[L1_LR0.2_B32] Epoch 5 | Train Acc: 18.89% | Train Loss: 2.2392 | Test Acc: 19.59% | Test Loss: 2.2280\n",
      "[L1_LR0.2_B32] Epoch 6 | Train Acc: 18.87% | Train Loss: 2.2392 | Test Acc: 19.59% | Test Loss: 2.2263\n",
      "[L1_LR0.2_B32] Epoch 7 | Train Acc: 18.85% | Train Loss: 2.2392 | Test Acc: 19.59% | Test Loss: 2.2271\n",
      "[L1_LR0.2_B32] Epoch 8 | Train Acc: 18.89% | Train Loss: 2.2393 | Test Acc: 19.59% | Test Loss: 2.2305\n",
      "[L1_LR0.2_B32] Epoch 9 | Train Acc: 18.87% | Train Loss: 2.2392 | Test Acc: 19.59% | Test Loss: 2.2264\n",
      "[L1_LR0.2_B32] Epoch 10 | Train Acc: 18.90% | Train Loss: 2.2392 | Test Acc: 19.59% | Test Loss: 2.2301\n",
      "\n",
      "===== Training MODEL: L=1, LR=0.2, B=64 =====\n",
      "[L1_LR0.2_B64] Epoch 1 | Train Acc: 27.43% | Train Loss: 2.0526 | Test Acc: 52.07% | Test Loss: 1.4836\n",
      "[L1_LR0.2_B64] Epoch 2 | Train Acc: 44.82% | Train Loss: 1.6564 | Test Acc: 38.26% | Test Loss: 1.8555\n",
      "[L1_LR0.2_B64] Epoch 3 | Train Acc: 42.40% | Train Loss: 1.7122 | Test Acc: 55.32% | Test Loss: 1.4449\n",
      "[L1_LR0.2_B64] Epoch 4 | Train Acc: 44.68% | Train Loss: 1.6595 | Test Acc: 53.80% | Test Loss: 1.4740\n",
      "[L1_LR0.2_B64] Epoch 5 | Train Acc: 45.44% | Train Loss: 1.6401 | Test Acc: 57.99% | Test Loss: 1.3869\n",
      "[L1_LR0.2_B64] Epoch 6 | Train Acc: 45.11% | Train Loss: 1.6436 | Test Acc: 49.57% | Test Loss: 1.6231\n",
      "[L1_LR0.2_B64] Epoch 7 | Train Acc: 47.12% | Train Loss: 1.6040 | Test Acc: 61.20% | Test Loss: 1.2995\n",
      "[L1_LR0.2_B64] Epoch 8 | Train Acc: 48.00% | Train Loss: 1.5755 | Test Acc: 48.00% | Test Loss: 1.6200\n",
      "[L1_LR0.2_B64] Epoch 9 | Train Acc: 50.38% | Train Loss: 1.5265 | Test Acc: 51.66% | Test Loss: 1.5065\n",
      "[L1_LR0.2_B64] Epoch 10 | Train Acc: 47.16% | Train Loss: 1.5960 | Test Acc: 49.00% | Test Loss: 1.5911\n",
      "\n",
      "===== Training MODEL: L=1, LR=0.2, B=128 =====\n",
      "[L1_LR0.2_B128] Epoch 1 | Train Acc: 27.98% | Train Loss: 2.0439 | Test Acc: 55.43% | Test Loss: 1.4275\n",
      "[L1_LR0.2_B128] Epoch 2 | Train Acc: 45.67% | Train Loss: 1.6147 | Test Acc: 49.44% | Test Loss: 1.5771\n",
      "[L1_LR0.2_B128] Epoch 3 | Train Acc: 49.08% | Train Loss: 1.5179 | Test Acc: 54.51% | Test Loss: 1.4696\n",
      "[L1_LR0.2_B128] Epoch 4 | Train Acc: 59.77% | Train Loss: 1.2598 | Test Acc: 64.07% | Test Loss: 1.1801\n",
      "[L1_LR0.2_B128] Epoch 5 | Train Acc: 62.42% | Train Loss: 1.1805 | Test Acc: 68.51% | Test Loss: 1.1026\n",
      "[L1_LR0.2_B128] Epoch 6 | Train Acc: 63.64% | Train Loss: 1.1412 | Test Acc: 68.37% | Test Loss: 1.0790\n",
      "[L1_LR0.2_B128] Epoch 7 | Train Acc: 64.23% | Train Loss: 1.1241 | Test Acc: 66.28% | Test Loss: 1.1216\n",
      "[L1_LR0.2_B128] Epoch 8 | Train Acc: 64.99% | Train Loss: 1.1010 | Test Acc: 71.30% | Test Loss: 0.9868\n",
      "[L1_LR0.2_B128] Epoch 9 | Train Acc: 65.44% | Train Loss: 1.0907 | Test Acc: 67.56% | Test Loss: 1.0857\n",
      "[L1_LR0.2_B128] Epoch 10 | Train Acc: 66.13% | Train Loss: 1.0631 | Test Acc: 60.92% | Test Loss: 1.3096\n",
      "\n",
      "===== Training MODEL: L=1, LR=0.2, B=256 =====\n",
      "[L1_LR0.2_B256] Epoch 1 | Train Acc: 37.98% | Train Loss: 1.7953 | Test Acc: 58.64% | Test Loss: 1.2974\n",
      "[L1_LR0.2_B256] Epoch 2 | Train Acc: 62.67% | Train Loss: 1.1722 | Test Acc: 65.70% | Test Loss: 1.1226\n",
      "[L1_LR0.2_B256] Epoch 3 | Train Acc: 66.81% | Train Loss: 1.0573 | Test Acc: 67.85% | Test Loss: 1.0675\n",
      "[L1_LR0.2_B256] Epoch 4 | Train Acc: 68.68% | Train Loss: 1.0002 | Test Acc: 66.38% | Test Loss: 1.1286\n",
      "[L1_LR0.2_B256] Epoch 5 | Train Acc: 69.95% | Train Loss: 0.9668 | Test Acc: 69.10% | Test Loss: 1.0378\n",
      "[L1_LR0.2_B256] Epoch 6 | Train Acc: 72.48% | Train Loss: 0.8993 | Test Acc: 73.43% | Test Loss: 0.9124\n",
      "[L1_LR0.2_B256] Epoch 7 | Train Acc: 74.14% | Train Loss: 0.8568 | Test Acc: 74.19% | Test Loss: 0.9289\n",
      "[L1_LR0.2_B256] Epoch 8 | Train Acc: 75.49% | Train Loss: 0.8157 | Test Acc: 73.46% | Test Loss: 0.9214\n",
      "[L1_LR0.2_B256] Epoch 9 | Train Acc: 76.41% | Train Loss: 0.7842 | Test Acc: 70.78% | Test Loss: 1.0397\n",
      "[L1_LR0.2_B256] Epoch 10 | Train Acc: 76.34% | Train Loss: 0.7876 | Test Acc: 70.19% | Test Loss: 1.0379\n",
      "\n",
      "===== Training MODEL: L=1, LR=0.1, B=32 =====\n",
      "[L1_LR0.1_B32] Epoch 1 | Train Acc: 33.40% | Train Loss: 1.8990 | Test Acc: 51.23% | Test Loss: 1.5312\n",
      "[L1_LR0.1_B32] Epoch 2 | Train Acc: 34.47% | Train Loss: 1.8830 | Test Acc: 19.60% | Test Loss: 2.2259\n",
      "[L1_LR0.1_B32] Epoch 3 | Train Acc: 40.04% | Train Loss: 1.7424 | Test Acc: 60.97% | Test Loss: 1.2767\n",
      "[L1_LR0.1_B32] Epoch 4 | Train Acc: 47.38% | Train Loss: 1.5727 | Test Acc: 55.16% | Test Loss: 1.4934\n",
      "[L1_LR0.1_B32] Epoch 5 | Train Acc: 47.81% | Train Loss: 1.5621 | Test Acc: 44.36% | Test Loss: 1.6950\n",
      "[L1_LR0.1_B32] Epoch 6 | Train Acc: 44.91% | Train Loss: 1.6383 | Test Acc: 59.62% | Test Loss: 1.3294\n",
      "[L1_LR0.1_B32] Epoch 7 | Train Acc: 57.11% | Train Loss: 1.3372 | Test Acc: 63.78% | Test Loss: 1.2160\n",
      "[L1_LR0.1_B32] Epoch 8 | Train Acc: 58.55% | Train Loss: 1.3048 | Test Acc: 64.08% | Test Loss: 1.1926\n",
      "[L1_LR0.1_B32] Epoch 9 | Train Acc: 59.00% | Train Loss: 1.2910 | Test Acc: 68.26% | Test Loss: 1.1038\n",
      "[L1_LR0.1_B32] Epoch 10 | Train Acc: 56.92% | Train Loss: 1.3426 | Test Acc: 52.58% | Test Loss: 1.4966\n",
      "\n",
      "===== Training MODEL: L=1, LR=0.1, B=64 =====\n",
      "[L1_LR0.1_B64] Epoch 1 | Train Acc: 50.82% | Train Loss: 1.4760 | Test Acc: 65.93% | Test Loss: 1.1166\n",
      "[L1_LR0.1_B64] Epoch 2 | Train Acc: 67.58% | Train Loss: 1.0450 | Test Acc: 65.10% | Test Loss: 1.1432\n",
      "[L1_LR0.1_B64] Epoch 3 | Train Acc: 69.45% | Train Loss: 0.9867 | Test Acc: 75.27% | Test Loss: 0.8630\n",
      "[L1_LR0.1_B64] Epoch 4 | Train Acc: 70.92% | Train Loss: 0.9482 | Test Acc: 76.12% | Test Loss: 0.8404\n",
      "[L1_LR0.1_B64] Epoch 5 | Train Acc: 71.51% | Train Loss: 0.9229 | Test Acc: 75.16% | Test Loss: 0.8577\n",
      "[L1_LR0.1_B64] Epoch 6 | Train Acc: 72.34% | Train Loss: 0.9033 | Test Acc: 74.74% | Test Loss: 0.9104\n",
      "[L1_LR0.1_B64] Epoch 7 | Train Acc: 73.26% | Train Loss: 0.8767 | Test Acc: 75.37% | Test Loss: 0.8761\n",
      "[L1_LR0.1_B64] Epoch 8 | Train Acc: 73.87% | Train Loss: 0.8602 | Test Acc: 75.61% | Test Loss: 0.8661\n",
      "[L1_LR0.1_B64] Epoch 9 | Train Acc: 74.69% | Train Loss: 0.8323 | Test Acc: 75.31% | Test Loss: 0.8693\n",
      "[L1_LR0.1_B64] Epoch 10 | Train Acc: 74.49% | Train Loss: 0.8366 | Test Acc: 72.24% | Test Loss: 0.9627\n",
      "\n",
      "===== Training MODEL: L=1, LR=0.1, B=128 =====\n",
      "[L1_LR0.1_B128] Epoch 1 | Train Acc: 34.80% | Train Loss: 1.8778 | Test Acc: 65.25% | Test Loss: 1.1475\n",
      "[L1_LR0.1_B128] Epoch 2 | Train Acc: 64.00% | Train Loss: 1.1356 | Test Acc: 71.09% | Test Loss: 0.9701\n",
      "[L1_LR0.1_B128] Epoch 3 | Train Acc: 69.44% | Train Loss: 0.9777 | Test Acc: 74.52% | Test Loss: 0.8896\n",
      "[L1_LR0.1_B128] Epoch 4 | Train Acc: 73.78% | Train Loss: 0.8667 | Test Acc: 76.38% | Test Loss: 0.8257\n",
      "[L1_LR0.1_B128] Epoch 5 | Train Acc: 76.30% | Train Loss: 0.7923 | Test Acc: 79.64% | Test Loss: 0.7376\n",
      "[L1_LR0.1_B128] Epoch 6 | Train Acc: 77.74% | Train Loss: 0.7418 | Test Acc: 79.81% | Test Loss: 0.7342\n",
      "[L1_LR0.1_B128] Epoch 7 | Train Acc: 78.76% | Train Loss: 0.7096 | Test Acc: 79.58% | Test Loss: 0.7445\n",
      "[L1_LR0.1_B128] Epoch 8 | Train Acc: 79.58% | Train Loss: 0.6815 | Test Acc: 79.92% | Test Loss: 0.7262\n",
      "[L1_LR0.1_B128] Epoch 9 | Train Acc: 80.05% | Train Loss: 0.6705 | Test Acc: 78.80% | Test Loss: 0.7707\n",
      "[L1_LR0.1_B128] Epoch 10 | Train Acc: 80.36% | Train Loss: 0.6591 | Test Acc: 80.45% | Test Loss: 0.7268\n",
      "\n",
      "===== Training MODEL: L=1, LR=0.1, B=256 =====\n",
      "[L1_LR0.1_B256] Epoch 1 | Train Acc: 33.94% | Train Loss: 1.9099 | Test Acc: 58.04% | Test Loss: 1.3346\n",
      "[L1_LR0.1_B256] Epoch 2 | Train Acc: 69.18% | Train Loss: 1.0250 | Test Acc: 73.88% | Test Loss: 0.9011\n",
      "[L1_LR0.1_B256] Epoch 3 | Train Acc: 74.86% | Train Loss: 0.8534 | Test Acc: 75.58% | Test Loss: 0.8424\n",
      "[L1_LR0.1_B256] Epoch 4 | Train Acc: 76.94% | Train Loss: 0.7829 | Test Acc: 70.66% | Test Loss: 1.0243\n",
      "[L1_LR0.1_B256] Epoch 5 | Train Acc: 78.39% | Train Loss: 0.7387 | Test Acc: 76.73% | Test Loss: 0.8087\n",
      "[L1_LR0.1_B256] Epoch 6 | Train Acc: 79.37% | Train Loss: 0.6985 | Test Acc: 78.74% | Test Loss: 0.7559\n",
      "[L1_LR0.1_B256] Epoch 7 | Train Acc: 80.20% | Train Loss: 0.6713 | Test Acc: 76.52% | Test Loss: 0.8225\n",
      "[L1_LR0.1_B256] Epoch 8 | Train Acc: 80.81% | Train Loss: 0.6554 | Test Acc: 80.17% | Test Loss: 0.7203\n",
      "[L1_LR0.1_B256] Epoch 9 | Train Acc: 81.39% | Train Loss: 0.6360 | Test Acc: 79.56% | Test Loss: 0.7358\n",
      "[L1_LR0.1_B256] Epoch 10 | Train Acc: 81.69% | Train Loss: 0.6209 | Test Acc: 77.81% | Test Loss: 0.7954\n",
      "\n",
      "===== Training MODEL: L=1, LR=0.01, B=32 =====\n",
      "[L1_LR0.01_B32] Epoch 1 | Train Acc: 32.79% | Train Loss: 1.9343 | Test Acc: 61.85% | Test Loss: 1.2748\n",
      "[L1_LR0.01_B32] Epoch 2 | Train Acc: 71.40% | Train Loss: 0.9634 | Test Acc: 75.54% | Test Loss: 0.8416\n",
      "[L1_LR0.01_B32] Epoch 3 | Train Acc: 77.08% | Train Loss: 0.7864 | Test Acc: 78.83% | Test Loss: 0.7428\n",
      "[L1_LR0.01_B32] Epoch 4 | Train Acc: 79.21% | Train Loss: 0.7085 | Test Acc: 80.04% | Test Loss: 0.7071\n",
      "[L1_LR0.01_B32] Epoch 5 | Train Acc: 80.86% | Train Loss: 0.6639 | Test Acc: 81.21% | Test Loss: 0.6687\n",
      "[L1_LR0.01_B32] Epoch 6 | Train Acc: 81.85% | Train Loss: 0.6275 | Test Acc: 81.50% | Test Loss: 0.6674\n",
      "[L1_LR0.01_B32] Epoch 7 | Train Acc: 82.61% | Train Loss: 0.6038 | Test Acc: 82.28% | Test Loss: 0.6440\n",
      "[L1_LR0.01_B32] Epoch 8 | Train Acc: 83.21% | Train Loss: 0.5817 | Test Acc: 82.24% | Test Loss: 0.6521\n",
      "[L1_LR0.01_B32] Epoch 9 | Train Acc: 83.69% | Train Loss: 0.5657 | Test Acc: 82.84% | Test Loss: 0.6389\n",
      "[L1_LR0.01_B32] Epoch 10 | Train Acc: 83.93% | Train Loss: 0.5528 | Test Acc: 82.26% | Test Loss: 0.6599\n",
      "\n",
      "===== Training MODEL: L=1, LR=0.01, B=64 =====\n",
      "[L1_LR0.01_B64] Epoch 1 | Train Acc: 18.77% | Train Loss: 2.2367 | Test Acc: 19.69% | Test Loss: 2.1992\n",
      "[L1_LR0.01_B64] Epoch 2 | Train Acc: 44.24% | Train Loss: 1.6901 | Test Acc: 64.83% | Test Loss: 1.1851\n",
      "[L1_LR0.01_B64] Epoch 3 | Train Acc: 69.70% | Train Loss: 1.0236 | Test Acc: 73.20% | Test Loss: 0.9084\n",
      "[L1_LR0.01_B64] Epoch 4 | Train Acc: 75.82% | Train Loss: 0.8267 | Test Acc: 77.59% | Test Loss: 0.7852\n",
      "[L1_LR0.01_B64] Epoch 5 | Train Acc: 78.98% | Train Loss: 0.7282 | Test Acc: 79.56% | Test Loss: 0.7312\n",
      "[L1_LR0.01_B64] Epoch 6 | Train Acc: 80.79% | Train Loss: 0.6709 | Test Acc: 80.04% | Test Loss: 0.7160\n",
      "[L1_LR0.01_B64] Epoch 7 | Train Acc: 81.71% | Train Loss: 0.6323 | Test Acc: 81.27% | Test Loss: 0.6857\n",
      "[L1_LR0.01_B64] Epoch 8 | Train Acc: 82.67% | Train Loss: 0.6041 | Test Acc: 81.71% | Test Loss: 0.6712\n",
      "[L1_LR0.01_B64] Epoch 9 | Train Acc: 83.34% | Train Loss: 0.5848 | Test Acc: 81.34% | Test Loss: 0.6837\n",
      "[L1_LR0.01_B64] Epoch 10 | Train Acc: 83.77% | Train Loss: 0.5672 | Test Acc: 82.21% | Test Loss: 0.6543\n",
      "\n",
      "===== Training MODEL: L=1, LR=0.01, B=128 =====\n",
      "[L1_LR0.01_B128] Epoch 1 | Train Acc: 18.92% | Train Loss: 2.2449 | Test Acc: 19.59% | Test Loss: 2.2221\n",
      "[L1_LR0.01_B128] Epoch 2 | Train Acc: 20.19% | Train Loss: 2.2154 | Test Acc: 21.55% | Test Loss: 2.1585\n",
      "[L1_LR0.01_B128] Epoch 3 | Train Acc: 34.16% | Train Loss: 1.9534 | Test Acc: 42.34% | Test Loss: 1.6573\n",
      "[L1_LR0.01_B128] Epoch 4 | Train Acc: 59.05% | Train Loss: 1.3326 | Test Acc: 66.57% | Test Loss: 1.1105\n",
      "[L1_LR0.01_B128] Epoch 5 | Train Acc: 68.58% | Train Loss: 1.0637 | Test Acc: 70.45% | Test Loss: 0.9865\n",
      "[L1_LR0.01_B128] Epoch 6 | Train Acc: 72.23% | Train Loss: 0.9519 | Test Acc: 74.17% | Test Loss: 0.8869\n",
      "[L1_LR0.01_B128] Epoch 7 | Train Acc: 74.59% | Train Loss: 0.8800 | Test Acc: 74.93% | Test Loss: 0.8580\n",
      "[L1_LR0.01_B128] Epoch 8 | Train Acc: 75.98% | Train Loss: 0.8304 | Test Acc: 76.31% | Test Loss: 0.8168\n",
      "[L1_LR0.01_B128] Epoch 9 | Train Acc: 77.40% | Train Loss: 0.7855 | Test Acc: 77.62% | Test Loss: 0.7893\n",
      "[L1_LR0.01_B128] Epoch 10 | Train Acc: 78.54% | Train Loss: 0.7514 | Test Acc: 77.60% | Test Loss: 0.7760\n",
      "\n",
      "===== Training MODEL: L=1, LR=0.01, B=256 =====\n",
      "[L1_LR0.01_B256] Epoch 1 | Train Acc: 18.75% | Train Loss: 2.2458 | Test Acc: 19.59% | Test Loss: 2.2283\n",
      "[L1_LR0.01_B256] Epoch 2 | Train Acc: 18.92% | Train Loss: 2.2341 | Test Acc: 19.59% | Test Loss: 2.2171\n",
      "[L1_LR0.01_B256] Epoch 3 | Train Acc: 19.18% | Train Loss: 2.2092 | Test Acc: 20.28% | Test Loss: 2.1671\n",
      "[L1_LR0.01_B256] Epoch 4 | Train Acc: 26.51% | Train Loss: 2.0727 | Test Acc: 27.09% | Test Loss: 2.0270\n",
      "[L1_LR0.01_B256] Epoch 5 | Train Acc: 39.37% | Train Loss: 1.7969 | Test Acc: 49.29% | Test Loss: 1.6355\n",
      "[L1_LR0.01_B256] Epoch 6 | Train Acc: 52.29% | Train Loss: 1.5014 | Test Acc: 54.78% | Test Loss: 1.4319\n",
      "[L1_LR0.01_B256] Epoch 7 | Train Acc: 60.84% | Train Loss: 1.2912 | Test Acc: 62.20% | Test Loss: 1.2428\n",
      "[L1_LR0.01_B256] Epoch 8 | Train Acc: 65.40% | Train Loss: 1.1653 | Test Acc: 61.44% | Test Loss: 1.2694\n",
      "[L1_LR0.01_B256] Epoch 9 | Train Acc: 68.37% | Train Loss: 1.0854 | Test Acc: 63.94% | Test Loss: 1.2062\n",
      "[L1_LR0.01_B256] Epoch 10 | Train Acc: 70.12% | Train Loss: 1.0251 | Test Acc: 64.85% | Test Loss: 1.1526\n",
      "\n",
      "=== FERTIG: 1 Convolutional Layer ===\n"
     ]
    }
   ],
   "execution_count": 123
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 2 Layer",
   "id": "935c6ef8a36ac765"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T22:13:14.545459Z",
     "start_time": "2025-11-29T21:07:15.317412Z"
    }
   },
   "cell_type": "code",
   "source": [
    "NUM_CONV_LAYERS = 2\n",
    "for lr in learning_rates:\n",
    "    for bs in batch_sizes:\n",
    "        print(f\"\\n===== Training MODEL: L={NUM_CONV_LAYERS}, LR={lr}, B={bs} =====\")\n",
    "\n",
    "        train_loader_exp = DataLoader(train_dataset, batch_size=bs, shuffle=True)\n",
    "\n",
    "        model = SVHNC5NN(num_conv_layers=NUM_CONV_LAYERS)\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.5)\n",
    "\n",
    "        exp_name = f\"L{NUM_CONV_LAYERS}_LR{lr}_B{bs}\"\n",
    "\n",
    "        trained_model, train_loss, test_loss, train_acc, test_acc = run_training(\n",
    "            model, optimizer, train_loader_exp, test_loader,\n",
    "            n_epochs=EPOCHS,\n",
    "            exp_info=exp_name\n",
    "        )\n",
    "\n",
    "        # Modelle & Ergebnisse speichern\n",
    "        ALL_MODELS[exp_name] = trained_model\n",
    "        ALL_RESULTS[exp_name] = {\n",
    "            \"train_loss\": train_loss,\n",
    "            \"test_loss\": test_loss,\n",
    "            \"train_acc\": train_acc,\n",
    "            \"test_acc\": test_acc,\n",
    "            \"layers\": NUM_CONV_LAYERS,\n",
    "            \"lr\": lr,\n",
    "            \"batch\": bs\n",
    "        }\n",
    "\n",
    "print(f\"\\n=== FERTIG: {NUM_CONV_LAYERS} Convolutional Layer ===\")\n"
   ],
   "id": "3256cbaca5d26c5b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Training MODEL: L=2, LR=0.2, B=32 =====\n",
      "[L2_LR0.2_B32] Epoch 1 | Train Acc: 18.88% | Train Loss: 2.2395 | Test Acc: 19.59% | Test Loss: 2.2279\n",
      "[L2_LR0.2_B32] Epoch 2 | Train Acc: 18.87% | Train Loss: 2.2389 | Test Acc: 19.59% | Test Loss: 2.2248\n",
      "[L2_LR0.2_B32] Epoch 3 | Train Acc: 18.93% | Train Loss: 2.2388 | Test Acc: 19.59% | Test Loss: 2.2262\n",
      "[L2_LR0.2_B32] Epoch 4 | Train Acc: 18.86% | Train Loss: 2.2393 | Test Acc: 19.59% | Test Loss: 2.2294\n",
      "[L2_LR0.2_B32] Epoch 5 | Train Acc: 18.88% | Train Loss: 2.2391 | Test Acc: 19.59% | Test Loss: 2.2277\n",
      "[L2_LR0.2_B32] Epoch 6 | Train Acc: 18.88% | Train Loss: 2.2394 | Test Acc: 19.59% | Test Loss: 2.2239\n",
      "[L2_LR0.2_B32] Epoch 7 | Train Acc: 18.90% | Train Loss: 2.2393 | Test Acc: 19.59% | Test Loss: 2.2256\n",
      "[L2_LR0.2_B32] Epoch 8 | Train Acc: 18.89% | Train Loss: 2.2393 | Test Acc: 19.59% | Test Loss: 2.2247\n",
      "[L2_LR0.2_B32] Epoch 9 | Train Acc: 18.99% | Train Loss: 2.2368 | Test Acc: 19.59% | Test Loss: 2.2267\n",
      "[L2_LR0.2_B32] Epoch 10 | Train Acc: 18.88% | Train Loss: 2.2392 | Test Acc: 19.59% | Test Loss: 2.2266\n",
      "\n",
      "===== Training MODEL: L=2, LR=0.2, B=64 =====\n",
      "[L2_LR0.2_B64] Epoch 1 | Train Acc: 38.77% | Train Loss: 1.7679 | Test Acc: 67.21% | Test Loss: 1.0702\n",
      "[L2_LR0.2_B64] Epoch 2 | Train Acc: 66.28% | Train Loss: 1.0780 | Test Acc: 68.22% | Test Loss: 1.0240\n",
      "[L2_LR0.2_B64] Epoch 3 | Train Acc: 70.52% | Train Loss: 0.9569 | Test Acc: 68.19% | Test Loss: 1.0308\n",
      "[L2_LR0.2_B64] Epoch 4 | Train Acc: 72.20% | Train Loss: 0.9088 | Test Acc: 74.37% | Test Loss: 0.8338\n",
      "[L2_LR0.2_B64] Epoch 5 | Train Acc: 72.79% | Train Loss: 0.8910 | Test Acc: 73.82% | Test Loss: 0.8865\n",
      "[L2_LR0.2_B64] Epoch 6 | Train Acc: 73.45% | Train Loss: 0.8624 | Test Acc: 74.10% | Test Loss: 0.8444\n",
      "[L2_LR0.2_B64] Epoch 7 | Train Acc: 74.47% | Train Loss: 0.8399 | Test Acc: 75.05% | Test Loss: 0.8382\n",
      "[L2_LR0.2_B64] Epoch 8 | Train Acc: 75.81% | Train Loss: 0.8017 | Test Acc: 75.86% | Test Loss: 0.8165\n",
      "[L2_LR0.2_B64] Epoch 9 | Train Acc: 75.95% | Train Loss: 0.7989 | Test Acc: 78.53% | Test Loss: 0.7373\n",
      "[L2_LR0.2_B64] Epoch 10 | Train Acc: 76.56% | Train Loss: 0.7747 | Test Acc: 76.66% | Test Loss: 0.8056\n",
      "\n",
      "===== Training MODEL: L=2, LR=0.2, B=128 =====\n",
      "[L2_LR0.2_B128] Epoch 1 | Train Acc: 43.59% | Train Loss: 1.6406 | Test Acc: 73.49% | Test Loss: 0.8934\n",
      "[L2_LR0.2_B128] Epoch 2 | Train Acc: 76.86% | Train Loss: 0.7685 | Test Acc: 76.84% | Test Loss: 0.8024\n",
      "[L2_LR0.2_B128] Epoch 3 | Train Acc: 80.97% | Train Loss: 0.6419 | Test Acc: 82.16% | Test Loss: 0.6326\n",
      "[L2_LR0.2_B128] Epoch 4 | Train Acc: 82.81% | Train Loss: 0.5743 | Test Acc: 75.27% | Test Loss: 0.8411\n",
      "[L2_LR0.2_B128] Epoch 5 | Train Acc: 84.03% | Train Loss: 0.5328 | Test Acc: 84.37% | Test Loss: 0.5606\n",
      "[L2_LR0.2_B128] Epoch 6 | Train Acc: 84.71% | Train Loss: 0.5112 | Test Acc: 84.43% | Test Loss: 0.5674\n",
      "[L2_LR0.2_B128] Epoch 7 | Train Acc: 85.41% | Train Loss: 0.4877 | Test Acc: 84.47% | Test Loss: 0.5654\n",
      "[L2_LR0.2_B128] Epoch 8 | Train Acc: 85.74% | Train Loss: 0.4799 | Test Acc: 83.96% | Test Loss: 0.5769\n",
      "[L2_LR0.2_B128] Epoch 9 | Train Acc: 86.61% | Train Loss: 0.4488 | Test Acc: 84.95% | Test Loss: 0.5624\n",
      "[L2_LR0.2_B128] Epoch 10 | Train Acc: 86.16% | Train Loss: 0.4549 | Test Acc: 83.71% | Test Loss: 0.5708\n",
      "\n",
      "===== Training MODEL: L=2, LR=0.2, B=256 =====\n",
      "[L2_LR0.2_B256] Epoch 1 | Train Acc: 21.76% | Train Loss: 2.1736 | Test Acc: 42.71% | Test Loss: 1.8631\n",
      "[L2_LR0.2_B256] Epoch 2 | Train Acc: 68.95% | Train Loss: 0.9891 | Test Acc: 74.43% | Test Loss: 0.8558\n",
      "[L2_LR0.2_B256] Epoch 3 | Train Acc: 81.15% | Train Loss: 0.6324 | Test Acc: 78.04% | Test Loss: 0.7437\n",
      "[L2_LR0.2_B256] Epoch 4 | Train Acc: 83.85% | Train Loss: 0.5391 | Test Acc: 82.54% | Test Loss: 0.6176\n",
      "[L2_LR0.2_B256] Epoch 5 | Train Acc: 85.65% | Train Loss: 0.4905 | Test Acc: 85.38% | Test Loss: 0.5166\n",
      "[L2_LR0.2_B256] Epoch 6 | Train Acc: 86.19% | Train Loss: 0.4623 | Test Acc: 73.41% | Test Loss: 0.8463\n",
      "[L2_LR0.2_B256] Epoch 7 | Train Acc: 86.97% | Train Loss: 0.4341 | Test Acc: 84.90% | Test Loss: 0.5306\n",
      "[L2_LR0.2_B256] Epoch 8 | Train Acc: 87.84% | Train Loss: 0.4077 | Test Acc: 86.54% | Test Loss: 0.5020\n",
      "[L2_LR0.2_B256] Epoch 9 | Train Acc: 88.23% | Train Loss: 0.3935 | Test Acc: 86.95% | Test Loss: 0.4683\n",
      "[L2_LR0.2_B256] Epoch 10 | Train Acc: 88.39% | Train Loss: 0.3796 | Test Acc: 86.51% | Test Loss: 0.4958\n",
      "\n",
      "===== Training MODEL: L=2, LR=0.1, B=32 =====\n",
      "[L2_LR0.1_B32] Epoch 1 | Train Acc: 51.67% | Train Loss: 1.4451 | Test Acc: 64.99% | Test Loss: 1.1932\n",
      "[L2_LR0.1_B32] Epoch 2 | Train Acc: 70.99% | Train Loss: 0.9418 | Test Acc: 56.14% | Test Loss: 1.3725\n",
      "[L2_LR0.1_B32] Epoch 3 | Train Acc: 73.25% | Train Loss: 0.8661 | Test Acc: 76.65% | Test Loss: 0.7821\n",
      "[L2_LR0.1_B32] Epoch 4 | Train Acc: 75.84% | Train Loss: 0.7914 | Test Acc: 77.45% | Test Loss: 0.7710\n",
      "[L2_LR0.1_B32] Epoch 5 | Train Acc: 77.37% | Train Loss: 0.7402 | Test Acc: 79.30% | Test Loss: 0.7192\n",
      "[L2_LR0.1_B32] Epoch 6 | Train Acc: 78.27% | Train Loss: 0.7129 | Test Acc: 77.87% | Test Loss: 0.7612\n",
      "[L2_LR0.1_B32] Epoch 7 | Train Acc: 79.08% | Train Loss: 0.6930 | Test Acc: 73.04% | Test Loss: 0.8791\n",
      "[L2_LR0.1_B32] Epoch 8 | Train Acc: 79.86% | Train Loss: 0.6748 | Test Acc: 80.89% | Test Loss: 0.6632\n",
      "[L2_LR0.1_B32] Epoch 9 | Train Acc: 79.88% | Train Loss: 0.6705 | Test Acc: 81.10% | Test Loss: 0.7185\n",
      "[L2_LR0.1_B32] Epoch 10 | Train Acc: 80.39% | Train Loss: 0.6561 | Test Acc: 80.74% | Test Loss: 0.6880\n",
      "\n",
      "===== Training MODEL: L=2, LR=0.1, B=64 =====\n",
      "[L2_LR0.1_B64] Epoch 1 | Train Acc: 46.31% | Train Loss: 1.5592 | Test Acc: 78.56% | Test Loss: 0.7371\n",
      "[L2_LR0.1_B64] Epoch 2 | Train Acc: 80.59% | Train Loss: 0.6552 | Test Acc: 80.65% | Test Loss: 0.6734\n",
      "[L2_LR0.1_B64] Epoch 3 | Train Acc: 83.73% | Train Loss: 0.5469 | Test Acc: 83.06% | Test Loss: 0.5930\n",
      "[L2_LR0.1_B64] Epoch 4 | Train Acc: 85.20% | Train Loss: 0.4949 | Test Acc: 85.97% | Test Loss: 0.5035\n",
      "[L2_LR0.1_B64] Epoch 5 | Train Acc: 86.16% | Train Loss: 0.4651 | Test Acc: 84.28% | Test Loss: 0.5479\n",
      "[L2_LR0.1_B64] Epoch 6 | Train Acc: 86.70% | Train Loss: 0.4451 | Test Acc: 82.80% | Test Loss: 0.5771\n",
      "[L2_LR0.1_B64] Epoch 7 | Train Acc: 87.10% | Train Loss: 0.4248 | Test Acc: 86.40% | Test Loss: 0.4939\n",
      "[L2_LR0.1_B64] Epoch 8 | Train Acc: 87.74% | Train Loss: 0.4088 | Test Acc: 86.70% | Test Loss: 0.4906\n",
      "[L2_LR0.1_B64] Epoch 9 | Train Acc: 87.88% | Train Loss: 0.4009 | Test Acc: 87.33% | Test Loss: 0.4823\n",
      "[L2_LR0.1_B64] Epoch 10 | Train Acc: 88.25% | Train Loss: 0.3878 | Test Acc: 86.49% | Test Loss: 0.5114\n",
      "\n",
      "===== Training MODEL: L=2, LR=0.1, B=128 =====\n",
      "[L2_LR0.1_B128] Epoch 1 | Train Acc: 18.83% | Train Loss: 2.2393 | Test Acc: 19.59% | Test Loss: 2.2244\n",
      "[L2_LR0.1_B128] Epoch 2 | Train Acc: 33.76% | Train Loss: 1.8912 | Test Acc: 70.04% | Test Loss: 1.0279\n",
      "[L2_LR0.1_B128] Epoch 3 | Train Acc: 75.38% | Train Loss: 0.8126 | Test Acc: 78.08% | Test Loss: 0.7437\n",
      "[L2_LR0.1_B128] Epoch 4 | Train Acc: 79.81% | Train Loss: 0.6670 | Test Acc: 81.19% | Test Loss: 0.6495\n",
      "[L2_LR0.1_B128] Epoch 5 | Train Acc: 82.36% | Train Loss: 0.5924 | Test Acc: 82.69% | Test Loss: 0.6261\n",
      "[L2_LR0.1_B128] Epoch 6 | Train Acc: 83.93% | Train Loss: 0.5447 | Test Acc: 82.43% | Test Loss: 0.6249\n",
      "[L2_LR0.1_B128] Epoch 7 | Train Acc: 84.82% | Train Loss: 0.5102 | Test Acc: 83.94% | Test Loss: 0.5677\n",
      "[L2_LR0.1_B128] Epoch 8 | Train Acc: 85.61% | Train Loss: 0.4847 | Test Acc: 83.78% | Test Loss: 0.5721\n",
      "[L2_LR0.1_B128] Epoch 9 | Train Acc: 86.12% | Train Loss: 0.4654 | Test Acc: 85.11% | Test Loss: 0.5346\n",
      "[L2_LR0.1_B128] Epoch 10 | Train Acc: 86.74% | Train Loss: 0.4450 | Test Acc: 84.89% | Test Loss: 0.5454\n",
      "\n",
      "===== Training MODEL: L=2, LR=0.1, B=256 =====\n",
      "[L2_LR0.1_B256] Epoch 1 | Train Acc: 21.63% | Train Loss: 2.1786 | Test Acc: 48.89% | Test Loss: 1.5818\n",
      "[L2_LR0.1_B256] Epoch 2 | Train Acc: 71.26% | Train Loss: 0.9429 | Test Acc: 80.50% | Test Loss: 0.6954\n",
      "[L2_LR0.1_B256] Epoch 3 | Train Acc: 82.04% | Train Loss: 0.6138 | Test Acc: 82.38% | Test Loss: 0.6319\n",
      "[L2_LR0.1_B256] Epoch 4 | Train Acc: 84.66% | Train Loss: 0.5198 | Test Acc: 82.63% | Test Loss: 0.6075\n",
      "[L2_LR0.1_B256] Epoch 5 | Train Acc: 86.60% | Train Loss: 0.4589 | Test Acc: 86.48% | Test Loss: 0.4879\n",
      "[L2_LR0.1_B256] Epoch 6 | Train Acc: 87.61% | Train Loss: 0.4231 | Test Acc: 84.66% | Test Loss: 0.5195\n",
      "[L2_LR0.1_B256] Epoch 7 | Train Acc: 88.15% | Train Loss: 0.3999 | Test Acc: 84.82% | Test Loss: 0.5187\n",
      "[L2_LR0.1_B256] Epoch 8 | Train Acc: 88.94% | Train Loss: 0.3754 | Test Acc: 87.80% | Test Loss: 0.4258\n",
      "[L2_LR0.1_B256] Epoch 9 | Train Acc: 89.50% | Train Loss: 0.3570 | Test Acc: 86.74% | Test Loss: 0.4577\n",
      "[L2_LR0.1_B256] Epoch 10 | Train Acc: 89.94% | Train Loss: 0.3405 | Test Acc: 86.95% | Test Loss: 0.4638\n",
      "\n",
      "===== Training MODEL: L=2, LR=0.01, B=32 =====\n",
      "[L2_LR0.01_B32] Epoch 1 | Train Acc: 23.56% | Train Loss: 2.1396 | Test Acc: 51.81% | Test Loss: 1.5563\n",
      "[L2_LR0.01_B32] Epoch 2 | Train Acc: 67.77% | Train Loss: 1.0599 | Test Acc: 80.43% | Test Loss: 0.7013\n",
      "[L2_LR0.01_B32] Epoch 3 | Train Acc: 81.80% | Train Loss: 0.6251 | Test Acc: 83.59% | Test Loss: 0.5599\n",
      "[L2_LR0.01_B32] Epoch 4 | Train Acc: 84.03% | Train Loss: 0.5395 | Test Acc: 85.18% | Test Loss: 0.5108\n",
      "[L2_LR0.01_B32] Epoch 5 | Train Acc: 85.59% | Train Loss: 0.4848 | Test Acc: 85.63% | Test Loss: 0.4794\n",
      "[L2_LR0.01_B32] Epoch 6 | Train Acc: 86.64% | Train Loss: 0.4494 | Test Acc: 86.92% | Test Loss: 0.4644\n",
      "[L2_LR0.01_B32] Epoch 7 | Train Acc: 87.47% | Train Loss: 0.4217 | Test Acc: 87.43% | Test Loss: 0.4310\n",
      "[L2_LR0.01_B32] Epoch 8 | Train Acc: 88.18% | Train Loss: 0.3942 | Test Acc: 87.88% | Test Loss: 0.4211\n",
      "[L2_LR0.01_B32] Epoch 9 | Train Acc: 88.69% | Train Loss: 0.3772 | Test Acc: 87.90% | Test Loss: 0.4229\n",
      "[L2_LR0.01_B32] Epoch 10 | Train Acc: 89.15% | Train Loss: 0.3635 | Test Acc: 87.40% | Test Loss: 0.4461\n",
      "\n",
      "===== Training MODEL: L=2, LR=0.01, B=64 =====\n",
      "[L2_LR0.01_B64] Epoch 1 | Train Acc: 18.87% | Train Loss: 2.2405 | Test Acc: 19.59% | Test Loss: 2.2191\n",
      "[L2_LR0.01_B64] Epoch 2 | Train Acc: 39.57% | Train Loss: 1.7740 | Test Acc: 70.34% | Test Loss: 1.0403\n",
      "[L2_LR0.01_B64] Epoch 3 | Train Acc: 74.82% | Train Loss: 0.8697 | Test Acc: 79.82% | Test Loss: 0.7315\n",
      "[L2_LR0.01_B64] Epoch 4 | Train Acc: 80.77% | Train Loss: 0.6727 | Test Acc: 82.35% | Test Loss: 0.6397\n",
      "[L2_LR0.01_B64] Epoch 5 | Train Acc: 83.08% | Train Loss: 0.5929 | Test Acc: 84.12% | Test Loss: 0.5694\n",
      "[L2_LR0.01_B64] Epoch 6 | Train Acc: 84.53% | Train Loss: 0.5419 | Test Acc: 84.78% | Test Loss: 0.5397\n",
      "[L2_LR0.01_B64] Epoch 7 | Train Acc: 85.46% | Train Loss: 0.5027 | Test Acc: 85.05% | Test Loss: 0.5255\n",
      "[L2_LR0.01_B64] Epoch 8 | Train Acc: 86.18% | Train Loss: 0.4750 | Test Acc: 85.55% | Test Loss: 0.5106\n",
      "[L2_LR0.01_B64] Epoch 9 | Train Acc: 86.73% | Train Loss: 0.4546 | Test Acc: 86.05% | Test Loss: 0.4877\n",
      "[L2_LR0.01_B64] Epoch 10 | Train Acc: 87.30% | Train Loss: 0.4334 | Test Acc: 86.07% | Test Loss: 0.4859\n",
      "\n",
      "===== Training MODEL: L=2, LR=0.01, B=128 =====\n",
      "[L2_LR0.01_B128] Epoch 1 | Train Acc: 18.47% | Train Loss: 2.2451 | Test Acc: 19.59% | Test Loss: 2.2261\n",
      "[L2_LR0.01_B128] Epoch 2 | Train Acc: 18.92% | Train Loss: 2.2361 | Test Acc: 19.59% | Test Loss: 2.2191\n",
      "[L2_LR0.01_B128] Epoch 3 | Train Acc: 20.21% | Train Loss: 2.2021 | Test Acc: 26.32% | Test Loss: 2.0920\n",
      "[L2_LR0.01_B128] Epoch 4 | Train Acc: 44.58% | Train Loss: 1.6791 | Test Acc: 65.19% | Test Loss: 1.1699\n",
      "[L2_LR0.01_B128] Epoch 5 | Train Acc: 70.30% | Train Loss: 1.0012 | Test Acc: 75.66% | Test Loss: 0.8544\n",
      "[L2_LR0.01_B128] Epoch 6 | Train Acc: 77.12% | Train Loss: 0.7927 | Test Acc: 79.75% | Test Loss: 0.7129\n",
      "[L2_LR0.01_B128] Epoch 7 | Train Acc: 80.40% | Train Loss: 0.6837 | Test Acc: 82.60% | Test Loss: 0.6290\n",
      "[L2_LR0.01_B128] Epoch 8 | Train Acc: 82.18% | Train Loss: 0.6199 | Test Acc: 83.36% | Test Loss: 0.5969\n",
      "[L2_LR0.01_B128] Epoch 9 | Train Acc: 83.52% | Train Loss: 0.5792 | Test Acc: 84.22% | Test Loss: 0.5685\n",
      "[L2_LR0.01_B128] Epoch 10 | Train Acc: 84.19% | Train Loss: 0.5504 | Test Acc: 85.27% | Test Loss: 0.5354\n",
      "\n",
      "===== Training MODEL: L=2, LR=0.01, B=256 =====\n",
      "[L2_LR0.01_B256] Epoch 1 | Train Acc: 17.47% | Train Loss: 2.2550 | Test Acc: 19.59% | Test Loss: 2.2296\n",
      "[L2_LR0.01_B256] Epoch 2 | Train Acc: 18.92% | Train Loss: 2.2402 | Test Acc: 19.59% | Test Loss: 2.2275\n",
      "[L2_LR0.01_B256] Epoch 3 | Train Acc: 18.92% | Train Loss: 2.2372 | Test Acc: 19.59% | Test Loss: 2.2224\n",
      "[L2_LR0.01_B256] Epoch 4 | Train Acc: 18.99% | Train Loss: 2.2320 | Test Acc: 19.76% | Test Loss: 2.2129\n",
      "[L2_LR0.01_B256] Epoch 5 | Train Acc: 20.71% | Train Loss: 2.2110 | Test Acc: 23.84% | Test Loss: 2.1640\n",
      "[L2_LR0.01_B256] Epoch 6 | Train Acc: 30.63% | Train Loss: 2.0359 | Test Acc: 40.31% | Test Loss: 1.7808\n",
      "[L2_LR0.01_B256] Epoch 7 | Train Acc: 54.63% | Train Loss: 1.4528 | Test Acc: 66.19% | Test Loss: 1.1287\n",
      "[L2_LR0.01_B256] Epoch 8 | Train Acc: 70.84% | Train Loss: 1.0027 | Test Acc: 75.75% | Test Loss: 0.8763\n",
      "[L2_LR0.01_B256] Epoch 9 | Train Acc: 76.03% | Train Loss: 0.8357 | Test Acc: 76.96% | Test Loss: 0.7943\n",
      "[L2_LR0.01_B256] Epoch 10 | Train Acc: 78.87% | Train Loss: 0.7500 | Test Acc: 80.64% | Test Loss: 0.7166\n",
      "\n",
      "=== FERTIG: 2 Convolutional Layer ===\n"
     ]
    }
   ],
   "execution_count": 124
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3 Layer",
   "id": "f186c23a96c07532"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T23:30:51.371094Z",
     "start_time": "2025-11-29T22:14:12.310724Z"
    }
   },
   "cell_type": "code",
   "source": [
    "NUM_CONV_LAYERS = 3\n",
    "for lr in learning_rates:\n",
    "    for bs in batch_sizes:\n",
    "        print(f\"\\n===== Training MODEL: L={NUM_CONV_LAYERS}, LR={lr}, B={bs} =====\")\n",
    "\n",
    "        train_loader_exp = DataLoader(train_dataset, batch_size=bs, shuffle=True)\n",
    "\n",
    "        model = SVHNC5NN(num_conv_layers=NUM_CONV_LAYERS)\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.5)\n",
    "\n",
    "        exp_name = f\"L{NUM_CONV_LAYERS}_LR{lr}_B{bs}\"\n",
    "\n",
    "        trained_model, train_loss, test_loss, train_acc, test_acc = run_training(\n",
    "            model, optimizer, train_loader_exp, test_loader,\n",
    "            n_epochs=EPOCHS,\n",
    "            exp_info=exp_name\n",
    "        )\n",
    "\n",
    "        # Modelle & Ergebnisse speichern\n",
    "        ALL_MODELS[exp_name] = trained_model\n",
    "        ALL_RESULTS[exp_name] = {\n",
    "            \"train_loss\": train_loss,\n",
    "            \"test_loss\": test_loss,\n",
    "            \"train_acc\": train_acc,\n",
    "            \"test_acc\": test_acc,\n",
    "            \"layers\": NUM_CONV_LAYERS,\n",
    "            \"lr\": lr,\n",
    "            \"batch\": bs\n",
    "        }\n",
    "\n",
    "print(f\"\\n=== FERTIG: {NUM_CONV_LAYERS} Convolutional Layer ===\")\n"
   ],
   "id": "20f4c7bbdcfcc561",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Training MODEL: L=3, LR=0.2, B=32 =====\n",
      "[L3_LR0.2_B32] Epoch 1 | Train Acc: 21.64% | Train Loss: 2.1801 | Test Acc: 19.59% | Test Loss: 2.2267\n",
      "[L3_LR0.2_B32] Epoch 2 | Train Acc: 18.85% | Train Loss: 2.2393 | Test Acc: 19.59% | Test Loss: 2.2288\n",
      "[L3_LR0.2_B32] Epoch 3 | Train Acc: 18.90% | Train Loss: 2.2395 | Test Acc: 19.59% | Test Loss: 2.2282\n",
      "[L3_LR0.2_B32] Epoch 4 | Train Acc: 18.90% | Train Loss: 2.2393 | Test Acc: 19.59% | Test Loss: 2.2265\n",
      "[L3_LR0.2_B32] Epoch 5 | Train Acc: 18.88% | Train Loss: 2.2394 | Test Acc: 19.59% | Test Loss: 2.2254\n",
      "[L3_LR0.2_B32] Epoch 6 | Train Acc: 18.91% | Train Loss: 2.2393 | Test Acc: 19.59% | Test Loss: 2.2290\n",
      "[L3_LR0.2_B32] Epoch 7 | Train Acc: 18.88% | Train Loss: 2.2390 | Test Acc: 19.59% | Test Loss: 2.2248\n",
      "[L3_LR0.2_B32] Epoch 8 | Train Acc: 18.88% | Train Loss: 2.2393 | Test Acc: 19.59% | Test Loss: 2.2289\n",
      "[L3_LR0.2_B32] Epoch 9 | Train Acc: 18.89% | Train Loss: 2.2390 | Test Acc: 19.59% | Test Loss: 2.2272\n",
      "[L3_LR0.2_B32] Epoch 10 | Train Acc: 18.87% | Train Loss: 2.2392 | Test Acc: 19.59% | Test Loss: 2.2267\n",
      "\n",
      "===== Training MODEL: L=3, LR=0.2, B=64 =====\n",
      "[L3_LR0.2_B64] Epoch 1 | Train Acc: 48.84% | Train Loss: 1.4996 | Test Acc: 74.93% | Test Loss: 0.8770\n",
      "[L3_LR0.2_B64] Epoch 2 | Train Acc: 75.08% | Train Loss: 0.8208 | Test Acc: 78.61% | Test Loss: 0.7528\n",
      "[L3_LR0.2_B64] Epoch 3 | Train Acc: 77.28% | Train Loss: 0.7557 | Test Acc: 79.17% | Test Loss: 0.7321\n",
      "[L3_LR0.2_B64] Epoch 4 | Train Acc: 78.09% | Train Loss: 0.7217 | Test Acc: 81.29% | Test Loss: 0.6785\n",
      "[L3_LR0.2_B64] Epoch 5 | Train Acc: 79.09% | Train Loss: 0.6998 | Test Acc: 79.49% | Test Loss: 0.7112\n",
      "[L3_LR0.2_B64] Epoch 6 | Train Acc: 78.87% | Train Loss: 0.7034 | Test Acc: 80.72% | Test Loss: 0.6799\n",
      "[L3_LR0.2_B64] Epoch 7 | Train Acc: 78.81% | Train Loss: 0.7015 | Test Acc: 78.57% | Test Loss: 0.7581\n",
      "[L3_LR0.2_B64] Epoch 8 | Train Acc: 79.45% | Train Loss: 0.6878 | Test Acc: 80.17% | Test Loss: 0.6897\n",
      "[L3_LR0.2_B64] Epoch 9 | Train Acc: 79.56% | Train Loss: 0.6815 | Test Acc: 78.47% | Test Loss: 0.7413\n",
      "[L3_LR0.2_B64] Epoch 10 | Train Acc: 80.15% | Train Loss: 0.6618 | Test Acc: 64.25% | Test Loss: 1.1496\n",
      "\n",
      "===== Training MODEL: L=3, LR=0.2, B=128 =====\n",
      "[L3_LR0.2_B128] Epoch 1 | Train Acc: 38.84% | Train Loss: 1.7566 | Test Acc: 74.29% | Test Loss: 0.8672\n",
      "[L3_LR0.2_B128] Epoch 2 | Train Acc: 75.40% | Train Loss: 0.7982 | Test Acc: 80.37% | Test Loss: 0.6648\n",
      "[L3_LR0.2_B128] Epoch 3 | Train Acc: 79.78% | Train Loss: 0.6612 | Test Acc: 75.12% | Test Loss: 0.7780\n",
      "[L3_LR0.2_B128] Epoch 4 | Train Acc: 79.34% | Train Loss: 0.6647 | Test Acc: 19.59% | Test Loss: 2.2325\n",
      "[L3_LR0.2_B128] Epoch 5 | Train Acc: 18.87% | Train Loss: 2.2392 | Test Acc: 19.59% | Test Loss: 2.2244\n",
      "[L3_LR0.2_B128] Epoch 6 | Train Acc: 18.92% | Train Loss: 2.2371 | Test Acc: 19.59% | Test Loss: 2.2266\n",
      "[L3_LR0.2_B128] Epoch 7 | Train Acc: 18.92% | Train Loss: 2.2371 | Test Acc: 19.59% | Test Loss: 2.2264\n",
      "[L3_LR0.2_B128] Epoch 8 | Train Acc: 18.93% | Train Loss: 2.2372 | Test Acc: 19.59% | Test Loss: 2.2246\n",
      "[L3_LR0.2_B128] Epoch 9 | Train Acc: 18.92% | Train Loss: 2.2372 | Test Acc: 19.59% | Test Loss: 2.2247\n",
      "[L3_LR0.2_B128] Epoch 10 | Train Acc: 18.92% | Train Loss: 2.2372 | Test Acc: 19.59% | Test Loss: 2.2241\n",
      "\n",
      "===== Training MODEL: L=3, LR=0.2, B=256 =====\n",
      "[L3_LR0.2_B256] Epoch 1 | Train Acc: 26.60% | Train Loss: 2.0552 | Test Acc: 54.06% | Test Loss: 1.3824\n",
      "[L3_LR0.2_B256] Epoch 2 | Train Acc: 72.42% | Train Loss: 0.8931 | Test Acc: 76.36% | Test Loss: 0.7583\n",
      "[L3_LR0.2_B256] Epoch 3 | Train Acc: 53.04% | Train Loss: 1.3665 | Test Acc: 19.59% | Test Loss: 2.2241\n",
      "[L3_LR0.2_B256] Epoch 4 | Train Acc: 18.89% | Train Loss: 2.2369 | Test Acc: 19.59% | Test Loss: 2.2246\n",
      "[L3_LR0.2_B256] Epoch 5 | Train Acc: 18.90% | Train Loss: 2.2369 | Test Acc: 19.59% | Test Loss: 2.2243\n",
      "[L3_LR0.2_B256] Epoch 6 | Train Acc: 18.90% | Train Loss: 2.2368 | Test Acc: 19.59% | Test Loss: 2.2246\n",
      "[L3_LR0.2_B256] Epoch 7 | Train Acc: 18.92% | Train Loss: 2.2369 | Test Acc: 19.59% | Test Loss: 2.2243\n",
      "[L3_LR0.2_B256] Epoch 8 | Train Acc: 18.92% | Train Loss: 2.2369 | Test Acc: 19.59% | Test Loss: 2.2242\n",
      "[L3_LR0.2_B256] Epoch 9 | Train Acc: 18.92% | Train Loss: 2.2368 | Test Acc: 19.59% | Test Loss: 2.2241\n",
      "[L3_LR0.2_B256] Epoch 10 | Train Acc: 18.92% | Train Loss: 2.2369 | Test Acc: 19.59% | Test Loss: 2.2248\n",
      "\n",
      "===== Training MODEL: L=3, LR=0.1, B=32 =====\n",
      "[L3_LR0.1_B32] Epoch 1 | Train Acc: 52.02% | Train Loss: 1.4111 | Test Acc: 71.41% | Test Loss: 0.9152\n",
      "[L3_LR0.1_B32] Epoch 2 | Train Acc: 77.05% | Train Loss: 0.7496 | Test Acc: 78.40% | Test Loss: 0.7611\n",
      "[L3_LR0.1_B32] Epoch 3 | Train Acc: 79.14% | Train Loss: 0.6892 | Test Acc: 73.21% | Test Loss: 0.8680\n",
      "[L3_LR0.1_B32] Epoch 4 | Train Acc: 79.98% | Train Loss: 0.6620 | Test Acc: 79.84% | Test Loss: 0.6848\n",
      "[L3_LR0.1_B32] Epoch 5 | Train Acc: 80.66% | Train Loss: 0.6422 | Test Acc: 81.28% | Test Loss: 0.6406\n",
      "[L3_LR0.1_B32] Epoch 6 | Train Acc: 81.05% | Train Loss: 0.6335 | Test Acc: 82.81% | Test Loss: 0.5987\n",
      "[L3_LR0.1_B32] Epoch 7 | Train Acc: 80.71% | Train Loss: 0.6407 | Test Acc: 80.88% | Test Loss: 0.6662\n",
      "[L3_LR0.1_B32] Epoch 8 | Train Acc: 81.31% | Train Loss: 0.6286 | Test Acc: 78.31% | Test Loss: 0.7616\n",
      "[L3_LR0.1_B32] Epoch 9 | Train Acc: 81.14% | Train Loss: 0.6307 | Test Acc: 82.16% | Test Loss: 0.6297\n",
      "[L3_LR0.1_B32] Epoch 10 | Train Acc: 80.65% | Train Loss: 0.6562 | Test Acc: 77.34% | Test Loss: 0.7739\n",
      "\n",
      "===== Training MODEL: L=3, LR=0.1, B=64 =====\n",
      "[L3_LR0.1_B64] Epoch 1 | Train Acc: 52.98% | Train Loss: 1.3828 | Test Acc: 77.24% | Test Loss: 0.7396\n",
      "[L3_LR0.1_B64] Epoch 2 | Train Acc: 69.36% | Train Loss: 0.9480 | Test Acc: 79.31% | Test Loss: 0.6699\n",
      "[L3_LR0.1_B64] Epoch 3 | Train Acc: 81.57% | Train Loss: 0.6072 | Test Acc: 82.66% | Test Loss: 0.5998\n",
      "[L3_LR0.1_B64] Epoch 4 | Train Acc: 83.55% | Train Loss: 0.5445 | Test Acc: 84.27% | Test Loss: 0.5270\n",
      "[L3_LR0.1_B64] Epoch 5 | Train Acc: 84.38% | Train Loss: 0.5183 | Test Acc: 84.83% | Test Loss: 0.5068\n",
      "[L3_LR0.1_B64] Epoch 6 | Train Acc: 85.11% | Train Loss: 0.4993 | Test Acc: 85.51% | Test Loss: 0.4850\n",
      "[L3_LR0.1_B64] Epoch 7 | Train Acc: 85.65% | Train Loss: 0.4771 | Test Acc: 85.75% | Test Loss: 0.4962\n",
      "[L3_LR0.1_B64] Epoch 8 | Train Acc: 86.09% | Train Loss: 0.4632 | Test Acc: 85.12% | Test Loss: 0.5093\n",
      "[L3_LR0.1_B64] Epoch 9 | Train Acc: 86.27% | Train Loss: 0.4527 | Test Acc: 85.64% | Test Loss: 0.5054\n",
      "[L3_LR0.1_B64] Epoch 10 | Train Acc: 86.58% | Train Loss: 0.4445 | Test Acc: 85.96% | Test Loss: 0.5011\n",
      "\n",
      "===== Training MODEL: L=3, LR=0.1, B=128 =====\n",
      "[L3_LR0.1_B128] Epoch 1 | Train Acc: 35.61% | Train Loss: 1.8334 | Test Acc: 74.41% | Test Loss: 0.8443\n",
      "[L3_LR0.1_B128] Epoch 2 | Train Acc: 82.65% | Train Loss: 0.5784 | Test Acc: 80.53% | Test Loss: 0.6420\n",
      "[L3_LR0.1_B128] Epoch 3 | Train Acc: 85.86% | Train Loss: 0.4722 | Test Acc: 85.69% | Test Loss: 0.4910\n",
      "[L3_LR0.1_B128] Epoch 4 | Train Acc: 87.27% | Train Loss: 0.4206 | Test Acc: 88.33% | Test Loss: 0.3961\n",
      "[L3_LR0.1_B128] Epoch 5 | Train Acc: 88.29% | Train Loss: 0.3895 | Test Acc: 87.32% | Test Loss: 0.4266\n",
      "[L3_LR0.1_B128] Epoch 6 | Train Acc: 89.03% | Train Loss: 0.3635 | Test Acc: 88.10% | Test Loss: 0.4022\n",
      "[L3_LR0.1_B128] Epoch 7 | Train Acc: 89.64% | Train Loss: 0.3422 | Test Acc: 88.64% | Test Loss: 0.3945\n",
      "[L3_LR0.1_B128] Epoch 8 | Train Acc: 90.26% | Train Loss: 0.3230 | Test Acc: 88.83% | Test Loss: 0.3976\n",
      "[L3_LR0.1_B128] Epoch 9 | Train Acc: 90.57% | Train Loss: 0.3136 | Test Acc: 88.83% | Test Loss: 0.3955\n",
      "[L3_LR0.1_B128] Epoch 10 | Train Acc: 90.79% | Train Loss: 0.3042 | Test Acc: 89.23% | Test Loss: 0.3797\n",
      "\n",
      "===== Training MODEL: L=3, LR=0.1, B=256 =====\n",
      "[L3_LR0.1_B256] Epoch 1 | Train Acc: 18.78% | Train Loss: 2.2401 | Test Acc: 19.59% | Test Loss: 2.2225\n",
      "[L3_LR0.1_B256] Epoch 2 | Train Acc: 23.03% | Train Loss: 2.1414 | Test Acc: 49.44% | Test Loss: 1.5607\n",
      "[L3_LR0.1_B256] Epoch 3 | Train Acc: 70.87% | Train Loss: 0.9569 | Test Acc: 78.79% | Test Loss: 0.7160\n",
      "[L3_LR0.1_B256] Epoch 4 | Train Acc: 83.45% | Train Loss: 0.5564 | Test Acc: 83.60% | Test Loss: 0.5403\n",
      "[L3_LR0.1_B256] Epoch 5 | Train Acc: 86.34% | Train Loss: 0.4584 | Test Acc: 80.02% | Test Loss: 0.6471\n",
      "[L3_LR0.1_B256] Epoch 6 | Train Acc: 87.89% | Train Loss: 0.4073 | Test Acc: 87.05% | Test Loss: 0.4463\n",
      "[L3_LR0.1_B256] Epoch 7 | Train Acc: 88.93% | Train Loss: 0.3710 | Test Acc: 85.78% | Test Loss: 0.4690\n",
      "[L3_LR0.1_B256] Epoch 8 | Train Acc: 89.63% | Train Loss: 0.3479 | Test Acc: 88.85% | Test Loss: 0.3937\n",
      "[L3_LR0.1_B256] Epoch 9 | Train Acc: 90.25% | Train Loss: 0.3279 | Test Acc: 88.96% | Test Loss: 0.3946\n",
      "[L3_LR0.1_B256] Epoch 10 | Train Acc: 90.78% | Train Loss: 0.3093 | Test Acc: 89.18% | Test Loss: 0.3807\n",
      "\n",
      "===== Training MODEL: L=3, LR=0.01, B=32 =====\n",
      "[L3_LR0.01_B32] Epoch 1 | Train Acc: 18.88% | Train Loss: 2.2394 | Test Acc: 19.59% | Test Loss: 2.2233\n",
      "[L3_LR0.01_B32] Epoch 2 | Train Acc: 45.98% | Train Loss: 1.5798 | Test Acc: 69.81% | Test Loss: 0.9698\n",
      "[L3_LR0.01_B32] Epoch 3 | Train Acc: 81.45% | Train Loss: 0.6187 | Test Acc: 84.11% | Test Loss: 0.5244\n",
      "[L3_LR0.01_B32] Epoch 4 | Train Acc: 86.01% | Train Loss: 0.4684 | Test Acc: 87.32% | Test Loss: 0.4260\n",
      "[L3_LR0.01_B32] Epoch 5 | Train Acc: 87.87% | Train Loss: 0.4101 | Test Acc: 87.99% | Test Loss: 0.4052\n",
      "[L3_LR0.01_B32] Epoch 6 | Train Acc: 88.78% | Train Loss: 0.3748 | Test Acc: 89.13% | Test Loss: 0.3756\n",
      "[L3_LR0.01_B32] Epoch 7 | Train Acc: 89.59% | Train Loss: 0.3484 | Test Acc: 89.69% | Test Loss: 0.3562\n",
      "[L3_LR0.01_B32] Epoch 8 | Train Acc: 90.07% | Train Loss: 0.3318 | Test Acc: 89.68% | Test Loss: 0.3618\n",
      "[L3_LR0.01_B32] Epoch 9 | Train Acc: 90.57% | Train Loss: 0.3152 | Test Acc: 89.89% | Test Loss: 0.3491\n",
      "[L3_LR0.01_B32] Epoch 10 | Train Acc: 91.07% | Train Loss: 0.3021 | Test Acc: 90.34% | Test Loss: 0.3397\n",
      "\n",
      "===== Training MODEL: L=3, LR=0.01, B=64 =====\n",
      "[L3_LR0.01_B64] Epoch 1 | Train Acc: 18.28% | Train Loss: 2.2456 | Test Acc: 19.59% | Test Loss: 2.2244\n",
      "[L3_LR0.01_B64] Epoch 2 | Train Acc: 18.92% | Train Loss: 2.2358 | Test Acc: 19.59% | Test Loss: 2.2200\n",
      "[L3_LR0.01_B64] Epoch 3 | Train Acc: 32.80% | Train Loss: 1.9146 | Test Acc: 64.17% | Test Loss: 1.1618\n",
      "[L3_LR0.01_B64] Epoch 4 | Train Acc: 73.92% | Train Loss: 0.8647 | Test Acc: 80.57% | Test Loss: 0.6664\n",
      "[L3_LR0.01_B64] Epoch 5 | Train Acc: 82.72% | Train Loss: 0.5856 | Test Acc: 84.14% | Test Loss: 0.5379\n",
      "[L3_LR0.01_B64] Epoch 6 | Train Acc: 85.36% | Train Loss: 0.4966 | Test Acc: 86.06% | Test Loss: 0.4731\n",
      "[L3_LR0.01_B64] Epoch 7 | Train Acc: 86.69% | Train Loss: 0.4478 | Test Acc: 86.97% | Test Loss: 0.4371\n",
      "[L3_LR0.01_B64] Epoch 8 | Train Acc: 87.82% | Train Loss: 0.4114 | Test Acc: 88.28% | Test Loss: 0.4024\n",
      "[L3_LR0.01_B64] Epoch 9 | Train Acc: 88.45% | Train Loss: 0.3868 | Test Acc: 88.63% | Test Loss: 0.3878\n",
      "[L3_LR0.01_B64] Epoch 10 | Train Acc: 89.16% | Train Loss: 0.3661 | Test Acc: 88.87% | Test Loss: 0.3800\n",
      "\n",
      "===== Training MODEL: L=3, LR=0.01, B=128 =====\n",
      "[L3_LR0.01_B128] Epoch 1 | Train Acc: 17.82% | Train Loss: 2.2526 | Test Acc: 19.59% | Test Loss: 2.2272\n",
      "[L3_LR0.01_B128] Epoch 2 | Train Acc: 18.92% | Train Loss: 2.2383 | Test Acc: 19.59% | Test Loss: 2.2264\n",
      "[L3_LR0.01_B128] Epoch 3 | Train Acc: 18.92% | Train Loss: 2.2375 | Test Acc: 19.59% | Test Loss: 2.2240\n",
      "[L3_LR0.01_B128] Epoch 4 | Train Acc: 18.92% | Train Loss: 2.2366 | Test Acc: 19.59% | Test Loss: 2.2233\n",
      "[L3_LR0.01_B128] Epoch 5 | Train Acc: 18.92% | Train Loss: 2.2336 | Test Acc: 19.59% | Test Loss: 2.2138\n",
      "[L3_LR0.01_B128] Epoch 6 | Train Acc: 25.16% | Train Loss: 2.1044 | Test Acc: 42.92% | Test Loss: 1.6814\n",
      "[L3_LR0.01_B128] Epoch 7 | Train Acc: 59.92% | Train Loss: 1.2731 | Test Acc: 69.69% | Test Loss: 1.0100\n",
      "[L3_LR0.01_B128] Epoch 8 | Train Acc: 73.55% | Train Loss: 0.8829 | Test Acc: 76.02% | Test Loss: 0.8331\n",
      "[L3_LR0.01_B128] Epoch 9 | Train Acc: 77.19% | Train Loss: 0.7664 | Test Acc: 78.94% | Test Loss: 0.7391\n",
      "[L3_LR0.01_B128] Epoch 10 | Train Acc: 79.76% | Train Loss: 0.6793 | Test Acc: 80.82% | Test Loss: 0.6693\n",
      "\n",
      "===== Training MODEL: L=3, LR=0.01, B=256 =====\n",
      "[L3_LR0.01_B256] Epoch 1 | Train Acc: 17.41% | Train Loss: 2.2568 | Test Acc: 19.59% | Test Loss: 2.2302\n",
      "[L3_LR0.01_B256] Epoch 2 | Train Acc: 18.92% | Train Loss: 2.2400 | Test Acc: 19.59% | Test Loss: 2.2269\n",
      "[L3_LR0.01_B256] Epoch 3 | Train Acc: 18.92% | Train Loss: 2.2386 | Test Acc: 19.59% | Test Loss: 2.2247\n",
      "[L3_LR0.01_B256] Epoch 4 | Train Acc: 18.92% | Train Loss: 2.2376 | Test Acc: 19.59% | Test Loss: 2.2254\n",
      "[L3_LR0.01_B256] Epoch 5 | Train Acc: 18.92% | Train Loss: 2.2363 | Test Acc: 19.59% | Test Loss: 2.2215\n",
      "[L3_LR0.01_B256] Epoch 6 | Train Acc: 18.92% | Train Loss: 2.2333 | Test Acc: 19.59% | Test Loss: 2.2178\n",
      "[L3_LR0.01_B256] Epoch 7 | Train Acc: 18.96% | Train Loss: 2.2237 | Test Acc: 19.66% | Test Loss: 2.1975\n",
      "[L3_LR0.01_B256] Epoch 8 | Train Acc: 23.27% | Train Loss: 2.1464 | Test Acc: 34.20% | Test Loss: 1.9811\n",
      "[L3_LR0.01_B256] Epoch 9 | Train Acc: 49.08% | Train Loss: 1.5823 | Test Acc: 62.06% | Test Loss: 1.2425\n",
      "[L3_LR0.01_B256] Epoch 10 | Train Acc: 69.75% | Train Loss: 1.0163 | Test Acc: 64.91% | Test Loss: 1.1739\n",
      "\n",
      "=== FERTIG: 3 Convolutional Layer ===\n"
     ]
    }
   ],
   "execution_count": 125
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 4 Layer",
   "id": "81ebb00bea9e9d73"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "NUM_CONV_LAYERS = 4\n",
    "for lr in learning_rates:\n",
    "    for bs in batch_sizes:\n",
    "        print(f\"\\n===== Training MODEL: L={NUM_CONV_LAYERS}, LR={lr}, B={bs} =====\")\n",
    "\n",
    "        train_loader_exp = DataLoader(train_dataset, batch_size=bs, shuffle=True)\n",
    "\n",
    "        model = SVHNC5NN(num_conv_layers=NUM_CONV_LAYERS)\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.5)\n",
    "\n",
    "        exp_name = f\"L{NUM_CONV_LAYERS}_LR{lr}_B{bs}\"\n",
    "\n",
    "        trained_model, train_loss, test_loss, train_acc, test_acc = run_training(\n",
    "            model, optimizer, train_loader_exp, test_loader,\n",
    "            n_epochs=EPOCHS,\n",
    "            exp_info=exp_name\n",
    "        )\n",
    "\n",
    "        # Modelle & Ergebnisse speichern\n",
    "        ALL_MODELS[exp_name] = trained_model\n",
    "        ALL_RESULTS[exp_name] = {\n",
    "            \"train_loss\": train_loss,\n",
    "            \"test_loss\": test_loss,\n",
    "            \"train_acc\": train_acc,\n",
    "            \"test_acc\": test_acc,\n",
    "            \"layers\": NUM_CONV_LAYERS,\n",
    "            \"lr\": lr,\n",
    "            \"batch\": bs\n",
    "        }\n",
    "\n",
    "print(f\"\\n=== FERTIG: {NUM_CONV_LAYERS} Convolutional Layer ===\")\n"
   ],
   "id": "8bbbcf734640a30f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 5 Layer",
   "id": "feba23719668f6a1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "NUM_CONV_LAYERS = 5\n",
    "for lr in learning_rates:\n",
    "    for bs in batch_sizes:\n",
    "        print(f\"\\n===== Training MODEL: L={NUM_CONV_LAYERS}, LR={lr}, B={bs} =====\")\n",
    "\n",
    "        train_loader_exp = DataLoader(train_dataset, batch_size=bs, shuffle=True)\n",
    "\n",
    "        model = SVHNC5NN(num_conv_layers=NUM_CONV_LAYERS)\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.5)\n",
    "\n",
    "        exp_name = f\"L{NUM_CONV_LAYERS}_LR{lr}_B{bs}\"\n",
    "\n",
    "        trained_model, train_loss, test_loss, train_acc, test_acc = run_training(\n",
    "            model, optimizer, train_loader_exp, test_loader,\n",
    "            n_epochs=EPOCHS,\n",
    "            exp_info=exp_name\n",
    "        )\n",
    "\n",
    "        # Modelle & Ergebnisse speichern\n",
    "        ALL_MODELS[exp_name] = trained_model\n",
    "        ALL_RESULTS[exp_name] = {\n",
    "            \"train_loss\": train_loss,\n",
    "            \"test_loss\": test_loss,\n",
    "            \"train_acc\": train_acc,\n",
    "            \"test_acc\": test_acc,\n",
    "            \"layers\": NUM_CONV_LAYERS,\n",
    "            \"lr\": lr,\n",
    "            \"batch\": bs\n",
    "        }\n",
    "\n",
    "print(f\"\\n=== FERTIG: {NUM_CONV_LAYERS} Convolutional Layer ===\")\n"
   ],
   "id": "6a01c2cab67e28e4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 6 Layer",
   "id": "f5a824f438092a33"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "NUM_CONV_LAYERS = 6\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for bs in batch_sizes:\n",
    "\n",
    "        print(f\"\\n===== Training MODEL: L={NUM_CONV_LAYERS}, LR={lr}, B={bs} =====\")\n",
    "\n",
    "        train_loader_exp = DataLoader(train_dataset, batch_size=bs, shuffle=True)\n",
    "\n",
    "        model = SVHNC5NN(num_conv_layers=NUM_CONV_LAYERS)\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.5)\n",
    "\n",
    "        exp_name = f\"L{NUM_CONV_LAYERS}_LR{lr}_B{bs}\"\n",
    "\n",
    "        trained_model, train_loss, test_loss, acc = run_training(\n",
    "            model, optimizer, train_loader_exp, test_loader,\n",
    "            n_epochs=EPOCHS,\n",
    "            exp_info=exp_name\n",
    "        )\n",
    "\n",
    "        # Modelle & Ergebnisse speichern\n",
    "        ALL_MODELS[exp_name] = trained_model\n",
    "        ALL_RESULTS[exp_name] = {\n",
    "            \"train_loss\": train_loss,\n",
    "            \"test_loss\": test_loss,\n",
    "            \"accuracy\": acc,\n",
    "            \"layers\": NUM_CONV_LAYERS,\n",
    "            \"lr\": lr,\n",
    "            \"batch\": bs\n",
    "        }\n",
    "\n",
    "print(f\"\\n=== FERTIG: {NUM_CONV_LAYERS} Convolutional Layers ===\")\n"
   ],
   "id": "d81bef52381b616e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Visualisierung\n",
    "\n",
    "-> Sollte man das experiment mit 15 epochen als experiment c einfügen, könnte man hier auch die überschrift ändern, z.b. als simpler name \"sehr große Experimente\" oder \"Experimente mit stark erhöhter Batchsizes und Epochen\" oder ein vorschlag von ChatGPT \"Trainingskonfiguration: Batchgröße und Epochen (Effekt)\""
   ],
   "id": "f9bef1135bd0c311"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "675750aefbc2dc37"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
