{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# SVHN Projektarbeit",
   "id": "cf13ccb0f5657075"
  },
  {
   "cell_type": "markdown",
   "id": "234fcf1afc74a162",
   "metadata": {},
   "source": [
    "# Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40074f299749155",
   "metadata": {},
   "source": [
    "## wichtige Pakete einlesen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f112f3bba625c2f5",
   "metadata": {},
   "source": "gegebenenfalls müssen die Pakete (v.a. im Jupyter Hub) zunächst installiert werden, bevor sie importiert werden können"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T12:49:11.642718Z",
     "start_time": "2025-11-28T12:49:10.018475Z"
    }
   },
   "cell_type": "code",
   "source": [
    "!pip install scipy\n",
    "!pip install torchvision\n",
    "!pip install torch\n",
    "!pip install matplotlib\n",
    "!pip install scikit-learn\n",
    "!pip install torchsummary\n",
    "!pip install pandas"
   ],
   "id": "549a97c30782ccf5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scipy in ./.venv/lib/python3.9/site-packages (1.13.1)\r\n",
      "Requirement already satisfied: numpy<2.3,>=1.22.4 in ./.venv/lib/python3.9/site-packages (from scipy) (2.0.2)\r\n",
      "Requirement already satisfied: torchvision in ./.venv/lib/python3.9/site-packages (0.23.0)\r\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.9/site-packages (from torchvision) (2.0.2)\r\n",
      "Requirement already satisfied: torch==2.8.0 in ./.venv/lib/python3.9/site-packages (from torchvision) (2.8.0)\r\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in ./.venv/lib/python3.9/site-packages (from torchvision) (11.3.0)\r\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.9/site-packages (from torch==2.8.0->torchvision) (3.19.1)\r\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in ./.venv/lib/python3.9/site-packages (from torch==2.8.0->torchvision) (4.15.0)\r\n",
      "Requirement already satisfied: sympy>=1.13.3 in ./.venv/lib/python3.9/site-packages (from torch==2.8.0->torchvision) (1.14.0)\r\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.9/site-packages (from torch==2.8.0->torchvision) (3.2.1)\r\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.9/site-packages (from torch==2.8.0->torchvision) (3.1.6)\r\n",
      "Requirement already satisfied: fsspec in ./.venv/lib/python3.9/site-packages (from torch==2.8.0->torchvision) (2025.10.0)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.9/site-packages (from sympy>=1.13.3->torch==2.8.0->torchvision) (1.3.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.9/site-packages (from jinja2->torch==2.8.0->torchvision) (3.0.3)\r\n",
      "Requirement already satisfied: torch in ./.venv/lib/python3.9/site-packages (2.8.0)\r\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.9/site-packages (from torch) (3.19.1)\r\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in ./.venv/lib/python3.9/site-packages (from torch) (4.15.0)\r\n",
      "Requirement already satisfied: sympy>=1.13.3 in ./.venv/lib/python3.9/site-packages (from torch) (1.14.0)\r\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.9/site-packages (from torch) (3.2.1)\r\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.9/site-packages (from torch) (3.1.6)\r\n",
      "Requirement already satisfied: fsspec in ./.venv/lib/python3.9/site-packages (from torch) (2025.10.0)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.9/site-packages (from sympy>=1.13.3->torch) (1.3.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.9/site-packages (from jinja2->torch) (3.0.3)\r\n"
     ]
    }
   ],
   "execution_count": 111
  },
  {
   "cell_type": "code",
   "id": "fbc121e30a2defb3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-26T09:26:07.394537Z",
     "start_time": "2025-11-26T09:26:07.391349Z"
    }
   },
   "source": [
    "import torch                # PyTorch: das Hauptframework für Deep Learning (Tensors, Autograd, Modelle, Training)\n",
    "import torchvision          # TorchVision: Bibliothek mit Standard-Datasets, Modellen und Bildtransformationen\n",
    "import torch.nn as nn       # nn = Neural Network Module: für das Erstellen von Layern (z.B. Conv2d, Linear, ReLU)\n",
    "import torch.nn.functional as F  # Funktionale API: für direkte Nutzung von Aktivierungsfunktionen, Losses, etc. (z.B. F.relu)\n",
    "import torch.optim as optim       # Optimizer (z.B. SGD, Adam) für das Training von Modellen\n",
    "import matplotlib.pyplot as plt   # Matplotlib: zum Plotten von Bildern, Trainingskurven oder Loss/Accuracy\n",
    "import random                     # Python Standardbibliothek: für zufällige Zahlen, Shuffling, Seed-Kontrolle\n",
    "import scipy.io                   # SciPy I/O: um .mat-Dateien (MATLAB-Format) zu laden, z.B. deine train/test Daten\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "                                  # Dataset: Basisklasse für eigene Datensätze\n",
    "                                  # DataLoader: erleichtert Batch-Verarbeitung, Shuffling und Parallelisierung beim Training\n",
    "import torchvision.transforms as transforms\n",
    "                                  # Transformations für Bilder: z.B. Normalisierung, Resizing, RandomCrop, ToTensor\n",
    "import os                         # Zugriff auf Betriebssystemfunktionen (Dateien, Verzeichnisse)\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from torchsummary import summary\n",
    "import pandas as pd\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Daten",
   "id": "f6a1edabc7d934fd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Daten über torchvision",
   "id": "4a239986587698a6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Für unseren Datensatz gibt es zwei Wege die Daten sinnvoll im Notebook zu hinterlegen. Tatsächlich gibt es diesen Datensatz im Paket torchvision und kann direkt per Code heruntergeladen werden. Das ist vor allem dann sinnvoll, wenn der Datensatz auf dem lokalen Gerät nicht zur Verfügung steht. Es bietet aber außerdem den Vorteil, dass man damit besser über github arbeiten kann, da dort die Daten über 100MB nicht hochgeladen werden können. Zur Vollständigkeit ist aber dennoch der Code zum einlesen der Daten darunter.",
   "id": "547250f3e7f3bdcd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-26T09:31:05.150467Z",
     "start_time": "2025-11-26T09:26:11.909185Z"
    }
   },
   "cell_type": "code",
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Train-Set\n",
    "train_dataset = datasets.SVHN(\n",
    "    root=\"data\",\n",
    "    split='train',      # 'train' = train_32x32.mat\n",
    "    download=True,      # lädt automatisch herunter, wenn nicht vorhanden\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "# Test-Set\n",
    "test_dataset = datasets.SVHN(\n",
    "    root=\"data\",\n",
    "    split='test',       # 'test' = test_32x32.mat\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader  = DataLoader(test_dataset,  batch_size=1000, shuffle=False)\n"
   ],
   "id": "2e3d8bb0352fee3e",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n",
      "100.0%\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Daten über seine eigene os",
   "id": "563d3745bb55d93a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "In dieser Zelle wird eine **PyTorch Dataset-Klasse** für den SVHN-Datensatz definiert.\n",
    "\n",
    "- **Zweck:** Ermöglicht es, `.mat`-Dateien mit Bildern und Labels einfach in PyTorch zu laden und in Batches zu verarbeiten.\n",
    "- **Funktionen:**\n",
    "  - `__init__`: Lädt die Daten aus der `.mat`-Datei und speichert optionale Transformationen.\n",
    "  - `__len__`: Gibt die Anzahl der Bilder zurück (wichtig für DataLoader).\n",
    "  - `__getitem__`: Liefert ein einzelnes Bild und Label als Tensor, normalisiert die Pixel und wendet Transformationen an.\n",
    "- **Besonderheiten:**\n",
    "  - Pixelwerte werden von [0,255] auf [0,1] normalisiert.\n",
    "  - SVHN Label „10“ wird in `0` umgewandelt.\n",
    "  - PyTorch erwartet die Kanal-Reihenfolge `(C,H,W)`, daher wird das Bild umgeformt.\n"
   ],
   "id": "672bdb5d0b72a3e0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T19:16:40.676553Z",
     "start_time": "2025-11-29T19:16:40.671457Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class SVHNDataset(Dataset):\n",
    "    # Wir erben von torch.utils.data.Dataset, damit PyTorch den DataLoader nutzen kann\n",
    "\n",
    "    def __init__(self, mat_file, transform=None):\n",
    "        # Konstruktor: Lädt die .mat-Datei und initialisiert Variablen\n",
    "        data = scipy.io.loadmat(mat_file)   # scipy.io.loadmat: lädt MATLAB-Dateien\n",
    "        self.X = data['X']                  # X enthält die Bilder: shape (32, 32, 3, N)\n",
    "        self.y = data['y'].flatten()        # y enthält Labels: shape (N,). flatten() macht aus Spaltenvektor 1D-Array\n",
    "        self.transform = transform          # Transformationsobjekt für Bildvorverarbeitung (optional)\n",
    "\n",
    "        # SVHN-Spezialfall: Label \"10\" bedeutet Ziffer \"0\"\n",
    "        self.y[self.y == 10] = 0\n",
    "\n",
    "    def __len__(self):\n",
    "        # Gibt die Anzahl der Samples zurück, damit DataLoader weiß, wie viele es gibt\n",
    "        return self.X.shape[3]              # N = Anzahl Bilder\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Liefert ein einzelnes Sample (Bild + Label) für Index idx\n",
    "        img = self.X[:,:,:,idx]             # Bild: (32,32,3)\n",
    "        # torch.tensor: konvertiert numpy-array zu Tensor\n",
    "        # permute(2,0,1): PyTorch erwartet Kanal zuerst (C,H,W) statt H,W,C\n",
    "        # float()/255.0: normalisiert Pixelwerte von [0,255] auf [0,1]\n",
    "        img = torch.tensor(img).permute(2,0,1).float()/255.0\n",
    "        label = int(self.y[idx])            # Label als Integer\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)       # Falls Transform gesetzt, anwenden (z.B. RandomCrop, Normalize)\n",
    "\n",
    "        return img, label                     # Rückgabe: Tensorbild + Label\n"
   ],
   "id": "f67558b545203f4d",
   "outputs": [],
   "execution_count": 116
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "transform = transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))\n",
    "\n",
    "train_dataset = SVHNDataset('train_32x32.mat', transform=transform)\n",
    "test_dataset  = SVHNDataset('test_32x32.mat',  transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader  = DataLoader(test_dataset,  batch_size=1000, shuffle=False)\n"
   ],
   "id": "269c0f764e9b78c5"
  },
  {
   "cell_type": "markdown",
   "id": "827c5d27301f398c",
   "metadata": {},
   "source": "### Beispiel der Trainingsdaten\n"
  },
  {
   "cell_type": "code",
   "id": "b02d1d7c32ac5c9c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-26T09:32:36.707586Z",
     "start_time": "2025-11-26T09:32:36.190841Z"
    }
   },
   "source": [
    "# Iterator über den Test-DataLoader erstellen\n",
    "examples = enumerate(test_loader)\n",
    "\n",
    "# Den ersten Batch aus dem Test-DataLoader holen\n",
    "batch_idx, (example_data, example_targets) = next(examples)\n",
    "\n",
    "# Neue Figur für die Plots erstellen\n",
    "fig = plt.figure()\n",
    "\n",
    "# 6 Bilder aus dem Batch plotten\n",
    "for i in range(6):\n",
    "    # Unterteilt die Figur in 2x3 Raster, i+1 = aktuelle Position\n",
    "    plt.subplot(2, 3, i+1)\n",
    "    plt.tight_layout()  # Sorgt dafür, dass Plots sich nicht überlappen\n",
    "    # Zeige das Bild an; example_data[i][0] = 1. Kanal (Graustufe)\n",
    "    plt.imshow(example_data[i][0], cmap='gray', interpolation='none')\n",
    "    # Titel mit Ground-Truth-Label\n",
    "    plt.title(\"Ground Truth: {}\".format(example_targets[i]))\n",
    "    # Achsenbeschriftungen entfernen\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "\n",
    "# Alle geplotteten Bilder anzeigen\n",
    "plt.show()\n"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 6 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## klassisches neuronales Netz",
   "id": "159182b8463df21d"
  },
  {
   "cell_type": "markdown",
   "id": "54a8cbb0d3e384d",
   "metadata": {},
   "source": [
    "### Definition des CNN-Modells\n",
    "\n",
    "In dieser Zelle wird die Klasse `Net` definiert, die ein Convolutional Neural Network (CNN) implementiert.\n",
    "\n",
    "- **Zweck:** Definiert die Architektur des neuronalen Netzes für Bilderkennung.\n",
    "- **Aufbau:**\n",
    "  1. Convolutional Layer (Feature Extraction)\n",
    "  2. Max-Pooling (Reduzierung der Dimensionen)\n",
    "  3. ReLU (Nichtlinearität)\n",
    "  4. Dropout (Regularisierung)\n",
    "  5. Flatten (Umwandlung in 1D für Fully Connected Layer)\n",
    "  6. Fully Connected Layer(s) (klassische Klassifizierung)\n",
    "  7. Softmax (Wahrscheinlichkeitsausgabe für Klassen)\n",
    "- **TODO-Liste im Kommentar:** Zeigt, welche Layer du für ein vollständiges CNN einbauen solltest.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "7c922dbecff6e9ff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-26T09:33:25.682741Z",
     "start_time": "2025-11-26T09:33:25.675855Z"
    }
   },
   "source": [
    "# TODO Change this class to implement\n",
    "# 1. A valid convolution with kernel size 5, 1 input channel and 10 output channels\n",
    "# 2. A max pooling operation over a 2x2 area\n",
    "# 3. A Relu\n",
    "# 4. A valid convolution with kernel size 5, 10 input channels and 20 output channels\n",
    "# 5. A 2D Dropout layer\n",
    "# 6. A max pooling operation over a 2x2 area\n",
    "# 7. A relu\n",
    "# 8. A flattening operation\n",
    "# 9. A fully connected layer mapping from (whatever dimensions we are at-- find out using .shape) to 50\n",
    "# 10. A ReLU\n",
    "# 11. A fully connected layer mapping from 50 to 10 dimensions\n",
    "# 12. A softmax function.\n",
    "\n",
    "# Replace this class which implements a minimal network (which still does okay)\n",
    "\n",
    "# CNN-Klasse definieren\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        # --- Erster Convolutional Layer ---\n",
    "        # nn.Conv2d: 2D Convolution\n",
    "        # 3 Input-Kanäle (RGB), 10 Output-Kanäle (Features), Kernelgröße 5x5\n",
    "        self.conv1 = nn.Conv2d(3, 10, kernel_size=5)  # TODO: ggf. auf 1 Input-Kanal anpassen für Graustufen\n",
    "\n",
    "        # Dropout für die Convolutionen (reduziert Overfitting)\n",
    "        self.drop = nn.Dropout2d()\n",
    "\n",
    "        # Fully Connected Layer\n",
    "        # nn.Linear: verbindet alle Eingangseinheiten mit allen Ausgangseinheiten\n",
    "        # 1960 Eingangsdimensionen → 1000 Ausgangsdimensionen (muss evtl. an tatsächliche Flatten-Größe angepasst werden)\n",
    "        self.fc1 = nn.Linear(1960, 1000)\n",
    "\n",
    "    # --- Forward-Pass ---\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)               # 1. Convolution\n",
    "        x = self.drop(x)                # 2. Dropout\n",
    "        x = F.max_pool2d(x, 2)          # 3. Max-Pooling über 2x2\n",
    "        x = F.relu(x)                   # 4. ReLU-Aktivierung\n",
    "        x = x.flatten(1)                # 5. Flattening: CxHxW → 1D für Fully Connected Layer\n",
    "        x = self.fc1(x)                 # 6. Fully Connected Layer\n",
    "        x = F.log_softmax(x)            # 7. Log-Softmax für Klassenausgabe\n",
    "        return x                        # Rückgabe: Log-Wahrscheinlichkeiten pro Klasse"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "id": "153a6d1b806aa4d5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-26T09:33:28.250815Z",
     "start_time": "2025-11-26T09:33:28.248108Z"
    }
   },
   "source": [
    "# He initialization of weights\n",
    "def weights_init(layer_in):\n",
    "  if isinstance(layer_in, nn.Linear):\n",
    "    nn.init.kaiming_uniform_(layer_in.weight)\n",
    "    layer_in.bias.data.fill_(0.0)"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "id": "e9e28f00bbe1a2",
   "metadata": {},
   "source": [
    "### Initialisierung des Modells, der Gewichte und des Optimizers\n",
    "\n",
    "In dieser Zelle wird das neuronale Netz erstellt, seine Gewichte initialisiert und der Optimizer definiert.\n",
    "\n",
    "- `model = Net()`: Erstellt eine Instanz des zuvor definierten CNN-Modells.\n",
    "- `model.apply(weights_init)`: Wendet eine eigene Initialisierungsfunktion (`weights_init`) auf alle Layer des Modells an.\n",
    "  Dies sorgt dafür, dass die Anfangsgewichte der Layer sinnvoll gesetzt werden, was das Lernen stabiler und schneller macht.\n",
    "- `optim.SGD(...)`: Erstellt einen Stochastic Gradient Descent Optimizer.\n",
    "  - `model.parameters()`: Übergibt die trainierbaren Parameter des Modells.\n",
    "  - `lr=0.01`: Lernrate.\n",
    "  - `momentum=0.5`: Verbessert die Konvergenzgeschwindigkeit, indem vorherige Gradienten berücksichtigt werden.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "c2e06fa5ac0111d5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-26T11:12:39.587408Z",
     "start_time": "2025-11-26T11:12:39.569632Z"
    }
   },
   "source": [
    "# Erstellt eine Instanz des CNN-Modells (Klasse Net aus vorheriger Zelle)\n",
    "model = Net()\n",
    "\n",
    "# Initialisiert die Gewichte aller Layer mit einer benutzerdefinierten Funktion weights_init\n",
    "# Dadurch starten die Gewichte nicht zufällig schlecht, was Training verbessern kann\n",
    "model.apply(weights_init)\n",
    "\n",
    "# Definiert den Optimizer: Stochastic Gradient Descent\n",
    "# model.parameters(): alle trainierbaren Parameter des Modells\n",
    "# lr=0.01: Lernrate\n",
    "# momentum=0.5: hilft, \"Schwankungen\" im Gradienten auszugleichen\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n"
   ],
   "outputs": [],
   "execution_count": 37
  },
  {
   "cell_type": "markdown",
   "id": "43767df867a534a",
   "metadata": {},
   "source": [
    "### Trainingsroutine\n",
    "\n",
    "In dieser Zelle wird die Trainingsfunktion `train(epoch)` definiert.\n",
    "Sie führt einen kompletten Trainingsdurchlauf (Epoch) über den gesamten Trainingsdatensatz aus.\n",
    "\n",
    "Ablauf:\n",
    "1. `model.train()`: Schaltet das Modell in den Trainingsmodus (aktiviert z.B. Dropout).\n",
    "2. Schleife über alle Batches im `train_loader`.\n",
    "3. `optimizer.zero_grad()`: Setzt Gradienten des vorherigen Backpropagation-Schritts zurück.\n",
    "4. `output = model(data)`: Modellvorhersage für den aktuellen Batch.\n",
    "5. `loss = F.nll_loss(...)`: Berechnet die Negative Log-Likelihood Loss (passend für `log_softmax`).\n",
    "6. `loss.backward()`: Backpropagation – berechnet Gradienten.\n",
    "7. `optimizer.step()`: Aktualisiert die Modellparameter.\n",
    "8. Alle 10 Batches wird der aktuelle Trainingsfortschritt (Loss) ausgegeben.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "7f77e0e2fc3fec3d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-26T11:12:42.421287Z",
     "start_time": "2025-11-26T11:12:42.416429Z"
    }
   },
   "source": [
    "# Haupt-Trainingsroutine\n",
    "def train(epoch):\n",
    "    model.train()  # Modell in den Trainingsmodus setzen (Dropout aktiv, BatchNorm aktiviert)\n",
    "\n",
    "    # Schleife über alle Batches im Training DataLoader\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "\n",
    "        optimizer.zero_grad()       # Vor jedem Schritt Gradienten zurücksetzen\n",
    "\n",
    "        output = model(data)        # Modellvorhersage für aktuellen Batch\n",
    "        loss = F.nll_loss(output, target)  # Loss berechnen (für log_softmax geeignet)\n",
    "\n",
    "        loss.backward()             # Backpropagation: Gradienten berechnen\n",
    "        optimizer.step()            # Parameter-Update mittels Optimizer\n",
    "\n",
    "        # Fortschrittsanzeige alle 10 Batches\n",
    "        if batch_idx % 10 == 0:\n",
    "            print(\n",
    "                'Train Epoch: {} [{}/{}]\\tLoss: {:.6f}'.format(\n",
    "                    epoch,                          # Aktuelle Epoche\n",
    "                    batch_idx * len(data),          # Anzahl verarbeiteter Trainingsbeispiele\n",
    "                    len(train_loader.dataset),      # Gesamtzahl der Trainingsdaten\n",
    "                    loss.item()                     # Aktueller Loss-Wert\n",
    "                )\n",
    "            )\n"
   ],
   "outputs": [],
   "execution_count": 38
  },
  {
   "cell_type": "markdown",
   "id": "8123efde2fd30a84",
   "metadata": {},
   "source": [
    "### Test-/Evaluierungsroutine\n",
    "\n",
    "In dieser Zelle wird die Funktion `test()` definiert, die das Modell auf den Testdaten auswertet.\n",
    "\n",
    "Ablauf:\n",
    "1. `model.eval()`: Setzt das Modell in den Evaluierungsmodus\n",
    "   (Dropout deaktiviert, BatchNorm verwendet Durchschnittswerte).\n",
    "2. `torch.no_grad()`: Deaktiviert Gradientenberechnung – spart Zeit und Speicher.\n",
    "3. Schleife über `test_loader`:\n",
    "   - Modellvorhersagen berechnen\n",
    "   - Loss aufsummieren (für Durchschnitt am Ende)\n",
    "   - Vorhersagen mit den Zielwerten vergleichen → Accuracy\n",
    "4. Nach der Schleife:\n",
    "   - Durchschnitts-Loss berechnen\n",
    "   - Genauigkeit berechnen und ausgeben\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "43b154fbc5b9d301",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-26T11:12:45.534804Z",
     "start_time": "2025-11-26T11:12:45.529710Z"
    }
   },
   "source": [
    "# Testfunktion, um das Modell auf den Testdaten auszuwerten\n",
    "def test():\n",
    "    model.eval()      # Modell in den Evaluierungsmodus setzen (Dropout aus)\n",
    "    test_loss = 0     # Summe der Loss-Werte\n",
    "    correct = 0       # Anzahl korrekter Vorhersagen\n",
    "\n",
    "    # Im Testmodus keine Gradienten berechnen → schneller und spart Speicher\n",
    "    with torch.no_grad():\n",
    "        # Schleife über alle Test-Batches\n",
    "        for data, target in test_loader:\n",
    "\n",
    "            output = model(data)  # Modellvorhersage\n",
    "\n",
    "            # Addiere den Loss des Batches (size_average=False = Summe statt Mittelwert)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()\n",
    "\n",
    "            # Wähle die Klasse mit der höchsten Wahrscheinlichkeit\n",
    "            pred = output.data.max(1, keepdim=True)[1]\n",
    "\n",
    "            # Vergleiche Vorhersagen mit dem Ground-Truth\n",
    "            correct += pred.eq(target.data.view_as(pred)).sum()\n",
    "\n",
    "    # Durchschnittlichen Loss berechnen\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    # Ergebnisse ausgeben\n",
    "    print(\n",
    "        '\\nTest set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "            test_loss,\n",
    "            correct,\n",
    "            len(test_loader.dataset),\n",
    "            100. * correct / len(test_loader.dataset)\n",
    "        )\n",
    "    )"
   ],
   "outputs": [],
   "execution_count": 39
  },
  {
   "cell_type": "markdown",
   "id": "bfe5f4c34938c25a",
   "metadata": {},
   "source": [
    "### Ausführen des Trainings und Testens über mehrere Epochen\n",
    "\n",
    "In dieser Zelle wird zuerst die Anfangsleistung des Modells auf den Testdaten berechnet („untrainiertes Modell“).\n",
    "Anschließend wird das Modell über mehrere Epochen trainiert. Nach jeder Epoche wird erneut getestet, um den Fortschritt zu messen.\n",
    "\n",
    "Ablauf:\n",
    "1. `test()`: Bewertung des Modells vor dem Training (Baseline).\n",
    "2. `n_epochs = 3`: Anzahl der Trainingsdurchläufe (Epochen).\n",
    "3. Schleife über alle Epochen:\n",
    "   - `train(epoch)`: Trainingsdurchlauf über alle Trainingsdaten.\n",
    "   - `test()`: Bewertung der Modellleistung nach dieser Epoche.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "10649218547097f6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-26T11:13:32.095184Z",
     "start_time": "2025-11-26T11:12:48.819057Z"
    }
   },
   "source": [
    "# Anfangsleistung des Modells testen (ungelerntes Modell)\n",
    "test()\n",
    "\n",
    "# Anzahl der Trainings-Epochen festlegen\n",
    "n_epochs = 3\n",
    "\n",
    "# Trainings-/Testschleife über mehrere Epochen\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    train(epoch)  # Modell trainieren\n",
    "    test()        # Modell nach der Epoche testen\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gm/d7swcmj57md2th2_r0b315x80000gn/T/ipykernel_38549/299424523.py:43: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  x = F.log_softmax(x)            # 7. Log-Softmax für Klassenausgabe\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 7.0260, Accuracy: 0/26032 (0%)\n",
      "\n",
      "Train Epoch: 1 [0/73257]\tLoss: 7.019948\n",
      "Train Epoch: 1 [1280/73257]\tLoss: 5.422011\n",
      "Train Epoch: 1 [2560/73257]\tLoss: 4.442428\n",
      "Train Epoch: 1 [3840/73257]\tLoss: 3.591133\n",
      "Train Epoch: 1 [5120/73257]\tLoss: 3.523211\n",
      "Train Epoch: 1 [6400/73257]\tLoss: 3.417609\n",
      "Train Epoch: 1 [7680/73257]\tLoss: 3.326477\n",
      "Train Epoch: 1 [8960/73257]\tLoss: 3.509353\n",
      "Train Epoch: 1 [10240/73257]\tLoss: 3.236801\n",
      "Train Epoch: 1 [11520/73257]\tLoss: 3.747551\n",
      "Train Epoch: 1 [12800/73257]\tLoss: 3.316441\n",
      "Train Epoch: 1 [14080/73257]\tLoss: 3.606859\n",
      "Train Epoch: 1 [15360/73257]\tLoss: 4.066888\n",
      "Train Epoch: 1 [16640/73257]\tLoss: 3.446515\n",
      "Train Epoch: 1 [17920/73257]\tLoss: 3.195255\n",
      "Train Epoch: 1 [19200/73257]\tLoss: 3.382719\n",
      "Train Epoch: 1 [20480/73257]\tLoss: 3.588111\n",
      "Train Epoch: 1 [21760/73257]\tLoss: 3.586449\n",
      "Train Epoch: 1 [23040/73257]\tLoss: 3.592638\n",
      "Train Epoch: 1 [24320/73257]\tLoss: 3.387845\n",
      "Train Epoch: 1 [25600/73257]\tLoss: 3.451303\n",
      "Train Epoch: 1 [26880/73257]\tLoss: 3.441940\n",
      "Train Epoch: 1 [28160/73257]\tLoss: 3.403947\n",
      "Train Epoch: 1 [29440/73257]\tLoss: 3.728967\n",
      "Train Epoch: 1 [30720/73257]\tLoss: 3.324754\n",
      "Train Epoch: 1 [32000/73257]\tLoss: 3.320905\n",
      "Train Epoch: 1 [33280/73257]\tLoss: 3.549397\n",
      "Train Epoch: 1 [34560/73257]\tLoss: 3.556833\n",
      "Train Epoch: 1 [35840/73257]\tLoss: 3.009922\n",
      "Train Epoch: 1 [37120/73257]\tLoss: 3.370533\n",
      "Train Epoch: 1 [38400/73257]\tLoss: 3.257215\n",
      "Train Epoch: 1 [39680/73257]\tLoss: 3.671174\n",
      "Train Epoch: 1 [40960/73257]\tLoss: 3.555156\n",
      "Train Epoch: 1 [42240/73257]\tLoss: 3.634991\n",
      "Train Epoch: 1 [43520/73257]\tLoss: 3.520216\n",
      "Train Epoch: 1 [44800/73257]\tLoss: 3.634040\n",
      "Train Epoch: 1 [46080/73257]\tLoss: 3.272236\n",
      "Train Epoch: 1 [47360/73257]\tLoss: 3.690800\n",
      "Train Epoch: 1 [48640/73257]\tLoss: 3.312031\n",
      "Train Epoch: 1 [49920/73257]\tLoss: 3.491589\n",
      "Train Epoch: 1 [51200/73257]\tLoss: 3.136804\n",
      "Train Epoch: 1 [52480/73257]\tLoss: 3.089536\n",
      "Train Epoch: 1 [53760/73257]\tLoss: 3.394505\n",
      "Train Epoch: 1 [55040/73257]\tLoss: 3.363781\n",
      "Train Epoch: 1 [56320/73257]\tLoss: 3.582965\n",
      "Train Epoch: 1 [57600/73257]\tLoss: 3.461028\n",
      "Train Epoch: 1 [58880/73257]\tLoss: 3.462510\n",
      "Train Epoch: 1 [60160/73257]\tLoss: 3.627084\n",
      "Train Epoch: 1 [61440/73257]\tLoss: 3.072854\n",
      "Train Epoch: 1 [62720/73257]\tLoss: 2.979641\n",
      "Train Epoch: 1 [64000/73257]\tLoss: 3.111130\n",
      "Train Epoch: 1 [65280/73257]\tLoss: 2.851865\n",
      "Train Epoch: 1 [66560/73257]\tLoss: 2.768888\n",
      "Train Epoch: 1 [67840/73257]\tLoss: 3.053923\n",
      "Train Epoch: 1 [69120/73257]\tLoss: 2.982071\n",
      "Train Epoch: 1 [70400/73257]\tLoss: 2.764462\n",
      "Train Epoch: 1 [71680/73257]\tLoss: 2.509985\n",
      "Train Epoch: 1 [72960/73257]\tLoss: 2.873079\n",
      "\n",
      "Test set: Avg. loss: 2.3266, Accuracy: 5097/26032 (20%)\n",
      "\n",
      "Train Epoch: 2 [0/73257]\tLoss: 2.642552\n",
      "Train Epoch: 2 [1280/73257]\tLoss: 3.083095\n",
      "Train Epoch: 2 [2560/73257]\tLoss: 2.940811\n",
      "Train Epoch: 2 [3840/73257]\tLoss: 2.630381\n",
      "Train Epoch: 2 [5120/73257]\tLoss: 3.134398\n",
      "Train Epoch: 2 [6400/73257]\tLoss: 2.646534\n",
      "Train Epoch: 2 [7680/73257]\tLoss: 2.889861\n",
      "Train Epoch: 2 [8960/73257]\tLoss: 2.663751\n",
      "Train Epoch: 2 [10240/73257]\tLoss: 2.957978\n",
      "Train Epoch: 2 [11520/73257]\tLoss: 2.888613\n",
      "Train Epoch: 2 [12800/73257]\tLoss: 2.899235\n",
      "Train Epoch: 2 [14080/73257]\tLoss: 2.651624\n",
      "Train Epoch: 2 [15360/73257]\tLoss: 2.881016\n",
      "Train Epoch: 2 [16640/73257]\tLoss: 2.877301\n",
      "Train Epoch: 2 [17920/73257]\tLoss: 2.773690\n",
      "Train Epoch: 2 [19200/73257]\tLoss: 2.850177\n",
      "Train Epoch: 2 [20480/73257]\tLoss: 2.705559\n",
      "Train Epoch: 2 [21760/73257]\tLoss: 2.639607\n",
      "Train Epoch: 2 [23040/73257]\tLoss: 3.039027\n",
      "Train Epoch: 2 [24320/73257]\tLoss: 2.954003\n",
      "Train Epoch: 2 [25600/73257]\tLoss: 2.552933\n",
      "Train Epoch: 2 [26880/73257]\tLoss: 2.861610\n",
      "Train Epoch: 2 [28160/73257]\tLoss: 2.971979\n",
      "Train Epoch: 2 [29440/73257]\tLoss: 2.743769\n",
      "Train Epoch: 2 [30720/73257]\tLoss: 2.968084\n",
      "Train Epoch: 2 [32000/73257]\tLoss: 2.767715\n",
      "Train Epoch: 2 [33280/73257]\tLoss: 3.138464\n",
      "Train Epoch: 2 [34560/73257]\tLoss: 2.736064\n",
      "Train Epoch: 2 [35840/73257]\tLoss: 2.783772\n",
      "Train Epoch: 2 [37120/73257]\tLoss: 2.750077\n",
      "Train Epoch: 2 [38400/73257]\tLoss: 2.880088\n",
      "Train Epoch: 2 [39680/73257]\tLoss: 2.622209\n",
      "Train Epoch: 2 [40960/73257]\tLoss: 2.651929\n",
      "Train Epoch: 2 [42240/73257]\tLoss: 2.752656\n",
      "Train Epoch: 2 [43520/73257]\tLoss: 2.800812\n",
      "Train Epoch: 2 [44800/73257]\tLoss: 2.862595\n",
      "Train Epoch: 2 [46080/73257]\tLoss: 2.878808\n",
      "Train Epoch: 2 [47360/73257]\tLoss: 2.615080\n",
      "Train Epoch: 2 [48640/73257]\tLoss: 2.548252\n",
      "Train Epoch: 2 [49920/73257]\tLoss: 2.646057\n",
      "Train Epoch: 2 [51200/73257]\tLoss: 2.808646\n",
      "Train Epoch: 2 [52480/73257]\tLoss: 3.013874\n",
      "Train Epoch: 2 [53760/73257]\tLoss: 2.880257\n",
      "Train Epoch: 2 [55040/73257]\tLoss: 2.828686\n",
      "Train Epoch: 2 [56320/73257]\tLoss: 2.708411\n",
      "Train Epoch: 2 [57600/73257]\tLoss: 2.678521\n",
      "Train Epoch: 2 [58880/73257]\tLoss: 2.730254\n",
      "Train Epoch: 2 [60160/73257]\tLoss: 2.800993\n",
      "Train Epoch: 2 [61440/73257]\tLoss: 2.695388\n",
      "Train Epoch: 2 [62720/73257]\tLoss: 2.934298\n",
      "Train Epoch: 2 [64000/73257]\tLoss: 2.836193\n",
      "Train Epoch: 2 [65280/73257]\tLoss: 2.823063\n",
      "Train Epoch: 2 [66560/73257]\tLoss: 2.620929\n",
      "Train Epoch: 2 [67840/73257]\tLoss: 2.753327\n",
      "Train Epoch: 2 [69120/73257]\tLoss: 2.690872\n",
      "Train Epoch: 2 [70400/73257]\tLoss: 2.560905\n",
      "Train Epoch: 2 [71680/73257]\tLoss: 2.550229\n",
      "Train Epoch: 2 [72960/73257]\tLoss: 2.737444\n",
      "\n",
      "Test set: Avg. loss: 2.2117, Accuracy: 5163/26032 (20%)\n",
      "\n",
      "Train Epoch: 3 [0/73257]\tLoss: 2.639792\n",
      "Train Epoch: 3 [1280/73257]\tLoss: 2.884874\n",
      "Train Epoch: 3 [2560/73257]\tLoss: 2.695630\n",
      "Train Epoch: 3 [3840/73257]\tLoss: 2.761204\n",
      "Train Epoch: 3 [5120/73257]\tLoss: 2.658911\n",
      "Train Epoch: 3 [6400/73257]\tLoss: 2.475491\n",
      "Train Epoch: 3 [7680/73257]\tLoss: 2.788910\n",
      "Train Epoch: 3 [8960/73257]\tLoss: 2.659894\n",
      "Train Epoch: 3 [10240/73257]\tLoss: 2.875117\n",
      "Train Epoch: 3 [11520/73257]\tLoss: 2.863993\n",
      "Train Epoch: 3 [12800/73257]\tLoss: 2.975267\n",
      "Train Epoch: 3 [14080/73257]\tLoss: 2.965313\n",
      "Train Epoch: 3 [15360/73257]\tLoss: 2.784371\n",
      "Train Epoch: 3 [16640/73257]\tLoss: 2.682542\n",
      "Train Epoch: 3 [17920/73257]\tLoss: 2.656728\n",
      "Train Epoch: 3 [19200/73257]\tLoss: 2.659656\n",
      "Train Epoch: 3 [20480/73257]\tLoss: 2.725478\n",
      "Train Epoch: 3 [21760/73257]\tLoss: 2.874536\n",
      "Train Epoch: 3 [23040/73257]\tLoss: 2.487536\n",
      "Train Epoch: 3 [24320/73257]\tLoss: 2.820583\n",
      "Train Epoch: 3 [25600/73257]\tLoss: 2.862849\n",
      "Train Epoch: 3 [26880/73257]\tLoss: 2.738981\n",
      "Train Epoch: 3 [28160/73257]\tLoss: 2.628475\n",
      "Train Epoch: 3 [29440/73257]\tLoss: 2.669019\n",
      "Train Epoch: 3 [30720/73257]\tLoss: 2.909695\n",
      "Train Epoch: 3 [32000/73257]\tLoss: 2.577963\n",
      "Train Epoch: 3 [33280/73257]\tLoss: 2.738708\n",
      "Train Epoch: 3 [34560/73257]\tLoss: 2.651720\n",
      "Train Epoch: 3 [35840/73257]\tLoss: 2.779266\n",
      "Train Epoch: 3 [37120/73257]\tLoss: 2.724792\n",
      "Train Epoch: 3 [38400/73257]\tLoss: 2.710169\n",
      "Train Epoch: 3 [39680/73257]\tLoss: 2.821268\n",
      "Train Epoch: 3 [40960/73257]\tLoss: 2.589869\n",
      "Train Epoch: 3 [42240/73257]\tLoss: 2.556251\n",
      "Train Epoch: 3 [43520/73257]\tLoss: 2.612860\n",
      "Train Epoch: 3 [44800/73257]\tLoss: 2.833094\n",
      "Train Epoch: 3 [46080/73257]\tLoss: 2.538879\n",
      "Train Epoch: 3 [47360/73257]\tLoss: 2.473727\n",
      "Train Epoch: 3 [48640/73257]\tLoss: 2.612411\n",
      "Train Epoch: 3 [49920/73257]\tLoss: 2.808944\n",
      "Train Epoch: 3 [51200/73257]\tLoss: 2.746826\n",
      "Train Epoch: 3 [52480/73257]\tLoss: 2.621413\n",
      "Train Epoch: 3 [53760/73257]\tLoss: 2.888114\n",
      "Train Epoch: 3 [55040/73257]\tLoss: 2.886714\n",
      "Train Epoch: 3 [56320/73257]\tLoss: 2.695531\n",
      "Train Epoch: 3 [57600/73257]\tLoss: 2.848211\n",
      "Train Epoch: 3 [58880/73257]\tLoss: 2.723468\n",
      "Train Epoch: 3 [60160/73257]\tLoss: 2.710991\n",
      "Train Epoch: 3 [61440/73257]\tLoss: 2.736977\n",
      "Train Epoch: 3 [62720/73257]\tLoss: 2.564607\n",
      "Train Epoch: 3 [64000/73257]\tLoss: 2.783994\n",
      "Train Epoch: 3 [65280/73257]\tLoss: 2.676960\n",
      "Train Epoch: 3 [66560/73257]\tLoss: 2.770494\n",
      "Train Epoch: 3 [67840/73257]\tLoss: 2.682677\n",
      "Train Epoch: 3 [69120/73257]\tLoss: 2.601593\n",
      "Train Epoch: 3 [70400/73257]\tLoss: 2.628576\n",
      "Train Epoch: 3 [71680/73257]\tLoss: 2.554036\n",
      "Train Epoch: 3 [72960/73257]\tLoss: 2.701440\n",
      "\n",
      "Test set: Avg. loss: 2.1723, Accuracy: 5619/26032 (22%)\n",
      "\n"
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "cell_type": "markdown",
   "id": "2a8defd803f7a988",
   "metadata": {},
   "source": [
    "### Vorhersagen des trainierten Modells visualisieren\n",
    "\n",
    "In dieser Zelle wird das Modell auf einige zuvor geladene Beispielbilder angewendet.\n",
    "Die Predictions (Vorhersagen) werden zusammen mit den Bildern dargestellt.\n",
    "\n",
    "Ablauf:\n",
    "1. `output = model(example_data)`: Modell erzeugt Vorhersagen für die ausgewählten Testbilder.\n",
    "2. `plt.figure()`: Neue Plot-Figur.\n",
    "3. Schleife über mehrere Bilder:\n",
    "   - Bild anzeigen (`imshow`)\n",
    "   - Modellvorhersage ermitteln (`output.data.max(...)`)\n",
    "   - Prediction als Titel ausgeben\n",
    "   - Achsen entfernen\n",
    "4. `plt.show()`: Darstellung der Ergebnisse.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "92c082b0e372bf4d",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2025-11-26T11:13:32.357492Z",
     "start_time": "2025-11-26T11:13:32.101595Z"
    }
   },
   "source": [
    "# Modellvorhersagen für die zuvor geladenen Beispielbilder berechnen\n",
    "output = model(example_data)\n",
    "\n",
    "# Neue Figur für die Plots erstellen\n",
    "fig = plt.figure()\n",
    "\n",
    "# Zeige die ersten 10 Bilder und ihre Vorhersagen\n",
    "for i in range(10):\n",
    "    plt.subplot(5, 5, i+1)  # 5x5 Raster, aktueller Plot: i+1\n",
    "    plt.tight_layout()      # Überlappungen vermeiden\n",
    "\n",
    "    # Beispielbild anzeigen (Graustufe)\n",
    "    plt.imshow(example_data[i][0], cmap='gray', interpolation='none')\n",
    "\n",
    "    # Vorhersage ermitteln:\n",
    "    # max(1) → höchste Wahrscheinlichkeit entlang der Klassenachse\n",
    "    # [1] → Index des Maximums → vorhergesagte Klasse\n",
    "    predicted_label = output.data.max(1, keepdim=True)[1][i].item()\n",
    "\n",
    "    # Titel mit der Vorhersage\n",
    "    plt.title(\"Prediction: {}\".format(predicted_label))\n",
    "\n",
    "    # Achsen entfernen\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "\n",
    "# Plots anzeigen\n",
    "plt.show()\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gm/d7swcmj57md2th2_r0b315x80000gn/T/ipykernel_38549/299424523.py:43: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  x = F.log_softmax(x)            # 7. Log-Softmax für Klassenausgabe\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 10 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    }
   ],
   "execution_count": 41
  },
  {
   "cell_type": "markdown",
   "id": "b0168238de2cf947",
   "metadata": {},
   "source": "## flexible CNN Struktur mit 5 Layern"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T18:46:55.714637Z",
     "start_time": "2025-11-29T18:46:55.702794Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def run_training(model, optimizer, train_loader, test_loader, n_epochs=3, exp_info=\"\"):\n",
    "    train_losses, test_losses, test_accuracies = [], [], []\n",
    "\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "\n",
    "        model.train()\n",
    "        for data, target in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = F.nll_loss(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        train_losses.append(loss.item())\n",
    "\n",
    "        # ===== Testen =====\n",
    "        model.eval()\n",
    "        test_loss = 0\n",
    "        correct = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for data, target in test_loader:\n",
    "                output = model(data)\n",
    "                test_loss += F.nll_loss(output, target, reduction='sum').item()\n",
    "                pred = output.argmax(dim=1)\n",
    "                correct += pred.eq(target).sum().item()\n",
    "\n",
    "        test_loss /= len(test_loader.dataset)\n",
    "        accuracy = 100. * correct / len(test_loader.dataset)\n",
    "\n",
    "        test_losses.append(test_loss)\n",
    "        test_accuracies.append(accuracy)\n",
    "\n",
    "        prefix = f\"[{exp_info}] \" if exp_info else \"\"\n",
    "        print(f\"{prefix}Epoch {epoch} | Test Acc: {accuracy:.2f}% | Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "    return model, train_losses, test_losses, test_accuracies\n"
   ],
   "id": "78c4547ec0e6dbf",
   "outputs": [],
   "execution_count": 112
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Definition\n",
    "Flexible SVHN CNN mit 1–5 Convolutional Layers\n",
    "\n",
    "Dieser Code definiert eine **flexible Convolutional Neural Network (CNN) Architektur für das SVHN-Dataset**, die zwischen 1 und 5 Convolutional Layers variieren kann.\n",
    "Die Architektur umfasst:\n",
    "- Mehrere Convolutional Layers mit ReLU-Aktivierungen\n",
    "- Max-Pooling nach bestimmten Convs\n",
    "- Spatial Dropout nach der letzten Convolution\n",
    "- Fully Connected Layers für die Klassifikation in 10 Klassen (Ziffern 0–9)\n",
    "- Log-Softmax am Output für die Nutzung mit `NLLLoss`\n"
   ],
   "id": "a99b9806a7ca9b2b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-26T11:33:51.557495Z",
     "start_time": "2025-11-26T11:33:51.551715Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class SVHNC5NN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_conv_layers=5,   # Anzahl der Convolutional Layers, flexibel 1–5\n",
    "        dropout_rate=0.3,    # Dropout-Wahrscheinlichkeit nach Convs\n",
    "        fc_hidden=50         # Anzahl der Neuronen im Fully Connected Hidden Layer\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # Sicherstellen, dass num_conv_layers im erlaubten Bereich liegt\n",
    "        assert 1 <= num_conv_layers <= 5, \"num_conv_layers muss zwischen 1 und 5 liegen\"\n",
    "        self.num_conv_layers = num_conv_layers\n",
    "\n",
    "        # --- Convolutional Layers ---\n",
    "        # Erstellen einer flexiblen Liste von Conv-Layern\n",
    "        self.convs = nn.ModuleList()\n",
    "        in_channels = 3  # RGB-Bilder haben 3 Kanäle\n",
    "        out_channels_list = [10, 20, 30, 40, 50]  # Ausgabekanäle pro Layer\n",
    "        kernel_sizes = [5, 5, 3, 3, 3]           # Kernelgrößen pro Layer\n",
    "\n",
    "        for i in range(num_conv_layers):\n",
    "            # Conv Layer erstellen und zur Liste hinzufügen\n",
    "            self.convs.append(nn.Conv2d(in_channels, out_channels_list[i], kernel_size=kernel_sizes[i]))\n",
    "            in_channels = out_channels_list[i]\n",
    "\n",
    "        # Dropout Layer nach dem letzten Conv\n",
    "        self.dropout = nn.Dropout2d(p=dropout_rate)\n",
    "\n",
    "        # --- Dummy Forward Pass ---\n",
    "        # Berechnung der Flatten-Dimension automatisch\n",
    "        dummy_input = torch.zeros(1, 3, 32, 32)  # 1 Dummy-Bild\n",
    "        x = dummy_input\n",
    "        for i, conv in enumerate(self.convs):\n",
    "            x = F.relu(conv(x))\n",
    "            # MaxPooling nach Layer 2 und 4 (optional)\n",
    "            if i in [1, 3]:\n",
    "                x = F.max_pool2d(x, 2)\n",
    "        self.flatten_dim = x.numel()  # Anzahl der Features nach Flatten\n",
    "\n",
    "        # --- Fully Connected Layers ---\n",
    "        self.fc1 = nn.Linear(self.flatten_dim, fc_hidden)  # Hidden Layer\n",
    "        self.fc2 = nn.Linear(fc_hidden, 10)               # Output Layer: 10 Klassen (Ziffern)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # --- Convolutional Forward Pass ---\n",
    "        for i, conv in enumerate(self.convs):\n",
    "            x = F.relu(conv(x))\n",
    "            if i in [1, 3]:\n",
    "                x = F.max_pool2d(x, 2)  # Max-Pooling nach Layer 2 und 4\n",
    "\n",
    "        # Dropout anwenden\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Flatten für Fully Connected Layer\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        # Fully Connected Layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        # Log-Softmax für NLLLoss\n",
    "        return F.log_softmax(x, dim=1)\n"
   ],
   "id": "4da8accf2635af6e",
   "outputs": [],
   "execution_count": 58
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T20:43:01.503203Z",
     "start_time": "2025-11-29T20:43:01.497749Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def run_training(model, optimizer, train_loader, test_loader, n_epochs=3, exp_info=\"\"):\n",
    "    \"\"\"\n",
    "    Führt Training + Testen über mehrere Epochen durch.\n",
    "    Gibt zurück:\n",
    "      - das trainierte Modell\n",
    "      - train_loss pro Epoche\n",
    "      - test_loss pro Epoche\n",
    "      - train_accuracy pro Epoche\n",
    "      - test_accuracy pro Epoche\n",
    "    \"\"\"\n",
    "    train_losses, test_losses = [], []\n",
    "    train_accuracies, test_accuracies = [], []\n",
    "\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        # ===== Training =====\n",
    "        model.train()\n",
    "        correct_train = 0\n",
    "        train_loss_epoch = 0\n",
    "\n",
    "        for data, target in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = F.nll_loss(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss_epoch += loss.item() * data.size(0)\n",
    "            pred = output.argmax(dim=1)\n",
    "            correct_train += pred.eq(target).sum().item()\n",
    "\n",
    "        train_loss_epoch /= len(train_loader.dataset)\n",
    "        train_acc_epoch = 100. * correct_train / len(train_loader.dataset)\n",
    "\n",
    "        train_losses.append(train_loss_epoch)\n",
    "        train_accuracies.append(train_acc_epoch)\n",
    "\n",
    "        # ===== Test =====\n",
    "        model.eval()\n",
    "        test_loss_epoch = 0\n",
    "        correct_test = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for data, target in test_loader:\n",
    "                output = model(data)\n",
    "                test_loss_epoch += F.nll_loss(output, target, reduction='sum').item()\n",
    "                pred = output.argmax(dim=1)\n",
    "                correct_test += pred.eq(target).sum().item()\n",
    "\n",
    "        test_loss_epoch /= len(test_loader.dataset)\n",
    "        test_acc_epoch = 100. * correct_test / len(test_loader.dataset)\n",
    "\n",
    "        test_losses.append(test_loss_epoch)\n",
    "        test_accuracies.append(test_acc_epoch)\n",
    "\n",
    "        # Ausgabe\n",
    "        prefix = f\"[{exp_info}] \" if exp_info else \"\"\n",
    "        print(f\"{prefix}Epoch {epoch} | \"\n",
    "              f\"Train Acc: {train_acc_epoch:.2f}% | Train Loss: {train_loss_epoch:.4f} | \"\n",
    "              f\"Test Acc: {test_acc_epoch:.2f}% | Test Loss: {test_loss_epoch:.4f}\")\n",
    "\n",
    "    # Alles zurückgeben, inklusive trainiertem Modell\n",
    "    return model, train_losses, test_losses, train_accuracies, test_accuracies\n"
   ],
   "id": "895c8763a9cfb382",
   "outputs": [],
   "execution_count": 121
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T20:43:07.205176Z",
     "start_time": "2025-11-29T20:43:07.192078Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ALL_MODELS = {}      # speichert die trainierten Modelle\n",
    "ALL_RESULTS = {}     # speichert Losses & Accuracies pro Modell\n",
    "EPOCHS = 10\n",
    "learning_rates = [0.2, 0.1, 0.01]\n",
    "batch_sizes = [32, 64, 128, 256]\n"
   ],
   "id": "2d8b3b899fe47c20",
   "outputs": [],
   "execution_count": 122
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 1 Layer",
   "id": "b855459468f6f726"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T21:06:14.637131Z",
     "start_time": "2025-11-29T20:43:33.533412Z"
    }
   },
   "cell_type": "code",
   "source": [
    "NUM_CONV_LAYERS = 1\n",
    "for lr in learning_rates:\n",
    "    for bs in batch_sizes:\n",
    "        print(f\"\\n===== Training MODEL: L={NUM_CONV_LAYERS}, LR={lr}, B={bs} =====\")\n",
    "\n",
    "        train_loader_exp = DataLoader(train_dataset, batch_size=bs, shuffle=True)\n",
    "\n",
    "        model = SVHNC5NN(num_conv_layers=NUM_CONV_LAYERS)\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.5)\n",
    "\n",
    "        exp_name = f\"L{NUM_CONV_LAYERS}_LR{lr}_B{bs}\"\n",
    "\n",
    "        trained_model, train_loss, test_loss, train_acc, test_acc = run_training(\n",
    "            model, optimizer, train_loader_exp, test_loader,\n",
    "            n_epochs=EPOCHS,\n",
    "            exp_info=exp_name\n",
    "        )\n",
    "\n",
    "        # Modelle & Ergebnisse speichern\n",
    "        ALL_MODELS[exp_name] = trained_model\n",
    "        ALL_RESULTS[exp_name] = {\n",
    "            \"train_loss\": train_loss,\n",
    "            \"test_loss\": test_loss,\n",
    "            \"train_acc\": train_acc,\n",
    "            \"test_acc\": test_acc,\n",
    "            \"layers\": NUM_CONV_LAYERS,\n",
    "            \"lr\": lr,\n",
    "            \"batch\": bs\n",
    "        }\n",
    "\n",
    "print(f\"\\n=== FERTIG: {NUM_CONV_LAYERS} Convolutional Layer ===\")\n"
   ],
   "id": "29d061b41b68b9ef",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Training MODEL: L=1, LR=0.2, B=32 =====\n",
      "[L1_LR0.2_B32] Epoch 1 | Train Acc: 18.85% | Train Loss: 2.2398 | Test Acc: 19.59% | Test Loss: 2.2253\n",
      "[L1_LR0.2_B32] Epoch 2 | Train Acc: 18.89% | Train Loss: 2.2393 | Test Acc: 19.59% | Test Loss: 2.2265\n",
      "[L1_LR0.2_B32] Epoch 3 | Train Acc: 18.87% | Train Loss: 2.2392 | Test Acc: 19.59% | Test Loss: 2.2248\n",
      "[L1_LR0.2_B32] Epoch 4 | Train Acc: 18.91% | Train Loss: 2.2392 | Test Acc: 19.59% | Test Loss: 2.2274\n",
      "[L1_LR0.2_B32] Epoch 5 | Train Acc: 18.89% | Train Loss: 2.2392 | Test Acc: 19.59% | Test Loss: 2.2280\n",
      "[L1_LR0.2_B32] Epoch 6 | Train Acc: 18.87% | Train Loss: 2.2392 | Test Acc: 19.59% | Test Loss: 2.2263\n",
      "[L1_LR0.2_B32] Epoch 7 | Train Acc: 18.85% | Train Loss: 2.2392 | Test Acc: 19.59% | Test Loss: 2.2271\n",
      "[L1_LR0.2_B32] Epoch 8 | Train Acc: 18.89% | Train Loss: 2.2393 | Test Acc: 19.59% | Test Loss: 2.2305\n",
      "[L1_LR0.2_B32] Epoch 9 | Train Acc: 18.87% | Train Loss: 2.2392 | Test Acc: 19.59% | Test Loss: 2.2264\n",
      "[L1_LR0.2_B32] Epoch 10 | Train Acc: 18.90% | Train Loss: 2.2392 | Test Acc: 19.59% | Test Loss: 2.2301\n",
      "\n",
      "===== Training MODEL: L=1, LR=0.2, B=64 =====\n",
      "[L1_LR0.2_B64] Epoch 1 | Train Acc: 27.43% | Train Loss: 2.0526 | Test Acc: 52.07% | Test Loss: 1.4836\n",
      "[L1_LR0.2_B64] Epoch 2 | Train Acc: 44.82% | Train Loss: 1.6564 | Test Acc: 38.26% | Test Loss: 1.8555\n",
      "[L1_LR0.2_B64] Epoch 3 | Train Acc: 42.40% | Train Loss: 1.7122 | Test Acc: 55.32% | Test Loss: 1.4449\n",
      "[L1_LR0.2_B64] Epoch 4 | Train Acc: 44.68% | Train Loss: 1.6595 | Test Acc: 53.80% | Test Loss: 1.4740\n",
      "[L1_LR0.2_B64] Epoch 5 | Train Acc: 45.44% | Train Loss: 1.6401 | Test Acc: 57.99% | Test Loss: 1.3869\n",
      "[L1_LR0.2_B64] Epoch 6 | Train Acc: 45.11% | Train Loss: 1.6436 | Test Acc: 49.57% | Test Loss: 1.6231\n",
      "[L1_LR0.2_B64] Epoch 7 | Train Acc: 47.12% | Train Loss: 1.6040 | Test Acc: 61.20% | Test Loss: 1.2995\n",
      "[L1_LR0.2_B64] Epoch 8 | Train Acc: 48.00% | Train Loss: 1.5755 | Test Acc: 48.00% | Test Loss: 1.6200\n",
      "[L1_LR0.2_B64] Epoch 9 | Train Acc: 50.38% | Train Loss: 1.5265 | Test Acc: 51.66% | Test Loss: 1.5065\n",
      "[L1_LR0.2_B64] Epoch 10 | Train Acc: 47.16% | Train Loss: 1.5960 | Test Acc: 49.00% | Test Loss: 1.5911\n",
      "\n",
      "===== Training MODEL: L=1, LR=0.2, B=128 =====\n",
      "[L1_LR0.2_B128] Epoch 1 | Train Acc: 27.98% | Train Loss: 2.0439 | Test Acc: 55.43% | Test Loss: 1.4275\n",
      "[L1_LR0.2_B128] Epoch 2 | Train Acc: 45.67% | Train Loss: 1.6147 | Test Acc: 49.44% | Test Loss: 1.5771\n",
      "[L1_LR0.2_B128] Epoch 3 | Train Acc: 49.08% | Train Loss: 1.5179 | Test Acc: 54.51% | Test Loss: 1.4696\n",
      "[L1_LR0.2_B128] Epoch 4 | Train Acc: 59.77% | Train Loss: 1.2598 | Test Acc: 64.07% | Test Loss: 1.1801\n",
      "[L1_LR0.2_B128] Epoch 5 | Train Acc: 62.42% | Train Loss: 1.1805 | Test Acc: 68.51% | Test Loss: 1.1026\n",
      "[L1_LR0.2_B128] Epoch 6 | Train Acc: 63.64% | Train Loss: 1.1412 | Test Acc: 68.37% | Test Loss: 1.0790\n",
      "[L1_LR0.2_B128] Epoch 7 | Train Acc: 64.23% | Train Loss: 1.1241 | Test Acc: 66.28% | Test Loss: 1.1216\n",
      "[L1_LR0.2_B128] Epoch 8 | Train Acc: 64.99% | Train Loss: 1.1010 | Test Acc: 71.30% | Test Loss: 0.9868\n",
      "[L1_LR0.2_B128] Epoch 9 | Train Acc: 65.44% | Train Loss: 1.0907 | Test Acc: 67.56% | Test Loss: 1.0857\n",
      "[L1_LR0.2_B128] Epoch 10 | Train Acc: 66.13% | Train Loss: 1.0631 | Test Acc: 60.92% | Test Loss: 1.3096\n",
      "\n",
      "===== Training MODEL: L=1, LR=0.2, B=256 =====\n",
      "[L1_LR0.2_B256] Epoch 1 | Train Acc: 37.98% | Train Loss: 1.7953 | Test Acc: 58.64% | Test Loss: 1.2974\n",
      "[L1_LR0.2_B256] Epoch 2 | Train Acc: 62.67% | Train Loss: 1.1722 | Test Acc: 65.70% | Test Loss: 1.1226\n",
      "[L1_LR0.2_B256] Epoch 3 | Train Acc: 66.81% | Train Loss: 1.0573 | Test Acc: 67.85% | Test Loss: 1.0675\n",
      "[L1_LR0.2_B256] Epoch 4 | Train Acc: 68.68% | Train Loss: 1.0002 | Test Acc: 66.38% | Test Loss: 1.1286\n",
      "[L1_LR0.2_B256] Epoch 5 | Train Acc: 69.95% | Train Loss: 0.9668 | Test Acc: 69.10% | Test Loss: 1.0378\n",
      "[L1_LR0.2_B256] Epoch 6 | Train Acc: 72.48% | Train Loss: 0.8993 | Test Acc: 73.43% | Test Loss: 0.9124\n",
      "[L1_LR0.2_B256] Epoch 7 | Train Acc: 74.14% | Train Loss: 0.8568 | Test Acc: 74.19% | Test Loss: 0.9289\n",
      "[L1_LR0.2_B256] Epoch 8 | Train Acc: 75.49% | Train Loss: 0.8157 | Test Acc: 73.46% | Test Loss: 0.9214\n",
      "[L1_LR0.2_B256] Epoch 9 | Train Acc: 76.41% | Train Loss: 0.7842 | Test Acc: 70.78% | Test Loss: 1.0397\n",
      "[L1_LR0.2_B256] Epoch 10 | Train Acc: 76.34% | Train Loss: 0.7876 | Test Acc: 70.19% | Test Loss: 1.0379\n",
      "\n",
      "===== Training MODEL: L=1, LR=0.1, B=32 =====\n",
      "[L1_LR0.1_B32] Epoch 1 | Train Acc: 33.40% | Train Loss: 1.8990 | Test Acc: 51.23% | Test Loss: 1.5312\n",
      "[L1_LR0.1_B32] Epoch 2 | Train Acc: 34.47% | Train Loss: 1.8830 | Test Acc: 19.60% | Test Loss: 2.2259\n",
      "[L1_LR0.1_B32] Epoch 3 | Train Acc: 40.04% | Train Loss: 1.7424 | Test Acc: 60.97% | Test Loss: 1.2767\n",
      "[L1_LR0.1_B32] Epoch 4 | Train Acc: 47.38% | Train Loss: 1.5727 | Test Acc: 55.16% | Test Loss: 1.4934\n",
      "[L1_LR0.1_B32] Epoch 5 | Train Acc: 47.81% | Train Loss: 1.5621 | Test Acc: 44.36% | Test Loss: 1.6950\n",
      "[L1_LR0.1_B32] Epoch 6 | Train Acc: 44.91% | Train Loss: 1.6383 | Test Acc: 59.62% | Test Loss: 1.3294\n",
      "[L1_LR0.1_B32] Epoch 7 | Train Acc: 57.11% | Train Loss: 1.3372 | Test Acc: 63.78% | Test Loss: 1.2160\n",
      "[L1_LR0.1_B32] Epoch 8 | Train Acc: 58.55% | Train Loss: 1.3048 | Test Acc: 64.08% | Test Loss: 1.1926\n",
      "[L1_LR0.1_B32] Epoch 9 | Train Acc: 59.00% | Train Loss: 1.2910 | Test Acc: 68.26% | Test Loss: 1.1038\n",
      "[L1_LR0.1_B32] Epoch 10 | Train Acc: 56.92% | Train Loss: 1.3426 | Test Acc: 52.58% | Test Loss: 1.4966\n",
      "\n",
      "===== Training MODEL: L=1, LR=0.1, B=64 =====\n",
      "[L1_LR0.1_B64] Epoch 1 | Train Acc: 50.82% | Train Loss: 1.4760 | Test Acc: 65.93% | Test Loss: 1.1166\n",
      "[L1_LR0.1_B64] Epoch 2 | Train Acc: 67.58% | Train Loss: 1.0450 | Test Acc: 65.10% | Test Loss: 1.1432\n",
      "[L1_LR0.1_B64] Epoch 3 | Train Acc: 69.45% | Train Loss: 0.9867 | Test Acc: 75.27% | Test Loss: 0.8630\n",
      "[L1_LR0.1_B64] Epoch 4 | Train Acc: 70.92% | Train Loss: 0.9482 | Test Acc: 76.12% | Test Loss: 0.8404\n",
      "[L1_LR0.1_B64] Epoch 5 | Train Acc: 71.51% | Train Loss: 0.9229 | Test Acc: 75.16% | Test Loss: 0.8577\n",
      "[L1_LR0.1_B64] Epoch 6 | Train Acc: 72.34% | Train Loss: 0.9033 | Test Acc: 74.74% | Test Loss: 0.9104\n",
      "[L1_LR0.1_B64] Epoch 7 | Train Acc: 73.26% | Train Loss: 0.8767 | Test Acc: 75.37% | Test Loss: 0.8761\n",
      "[L1_LR0.1_B64] Epoch 8 | Train Acc: 73.87% | Train Loss: 0.8602 | Test Acc: 75.61% | Test Loss: 0.8661\n",
      "[L1_LR0.1_B64] Epoch 9 | Train Acc: 74.69% | Train Loss: 0.8323 | Test Acc: 75.31% | Test Loss: 0.8693\n",
      "[L1_LR0.1_B64] Epoch 10 | Train Acc: 74.49% | Train Loss: 0.8366 | Test Acc: 72.24% | Test Loss: 0.9627\n",
      "\n",
      "===== Training MODEL: L=1, LR=0.1, B=128 =====\n",
      "[L1_LR0.1_B128] Epoch 1 | Train Acc: 34.80% | Train Loss: 1.8778 | Test Acc: 65.25% | Test Loss: 1.1475\n",
      "[L1_LR0.1_B128] Epoch 2 | Train Acc: 64.00% | Train Loss: 1.1356 | Test Acc: 71.09% | Test Loss: 0.9701\n",
      "[L1_LR0.1_B128] Epoch 3 | Train Acc: 69.44% | Train Loss: 0.9777 | Test Acc: 74.52% | Test Loss: 0.8896\n",
      "[L1_LR0.1_B128] Epoch 4 | Train Acc: 73.78% | Train Loss: 0.8667 | Test Acc: 76.38% | Test Loss: 0.8257\n",
      "[L1_LR0.1_B128] Epoch 5 | Train Acc: 76.30% | Train Loss: 0.7923 | Test Acc: 79.64% | Test Loss: 0.7376\n",
      "[L1_LR0.1_B128] Epoch 6 | Train Acc: 77.74% | Train Loss: 0.7418 | Test Acc: 79.81% | Test Loss: 0.7342\n",
      "[L1_LR0.1_B128] Epoch 7 | Train Acc: 78.76% | Train Loss: 0.7096 | Test Acc: 79.58% | Test Loss: 0.7445\n",
      "[L1_LR0.1_B128] Epoch 8 | Train Acc: 79.58% | Train Loss: 0.6815 | Test Acc: 79.92% | Test Loss: 0.7262\n",
      "[L1_LR0.1_B128] Epoch 9 | Train Acc: 80.05% | Train Loss: 0.6705 | Test Acc: 78.80% | Test Loss: 0.7707\n",
      "[L1_LR0.1_B128] Epoch 10 | Train Acc: 80.36% | Train Loss: 0.6591 | Test Acc: 80.45% | Test Loss: 0.7268\n",
      "\n",
      "===== Training MODEL: L=1, LR=0.1, B=256 =====\n",
      "[L1_LR0.1_B256] Epoch 1 | Train Acc: 33.94% | Train Loss: 1.9099 | Test Acc: 58.04% | Test Loss: 1.3346\n",
      "[L1_LR0.1_B256] Epoch 2 | Train Acc: 69.18% | Train Loss: 1.0250 | Test Acc: 73.88% | Test Loss: 0.9011\n",
      "[L1_LR0.1_B256] Epoch 3 | Train Acc: 74.86% | Train Loss: 0.8534 | Test Acc: 75.58% | Test Loss: 0.8424\n",
      "[L1_LR0.1_B256] Epoch 4 | Train Acc: 76.94% | Train Loss: 0.7829 | Test Acc: 70.66% | Test Loss: 1.0243\n",
      "[L1_LR0.1_B256] Epoch 5 | Train Acc: 78.39% | Train Loss: 0.7387 | Test Acc: 76.73% | Test Loss: 0.8087\n",
      "[L1_LR0.1_B256] Epoch 6 | Train Acc: 79.37% | Train Loss: 0.6985 | Test Acc: 78.74% | Test Loss: 0.7559\n",
      "[L1_LR0.1_B256] Epoch 7 | Train Acc: 80.20% | Train Loss: 0.6713 | Test Acc: 76.52% | Test Loss: 0.8225\n",
      "[L1_LR0.1_B256] Epoch 8 | Train Acc: 80.81% | Train Loss: 0.6554 | Test Acc: 80.17% | Test Loss: 0.7203\n",
      "[L1_LR0.1_B256] Epoch 9 | Train Acc: 81.39% | Train Loss: 0.6360 | Test Acc: 79.56% | Test Loss: 0.7358\n",
      "[L1_LR0.1_B256] Epoch 10 | Train Acc: 81.69% | Train Loss: 0.6209 | Test Acc: 77.81% | Test Loss: 0.7954\n",
      "\n",
      "===== Training MODEL: L=1, LR=0.01, B=32 =====\n",
      "[L1_LR0.01_B32] Epoch 1 | Train Acc: 32.79% | Train Loss: 1.9343 | Test Acc: 61.85% | Test Loss: 1.2748\n",
      "[L1_LR0.01_B32] Epoch 2 | Train Acc: 71.40% | Train Loss: 0.9634 | Test Acc: 75.54% | Test Loss: 0.8416\n",
      "[L1_LR0.01_B32] Epoch 3 | Train Acc: 77.08% | Train Loss: 0.7864 | Test Acc: 78.83% | Test Loss: 0.7428\n",
      "[L1_LR0.01_B32] Epoch 4 | Train Acc: 79.21% | Train Loss: 0.7085 | Test Acc: 80.04% | Test Loss: 0.7071\n",
      "[L1_LR0.01_B32] Epoch 5 | Train Acc: 80.86% | Train Loss: 0.6639 | Test Acc: 81.21% | Test Loss: 0.6687\n",
      "[L1_LR0.01_B32] Epoch 6 | Train Acc: 81.85% | Train Loss: 0.6275 | Test Acc: 81.50% | Test Loss: 0.6674\n",
      "[L1_LR0.01_B32] Epoch 7 | Train Acc: 82.61% | Train Loss: 0.6038 | Test Acc: 82.28% | Test Loss: 0.6440\n",
      "[L1_LR0.01_B32] Epoch 8 | Train Acc: 83.21% | Train Loss: 0.5817 | Test Acc: 82.24% | Test Loss: 0.6521\n",
      "[L1_LR0.01_B32] Epoch 9 | Train Acc: 83.69% | Train Loss: 0.5657 | Test Acc: 82.84% | Test Loss: 0.6389\n",
      "[L1_LR0.01_B32] Epoch 10 | Train Acc: 83.93% | Train Loss: 0.5528 | Test Acc: 82.26% | Test Loss: 0.6599\n",
      "\n",
      "===== Training MODEL: L=1, LR=0.01, B=64 =====\n",
      "[L1_LR0.01_B64] Epoch 1 | Train Acc: 18.77% | Train Loss: 2.2367 | Test Acc: 19.69% | Test Loss: 2.1992\n",
      "[L1_LR0.01_B64] Epoch 2 | Train Acc: 44.24% | Train Loss: 1.6901 | Test Acc: 64.83% | Test Loss: 1.1851\n",
      "[L1_LR0.01_B64] Epoch 3 | Train Acc: 69.70% | Train Loss: 1.0236 | Test Acc: 73.20% | Test Loss: 0.9084\n",
      "[L1_LR0.01_B64] Epoch 4 | Train Acc: 75.82% | Train Loss: 0.8267 | Test Acc: 77.59% | Test Loss: 0.7852\n",
      "[L1_LR0.01_B64] Epoch 5 | Train Acc: 78.98% | Train Loss: 0.7282 | Test Acc: 79.56% | Test Loss: 0.7312\n",
      "[L1_LR0.01_B64] Epoch 6 | Train Acc: 80.79% | Train Loss: 0.6709 | Test Acc: 80.04% | Test Loss: 0.7160\n",
      "[L1_LR0.01_B64] Epoch 7 | Train Acc: 81.71% | Train Loss: 0.6323 | Test Acc: 81.27% | Test Loss: 0.6857\n",
      "[L1_LR0.01_B64] Epoch 8 | Train Acc: 82.67% | Train Loss: 0.6041 | Test Acc: 81.71% | Test Loss: 0.6712\n",
      "[L1_LR0.01_B64] Epoch 9 | Train Acc: 83.34% | Train Loss: 0.5848 | Test Acc: 81.34% | Test Loss: 0.6837\n",
      "[L1_LR0.01_B64] Epoch 10 | Train Acc: 83.77% | Train Loss: 0.5672 | Test Acc: 82.21% | Test Loss: 0.6543\n",
      "\n",
      "===== Training MODEL: L=1, LR=0.01, B=128 =====\n",
      "[L1_LR0.01_B128] Epoch 1 | Train Acc: 18.92% | Train Loss: 2.2449 | Test Acc: 19.59% | Test Loss: 2.2221\n",
      "[L1_LR0.01_B128] Epoch 2 | Train Acc: 20.19% | Train Loss: 2.2154 | Test Acc: 21.55% | Test Loss: 2.1585\n",
      "[L1_LR0.01_B128] Epoch 3 | Train Acc: 34.16% | Train Loss: 1.9534 | Test Acc: 42.34% | Test Loss: 1.6573\n",
      "[L1_LR0.01_B128] Epoch 4 | Train Acc: 59.05% | Train Loss: 1.3326 | Test Acc: 66.57% | Test Loss: 1.1105\n",
      "[L1_LR0.01_B128] Epoch 5 | Train Acc: 68.58% | Train Loss: 1.0637 | Test Acc: 70.45% | Test Loss: 0.9865\n",
      "[L1_LR0.01_B128] Epoch 6 | Train Acc: 72.23% | Train Loss: 0.9519 | Test Acc: 74.17% | Test Loss: 0.8869\n",
      "[L1_LR0.01_B128] Epoch 7 | Train Acc: 74.59% | Train Loss: 0.8800 | Test Acc: 74.93% | Test Loss: 0.8580\n",
      "[L1_LR0.01_B128] Epoch 8 | Train Acc: 75.98% | Train Loss: 0.8304 | Test Acc: 76.31% | Test Loss: 0.8168\n",
      "[L1_LR0.01_B128] Epoch 9 | Train Acc: 77.40% | Train Loss: 0.7855 | Test Acc: 77.62% | Test Loss: 0.7893\n",
      "[L1_LR0.01_B128] Epoch 10 | Train Acc: 78.54% | Train Loss: 0.7514 | Test Acc: 77.60% | Test Loss: 0.7760\n",
      "\n",
      "===== Training MODEL: L=1, LR=0.01, B=256 =====\n",
      "[L1_LR0.01_B256] Epoch 1 | Train Acc: 18.75% | Train Loss: 2.2458 | Test Acc: 19.59% | Test Loss: 2.2283\n",
      "[L1_LR0.01_B256] Epoch 2 | Train Acc: 18.92% | Train Loss: 2.2341 | Test Acc: 19.59% | Test Loss: 2.2171\n",
      "[L1_LR0.01_B256] Epoch 3 | Train Acc: 19.18% | Train Loss: 2.2092 | Test Acc: 20.28% | Test Loss: 2.1671\n",
      "[L1_LR0.01_B256] Epoch 4 | Train Acc: 26.51% | Train Loss: 2.0727 | Test Acc: 27.09% | Test Loss: 2.0270\n",
      "[L1_LR0.01_B256] Epoch 5 | Train Acc: 39.37% | Train Loss: 1.7969 | Test Acc: 49.29% | Test Loss: 1.6355\n",
      "[L1_LR0.01_B256] Epoch 6 | Train Acc: 52.29% | Train Loss: 1.5014 | Test Acc: 54.78% | Test Loss: 1.4319\n",
      "[L1_LR0.01_B256] Epoch 7 | Train Acc: 60.84% | Train Loss: 1.2912 | Test Acc: 62.20% | Test Loss: 1.2428\n",
      "[L1_LR0.01_B256] Epoch 8 | Train Acc: 65.40% | Train Loss: 1.1653 | Test Acc: 61.44% | Test Loss: 1.2694\n",
      "[L1_LR0.01_B256] Epoch 9 | Train Acc: 68.37% | Train Loss: 1.0854 | Test Acc: 63.94% | Test Loss: 1.2062\n",
      "[L1_LR0.01_B256] Epoch 10 | Train Acc: 70.12% | Train Loss: 1.0251 | Test Acc: 64.85% | Test Loss: 1.1526\n",
      "\n",
      "=== FERTIG: 1 Convolutional Layer ===\n"
     ]
    }
   ],
   "execution_count": 123
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 2 Layer",
   "id": "935c6ef8a36ac765"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T22:13:14.545459Z",
     "start_time": "2025-11-29T21:07:15.317412Z"
    }
   },
   "cell_type": "code",
   "source": [
    "NUM_CONV_LAYERS = 2\n",
    "for lr in learning_rates:\n",
    "    for bs in batch_sizes:\n",
    "        print(f\"\\n===== Training MODEL: L={NUM_CONV_LAYERS}, LR={lr}, B={bs} =====\")\n",
    "\n",
    "        train_loader_exp = DataLoader(train_dataset, batch_size=bs, shuffle=True)\n",
    "\n",
    "        model = SVHNC5NN(num_conv_layers=NUM_CONV_LAYERS)\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.5)\n",
    "\n",
    "        exp_name = f\"L{NUM_CONV_LAYERS}_LR{lr}_B{bs}\"\n",
    "\n",
    "        trained_model, train_loss, test_loss, train_acc, test_acc = run_training(\n",
    "            model, optimizer, train_loader_exp, test_loader,\n",
    "            n_epochs=EPOCHS,\n",
    "            exp_info=exp_name\n",
    "        )\n",
    "\n",
    "        # Modelle & Ergebnisse speichern\n",
    "        ALL_MODELS[exp_name] = trained_model\n",
    "        ALL_RESULTS[exp_name] = {\n",
    "            \"train_loss\": train_loss,\n",
    "            \"test_loss\": test_loss,\n",
    "            \"train_acc\": train_acc,\n",
    "            \"test_acc\": test_acc,\n",
    "            \"layers\": NUM_CONV_LAYERS,\n",
    "            \"lr\": lr,\n",
    "            \"batch\": bs\n",
    "        }\n",
    "\n",
    "print(f\"\\n=== FERTIG: {NUM_CONV_LAYERS} Convolutional Layer ===\")\n"
   ],
   "id": "3256cbaca5d26c5b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Training MODEL: L=2, LR=0.2, B=32 =====\n",
      "[L2_LR0.2_B32] Epoch 1 | Train Acc: 18.88% | Train Loss: 2.2395 | Test Acc: 19.59% | Test Loss: 2.2279\n",
      "[L2_LR0.2_B32] Epoch 2 | Train Acc: 18.87% | Train Loss: 2.2389 | Test Acc: 19.59% | Test Loss: 2.2248\n",
      "[L2_LR0.2_B32] Epoch 3 | Train Acc: 18.93% | Train Loss: 2.2388 | Test Acc: 19.59% | Test Loss: 2.2262\n",
      "[L2_LR0.2_B32] Epoch 4 | Train Acc: 18.86% | Train Loss: 2.2393 | Test Acc: 19.59% | Test Loss: 2.2294\n",
      "[L2_LR0.2_B32] Epoch 5 | Train Acc: 18.88% | Train Loss: 2.2391 | Test Acc: 19.59% | Test Loss: 2.2277\n",
      "[L2_LR0.2_B32] Epoch 6 | Train Acc: 18.88% | Train Loss: 2.2394 | Test Acc: 19.59% | Test Loss: 2.2239\n",
      "[L2_LR0.2_B32] Epoch 7 | Train Acc: 18.90% | Train Loss: 2.2393 | Test Acc: 19.59% | Test Loss: 2.2256\n",
      "[L2_LR0.2_B32] Epoch 8 | Train Acc: 18.89% | Train Loss: 2.2393 | Test Acc: 19.59% | Test Loss: 2.2247\n",
      "[L2_LR0.2_B32] Epoch 9 | Train Acc: 18.99% | Train Loss: 2.2368 | Test Acc: 19.59% | Test Loss: 2.2267\n",
      "[L2_LR0.2_B32] Epoch 10 | Train Acc: 18.88% | Train Loss: 2.2392 | Test Acc: 19.59% | Test Loss: 2.2266\n",
      "\n",
      "===== Training MODEL: L=2, LR=0.2, B=64 =====\n",
      "[L2_LR0.2_B64] Epoch 1 | Train Acc: 38.77% | Train Loss: 1.7679 | Test Acc: 67.21% | Test Loss: 1.0702\n",
      "[L2_LR0.2_B64] Epoch 2 | Train Acc: 66.28% | Train Loss: 1.0780 | Test Acc: 68.22% | Test Loss: 1.0240\n",
      "[L2_LR0.2_B64] Epoch 3 | Train Acc: 70.52% | Train Loss: 0.9569 | Test Acc: 68.19% | Test Loss: 1.0308\n",
      "[L2_LR0.2_B64] Epoch 4 | Train Acc: 72.20% | Train Loss: 0.9088 | Test Acc: 74.37% | Test Loss: 0.8338\n",
      "[L2_LR0.2_B64] Epoch 5 | Train Acc: 72.79% | Train Loss: 0.8910 | Test Acc: 73.82% | Test Loss: 0.8865\n",
      "[L2_LR0.2_B64] Epoch 6 | Train Acc: 73.45% | Train Loss: 0.8624 | Test Acc: 74.10% | Test Loss: 0.8444\n",
      "[L2_LR0.2_B64] Epoch 7 | Train Acc: 74.47% | Train Loss: 0.8399 | Test Acc: 75.05% | Test Loss: 0.8382\n",
      "[L2_LR0.2_B64] Epoch 8 | Train Acc: 75.81% | Train Loss: 0.8017 | Test Acc: 75.86% | Test Loss: 0.8165\n",
      "[L2_LR0.2_B64] Epoch 9 | Train Acc: 75.95% | Train Loss: 0.7989 | Test Acc: 78.53% | Test Loss: 0.7373\n",
      "[L2_LR0.2_B64] Epoch 10 | Train Acc: 76.56% | Train Loss: 0.7747 | Test Acc: 76.66% | Test Loss: 0.8056\n",
      "\n",
      "===== Training MODEL: L=2, LR=0.2, B=128 =====\n",
      "[L2_LR0.2_B128] Epoch 1 | Train Acc: 43.59% | Train Loss: 1.6406 | Test Acc: 73.49% | Test Loss: 0.8934\n",
      "[L2_LR0.2_B128] Epoch 2 | Train Acc: 76.86% | Train Loss: 0.7685 | Test Acc: 76.84% | Test Loss: 0.8024\n",
      "[L2_LR0.2_B128] Epoch 3 | Train Acc: 80.97% | Train Loss: 0.6419 | Test Acc: 82.16% | Test Loss: 0.6326\n",
      "[L2_LR0.2_B128] Epoch 4 | Train Acc: 82.81% | Train Loss: 0.5743 | Test Acc: 75.27% | Test Loss: 0.8411\n",
      "[L2_LR0.2_B128] Epoch 5 | Train Acc: 84.03% | Train Loss: 0.5328 | Test Acc: 84.37% | Test Loss: 0.5606\n",
      "[L2_LR0.2_B128] Epoch 6 | Train Acc: 84.71% | Train Loss: 0.5112 | Test Acc: 84.43% | Test Loss: 0.5674\n",
      "[L2_LR0.2_B128] Epoch 7 | Train Acc: 85.41% | Train Loss: 0.4877 | Test Acc: 84.47% | Test Loss: 0.5654\n",
      "[L2_LR0.2_B128] Epoch 8 | Train Acc: 85.74% | Train Loss: 0.4799 | Test Acc: 83.96% | Test Loss: 0.5769\n",
      "[L2_LR0.2_B128] Epoch 9 | Train Acc: 86.61% | Train Loss: 0.4488 | Test Acc: 84.95% | Test Loss: 0.5624\n",
      "[L2_LR0.2_B128] Epoch 10 | Train Acc: 86.16% | Train Loss: 0.4549 | Test Acc: 83.71% | Test Loss: 0.5708\n",
      "\n",
      "===== Training MODEL: L=2, LR=0.2, B=256 =====\n",
      "[L2_LR0.2_B256] Epoch 1 | Train Acc: 21.76% | Train Loss: 2.1736 | Test Acc: 42.71% | Test Loss: 1.8631\n",
      "[L2_LR0.2_B256] Epoch 2 | Train Acc: 68.95% | Train Loss: 0.9891 | Test Acc: 74.43% | Test Loss: 0.8558\n",
      "[L2_LR0.2_B256] Epoch 3 | Train Acc: 81.15% | Train Loss: 0.6324 | Test Acc: 78.04% | Test Loss: 0.7437\n",
      "[L2_LR0.2_B256] Epoch 4 | Train Acc: 83.85% | Train Loss: 0.5391 | Test Acc: 82.54% | Test Loss: 0.6176\n",
      "[L2_LR0.2_B256] Epoch 5 | Train Acc: 85.65% | Train Loss: 0.4905 | Test Acc: 85.38% | Test Loss: 0.5166\n",
      "[L2_LR0.2_B256] Epoch 6 | Train Acc: 86.19% | Train Loss: 0.4623 | Test Acc: 73.41% | Test Loss: 0.8463\n",
      "[L2_LR0.2_B256] Epoch 7 | Train Acc: 86.97% | Train Loss: 0.4341 | Test Acc: 84.90% | Test Loss: 0.5306\n",
      "[L2_LR0.2_B256] Epoch 8 | Train Acc: 87.84% | Train Loss: 0.4077 | Test Acc: 86.54% | Test Loss: 0.5020\n",
      "[L2_LR0.2_B256] Epoch 9 | Train Acc: 88.23% | Train Loss: 0.3935 | Test Acc: 86.95% | Test Loss: 0.4683\n",
      "[L2_LR0.2_B256] Epoch 10 | Train Acc: 88.39% | Train Loss: 0.3796 | Test Acc: 86.51% | Test Loss: 0.4958\n",
      "\n",
      "===== Training MODEL: L=2, LR=0.1, B=32 =====\n",
      "[L2_LR0.1_B32] Epoch 1 | Train Acc: 51.67% | Train Loss: 1.4451 | Test Acc: 64.99% | Test Loss: 1.1932\n",
      "[L2_LR0.1_B32] Epoch 2 | Train Acc: 70.99% | Train Loss: 0.9418 | Test Acc: 56.14% | Test Loss: 1.3725\n",
      "[L2_LR0.1_B32] Epoch 3 | Train Acc: 73.25% | Train Loss: 0.8661 | Test Acc: 76.65% | Test Loss: 0.7821\n",
      "[L2_LR0.1_B32] Epoch 4 | Train Acc: 75.84% | Train Loss: 0.7914 | Test Acc: 77.45% | Test Loss: 0.7710\n",
      "[L2_LR0.1_B32] Epoch 5 | Train Acc: 77.37% | Train Loss: 0.7402 | Test Acc: 79.30% | Test Loss: 0.7192\n",
      "[L2_LR0.1_B32] Epoch 6 | Train Acc: 78.27% | Train Loss: 0.7129 | Test Acc: 77.87% | Test Loss: 0.7612\n",
      "[L2_LR0.1_B32] Epoch 7 | Train Acc: 79.08% | Train Loss: 0.6930 | Test Acc: 73.04% | Test Loss: 0.8791\n",
      "[L2_LR0.1_B32] Epoch 8 | Train Acc: 79.86% | Train Loss: 0.6748 | Test Acc: 80.89% | Test Loss: 0.6632\n",
      "[L2_LR0.1_B32] Epoch 9 | Train Acc: 79.88% | Train Loss: 0.6705 | Test Acc: 81.10% | Test Loss: 0.7185\n",
      "[L2_LR0.1_B32] Epoch 10 | Train Acc: 80.39% | Train Loss: 0.6561 | Test Acc: 80.74% | Test Loss: 0.6880\n",
      "\n",
      "===== Training MODEL: L=2, LR=0.1, B=64 =====\n",
      "[L2_LR0.1_B64] Epoch 1 | Train Acc: 46.31% | Train Loss: 1.5592 | Test Acc: 78.56% | Test Loss: 0.7371\n",
      "[L2_LR0.1_B64] Epoch 2 | Train Acc: 80.59% | Train Loss: 0.6552 | Test Acc: 80.65% | Test Loss: 0.6734\n",
      "[L2_LR0.1_B64] Epoch 3 | Train Acc: 83.73% | Train Loss: 0.5469 | Test Acc: 83.06% | Test Loss: 0.5930\n",
      "[L2_LR0.1_B64] Epoch 4 | Train Acc: 85.20% | Train Loss: 0.4949 | Test Acc: 85.97% | Test Loss: 0.5035\n",
      "[L2_LR0.1_B64] Epoch 5 | Train Acc: 86.16% | Train Loss: 0.4651 | Test Acc: 84.28% | Test Loss: 0.5479\n",
      "[L2_LR0.1_B64] Epoch 6 | Train Acc: 86.70% | Train Loss: 0.4451 | Test Acc: 82.80% | Test Loss: 0.5771\n",
      "[L2_LR0.1_B64] Epoch 7 | Train Acc: 87.10% | Train Loss: 0.4248 | Test Acc: 86.40% | Test Loss: 0.4939\n",
      "[L2_LR0.1_B64] Epoch 8 | Train Acc: 87.74% | Train Loss: 0.4088 | Test Acc: 86.70% | Test Loss: 0.4906\n",
      "[L2_LR0.1_B64] Epoch 9 | Train Acc: 87.88% | Train Loss: 0.4009 | Test Acc: 87.33% | Test Loss: 0.4823\n",
      "[L2_LR0.1_B64] Epoch 10 | Train Acc: 88.25% | Train Loss: 0.3878 | Test Acc: 86.49% | Test Loss: 0.5114\n",
      "\n",
      "===== Training MODEL: L=2, LR=0.1, B=128 =====\n",
      "[L2_LR0.1_B128] Epoch 1 | Train Acc: 18.83% | Train Loss: 2.2393 | Test Acc: 19.59% | Test Loss: 2.2244\n",
      "[L2_LR0.1_B128] Epoch 2 | Train Acc: 33.76% | Train Loss: 1.8912 | Test Acc: 70.04% | Test Loss: 1.0279\n",
      "[L2_LR0.1_B128] Epoch 3 | Train Acc: 75.38% | Train Loss: 0.8126 | Test Acc: 78.08% | Test Loss: 0.7437\n",
      "[L2_LR0.1_B128] Epoch 4 | Train Acc: 79.81% | Train Loss: 0.6670 | Test Acc: 81.19% | Test Loss: 0.6495\n",
      "[L2_LR0.1_B128] Epoch 5 | Train Acc: 82.36% | Train Loss: 0.5924 | Test Acc: 82.69% | Test Loss: 0.6261\n",
      "[L2_LR0.1_B128] Epoch 6 | Train Acc: 83.93% | Train Loss: 0.5447 | Test Acc: 82.43% | Test Loss: 0.6249\n",
      "[L2_LR0.1_B128] Epoch 7 | Train Acc: 84.82% | Train Loss: 0.5102 | Test Acc: 83.94% | Test Loss: 0.5677\n",
      "[L2_LR0.1_B128] Epoch 8 | Train Acc: 85.61% | Train Loss: 0.4847 | Test Acc: 83.78% | Test Loss: 0.5721\n",
      "[L2_LR0.1_B128] Epoch 9 | Train Acc: 86.12% | Train Loss: 0.4654 | Test Acc: 85.11% | Test Loss: 0.5346\n",
      "[L2_LR0.1_B128] Epoch 10 | Train Acc: 86.74% | Train Loss: 0.4450 | Test Acc: 84.89% | Test Loss: 0.5454\n",
      "\n",
      "===== Training MODEL: L=2, LR=0.1, B=256 =====\n",
      "[L2_LR0.1_B256] Epoch 1 | Train Acc: 21.63% | Train Loss: 2.1786 | Test Acc: 48.89% | Test Loss: 1.5818\n",
      "[L2_LR0.1_B256] Epoch 2 | Train Acc: 71.26% | Train Loss: 0.9429 | Test Acc: 80.50% | Test Loss: 0.6954\n",
      "[L2_LR0.1_B256] Epoch 3 | Train Acc: 82.04% | Train Loss: 0.6138 | Test Acc: 82.38% | Test Loss: 0.6319\n",
      "[L2_LR0.1_B256] Epoch 4 | Train Acc: 84.66% | Train Loss: 0.5198 | Test Acc: 82.63% | Test Loss: 0.6075\n",
      "[L2_LR0.1_B256] Epoch 5 | Train Acc: 86.60% | Train Loss: 0.4589 | Test Acc: 86.48% | Test Loss: 0.4879\n",
      "[L2_LR0.1_B256] Epoch 6 | Train Acc: 87.61% | Train Loss: 0.4231 | Test Acc: 84.66% | Test Loss: 0.5195\n",
      "[L2_LR0.1_B256] Epoch 7 | Train Acc: 88.15% | Train Loss: 0.3999 | Test Acc: 84.82% | Test Loss: 0.5187\n",
      "[L2_LR0.1_B256] Epoch 8 | Train Acc: 88.94% | Train Loss: 0.3754 | Test Acc: 87.80% | Test Loss: 0.4258\n",
      "[L2_LR0.1_B256] Epoch 9 | Train Acc: 89.50% | Train Loss: 0.3570 | Test Acc: 86.74% | Test Loss: 0.4577\n",
      "[L2_LR0.1_B256] Epoch 10 | Train Acc: 89.94% | Train Loss: 0.3405 | Test Acc: 86.95% | Test Loss: 0.4638\n",
      "\n",
      "===== Training MODEL: L=2, LR=0.01, B=32 =====\n",
      "[L2_LR0.01_B32] Epoch 1 | Train Acc: 23.56% | Train Loss: 2.1396 | Test Acc: 51.81% | Test Loss: 1.5563\n",
      "[L2_LR0.01_B32] Epoch 2 | Train Acc: 67.77% | Train Loss: 1.0599 | Test Acc: 80.43% | Test Loss: 0.7013\n",
      "[L2_LR0.01_B32] Epoch 3 | Train Acc: 81.80% | Train Loss: 0.6251 | Test Acc: 83.59% | Test Loss: 0.5599\n",
      "[L2_LR0.01_B32] Epoch 4 | Train Acc: 84.03% | Train Loss: 0.5395 | Test Acc: 85.18% | Test Loss: 0.5108\n",
      "[L2_LR0.01_B32] Epoch 5 | Train Acc: 85.59% | Train Loss: 0.4848 | Test Acc: 85.63% | Test Loss: 0.4794\n",
      "[L2_LR0.01_B32] Epoch 6 | Train Acc: 86.64% | Train Loss: 0.4494 | Test Acc: 86.92% | Test Loss: 0.4644\n",
      "[L2_LR0.01_B32] Epoch 7 | Train Acc: 87.47% | Train Loss: 0.4217 | Test Acc: 87.43% | Test Loss: 0.4310\n",
      "[L2_LR0.01_B32] Epoch 8 | Train Acc: 88.18% | Train Loss: 0.3942 | Test Acc: 87.88% | Test Loss: 0.4211\n",
      "[L2_LR0.01_B32] Epoch 9 | Train Acc: 88.69% | Train Loss: 0.3772 | Test Acc: 87.90% | Test Loss: 0.4229\n",
      "[L2_LR0.01_B32] Epoch 10 | Train Acc: 89.15% | Train Loss: 0.3635 | Test Acc: 87.40% | Test Loss: 0.4461\n",
      "\n",
      "===== Training MODEL: L=2, LR=0.01, B=64 =====\n",
      "[L2_LR0.01_B64] Epoch 1 | Train Acc: 18.87% | Train Loss: 2.2405 | Test Acc: 19.59% | Test Loss: 2.2191\n",
      "[L2_LR0.01_B64] Epoch 2 | Train Acc: 39.57% | Train Loss: 1.7740 | Test Acc: 70.34% | Test Loss: 1.0403\n",
      "[L2_LR0.01_B64] Epoch 3 | Train Acc: 74.82% | Train Loss: 0.8697 | Test Acc: 79.82% | Test Loss: 0.7315\n",
      "[L2_LR0.01_B64] Epoch 4 | Train Acc: 80.77% | Train Loss: 0.6727 | Test Acc: 82.35% | Test Loss: 0.6397\n",
      "[L2_LR0.01_B64] Epoch 5 | Train Acc: 83.08% | Train Loss: 0.5929 | Test Acc: 84.12% | Test Loss: 0.5694\n",
      "[L2_LR0.01_B64] Epoch 6 | Train Acc: 84.53% | Train Loss: 0.5419 | Test Acc: 84.78% | Test Loss: 0.5397\n",
      "[L2_LR0.01_B64] Epoch 7 | Train Acc: 85.46% | Train Loss: 0.5027 | Test Acc: 85.05% | Test Loss: 0.5255\n",
      "[L2_LR0.01_B64] Epoch 8 | Train Acc: 86.18% | Train Loss: 0.4750 | Test Acc: 85.55% | Test Loss: 0.5106\n",
      "[L2_LR0.01_B64] Epoch 9 | Train Acc: 86.73% | Train Loss: 0.4546 | Test Acc: 86.05% | Test Loss: 0.4877\n",
      "[L2_LR0.01_B64] Epoch 10 | Train Acc: 87.30% | Train Loss: 0.4334 | Test Acc: 86.07% | Test Loss: 0.4859\n",
      "\n",
      "===== Training MODEL: L=2, LR=0.01, B=128 =====\n",
      "[L2_LR0.01_B128] Epoch 1 | Train Acc: 18.47% | Train Loss: 2.2451 | Test Acc: 19.59% | Test Loss: 2.2261\n",
      "[L2_LR0.01_B128] Epoch 2 | Train Acc: 18.92% | Train Loss: 2.2361 | Test Acc: 19.59% | Test Loss: 2.2191\n",
      "[L2_LR0.01_B128] Epoch 3 | Train Acc: 20.21% | Train Loss: 2.2021 | Test Acc: 26.32% | Test Loss: 2.0920\n",
      "[L2_LR0.01_B128] Epoch 4 | Train Acc: 44.58% | Train Loss: 1.6791 | Test Acc: 65.19% | Test Loss: 1.1699\n",
      "[L2_LR0.01_B128] Epoch 5 | Train Acc: 70.30% | Train Loss: 1.0012 | Test Acc: 75.66% | Test Loss: 0.8544\n",
      "[L2_LR0.01_B128] Epoch 6 | Train Acc: 77.12% | Train Loss: 0.7927 | Test Acc: 79.75% | Test Loss: 0.7129\n",
      "[L2_LR0.01_B128] Epoch 7 | Train Acc: 80.40% | Train Loss: 0.6837 | Test Acc: 82.60% | Test Loss: 0.6290\n",
      "[L2_LR0.01_B128] Epoch 8 | Train Acc: 82.18% | Train Loss: 0.6199 | Test Acc: 83.36% | Test Loss: 0.5969\n",
      "[L2_LR0.01_B128] Epoch 9 | Train Acc: 83.52% | Train Loss: 0.5792 | Test Acc: 84.22% | Test Loss: 0.5685\n",
      "[L2_LR0.01_B128] Epoch 10 | Train Acc: 84.19% | Train Loss: 0.5504 | Test Acc: 85.27% | Test Loss: 0.5354\n",
      "\n",
      "===== Training MODEL: L=2, LR=0.01, B=256 =====\n",
      "[L2_LR0.01_B256] Epoch 1 | Train Acc: 17.47% | Train Loss: 2.2550 | Test Acc: 19.59% | Test Loss: 2.2296\n",
      "[L2_LR0.01_B256] Epoch 2 | Train Acc: 18.92% | Train Loss: 2.2402 | Test Acc: 19.59% | Test Loss: 2.2275\n",
      "[L2_LR0.01_B256] Epoch 3 | Train Acc: 18.92% | Train Loss: 2.2372 | Test Acc: 19.59% | Test Loss: 2.2224\n",
      "[L2_LR0.01_B256] Epoch 4 | Train Acc: 18.99% | Train Loss: 2.2320 | Test Acc: 19.76% | Test Loss: 2.2129\n",
      "[L2_LR0.01_B256] Epoch 5 | Train Acc: 20.71% | Train Loss: 2.2110 | Test Acc: 23.84% | Test Loss: 2.1640\n",
      "[L2_LR0.01_B256] Epoch 6 | Train Acc: 30.63% | Train Loss: 2.0359 | Test Acc: 40.31% | Test Loss: 1.7808\n",
      "[L2_LR0.01_B256] Epoch 7 | Train Acc: 54.63% | Train Loss: 1.4528 | Test Acc: 66.19% | Test Loss: 1.1287\n",
      "[L2_LR0.01_B256] Epoch 8 | Train Acc: 70.84% | Train Loss: 1.0027 | Test Acc: 75.75% | Test Loss: 0.8763\n",
      "[L2_LR0.01_B256] Epoch 9 | Train Acc: 76.03% | Train Loss: 0.8357 | Test Acc: 76.96% | Test Loss: 0.7943\n",
      "[L2_LR0.01_B256] Epoch 10 | Train Acc: 78.87% | Train Loss: 0.7500 | Test Acc: 80.64% | Test Loss: 0.7166\n",
      "\n",
      "=== FERTIG: 2 Convolutional Layer ===\n"
     ]
    }
   ],
   "execution_count": 124
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3 Layer",
   "id": "f186c23a96c07532"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T23:30:51.371094Z",
     "start_time": "2025-11-29T22:14:12.310724Z"
    }
   },
   "cell_type": "code",
   "source": [
    "NUM_CONV_LAYERS = 3\n",
    "for lr in learning_rates:\n",
    "    for bs in batch_sizes:\n",
    "        print(f\"\\n===== Training MODEL: L={NUM_CONV_LAYERS}, LR={lr}, B={bs} =====\")\n",
    "\n",
    "        train_loader_exp = DataLoader(train_dataset, batch_size=bs, shuffle=True)\n",
    "\n",
    "        model = SVHNC5NN(num_conv_layers=NUM_CONV_LAYERS)\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.5)\n",
    "\n",
    "        exp_name = f\"L{NUM_CONV_LAYERS}_LR{lr}_B{bs}\"\n",
    "\n",
    "        trained_model, train_loss, test_loss, train_acc, test_acc = run_training(\n",
    "            model, optimizer, train_loader_exp, test_loader,\n",
    "            n_epochs=EPOCHS,\n",
    "            exp_info=exp_name\n",
    "        )\n",
    "\n",
    "        # Modelle & Ergebnisse speichern\n",
    "        ALL_MODELS[exp_name] = trained_model\n",
    "        ALL_RESULTS[exp_name] = {\n",
    "            \"train_loss\": train_loss,\n",
    "            \"test_loss\": test_loss,\n",
    "            \"train_acc\": train_acc,\n",
    "            \"test_acc\": test_acc,\n",
    "            \"layers\": NUM_CONV_LAYERS,\n",
    "            \"lr\": lr,\n",
    "            \"batch\": bs\n",
    "        }\n",
    "\n",
    "print(f\"\\n=== FERTIG: {NUM_CONV_LAYERS} Convolutional Layer ===\")\n"
   ],
   "id": "20f4c7bbdcfcc561",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Training MODEL: L=3, LR=0.2, B=32 =====\n",
      "[L3_LR0.2_B32] Epoch 1 | Train Acc: 21.64% | Train Loss: 2.1801 | Test Acc: 19.59% | Test Loss: 2.2267\n",
      "[L3_LR0.2_B32] Epoch 2 | Train Acc: 18.85% | Train Loss: 2.2393 | Test Acc: 19.59% | Test Loss: 2.2288\n",
      "[L3_LR0.2_B32] Epoch 3 | Train Acc: 18.90% | Train Loss: 2.2395 | Test Acc: 19.59% | Test Loss: 2.2282\n",
      "[L3_LR0.2_B32] Epoch 4 | Train Acc: 18.90% | Train Loss: 2.2393 | Test Acc: 19.59% | Test Loss: 2.2265\n",
      "[L3_LR0.2_B32] Epoch 5 | Train Acc: 18.88% | Train Loss: 2.2394 | Test Acc: 19.59% | Test Loss: 2.2254\n",
      "[L3_LR0.2_B32] Epoch 6 | Train Acc: 18.91% | Train Loss: 2.2393 | Test Acc: 19.59% | Test Loss: 2.2290\n",
      "[L3_LR0.2_B32] Epoch 7 | Train Acc: 18.88% | Train Loss: 2.2390 | Test Acc: 19.59% | Test Loss: 2.2248\n",
      "[L3_LR0.2_B32] Epoch 8 | Train Acc: 18.88% | Train Loss: 2.2393 | Test Acc: 19.59% | Test Loss: 2.2289\n",
      "[L3_LR0.2_B32] Epoch 9 | Train Acc: 18.89% | Train Loss: 2.2390 | Test Acc: 19.59% | Test Loss: 2.2272\n",
      "[L3_LR0.2_B32] Epoch 10 | Train Acc: 18.87% | Train Loss: 2.2392 | Test Acc: 19.59% | Test Loss: 2.2267\n",
      "\n",
      "===== Training MODEL: L=3, LR=0.2, B=64 =====\n",
      "[L3_LR0.2_B64] Epoch 1 | Train Acc: 48.84% | Train Loss: 1.4996 | Test Acc: 74.93% | Test Loss: 0.8770\n",
      "[L3_LR0.2_B64] Epoch 2 | Train Acc: 75.08% | Train Loss: 0.8208 | Test Acc: 78.61% | Test Loss: 0.7528\n",
      "[L3_LR0.2_B64] Epoch 3 | Train Acc: 77.28% | Train Loss: 0.7557 | Test Acc: 79.17% | Test Loss: 0.7321\n",
      "[L3_LR0.2_B64] Epoch 4 | Train Acc: 78.09% | Train Loss: 0.7217 | Test Acc: 81.29% | Test Loss: 0.6785\n",
      "[L3_LR0.2_B64] Epoch 5 | Train Acc: 79.09% | Train Loss: 0.6998 | Test Acc: 79.49% | Test Loss: 0.7112\n",
      "[L3_LR0.2_B64] Epoch 6 | Train Acc: 78.87% | Train Loss: 0.7034 | Test Acc: 80.72% | Test Loss: 0.6799\n",
      "[L3_LR0.2_B64] Epoch 7 | Train Acc: 78.81% | Train Loss: 0.7015 | Test Acc: 78.57% | Test Loss: 0.7581\n",
      "[L3_LR0.2_B64] Epoch 8 | Train Acc: 79.45% | Train Loss: 0.6878 | Test Acc: 80.17% | Test Loss: 0.6897\n",
      "[L3_LR0.2_B64] Epoch 9 | Train Acc: 79.56% | Train Loss: 0.6815 | Test Acc: 78.47% | Test Loss: 0.7413\n",
      "[L3_LR0.2_B64] Epoch 10 | Train Acc: 80.15% | Train Loss: 0.6618 | Test Acc: 64.25% | Test Loss: 1.1496\n",
      "\n",
      "===== Training MODEL: L=3, LR=0.2, B=128 =====\n",
      "[L3_LR0.2_B128] Epoch 1 | Train Acc: 38.84% | Train Loss: 1.7566 | Test Acc: 74.29% | Test Loss: 0.8672\n",
      "[L3_LR0.2_B128] Epoch 2 | Train Acc: 75.40% | Train Loss: 0.7982 | Test Acc: 80.37% | Test Loss: 0.6648\n",
      "[L3_LR0.2_B128] Epoch 3 | Train Acc: 79.78% | Train Loss: 0.6612 | Test Acc: 75.12% | Test Loss: 0.7780\n",
      "[L3_LR0.2_B128] Epoch 4 | Train Acc: 79.34% | Train Loss: 0.6647 | Test Acc: 19.59% | Test Loss: 2.2325\n",
      "[L3_LR0.2_B128] Epoch 5 | Train Acc: 18.87% | Train Loss: 2.2392 | Test Acc: 19.59% | Test Loss: 2.2244\n",
      "[L3_LR0.2_B128] Epoch 6 | Train Acc: 18.92% | Train Loss: 2.2371 | Test Acc: 19.59% | Test Loss: 2.2266\n",
      "[L3_LR0.2_B128] Epoch 7 | Train Acc: 18.92% | Train Loss: 2.2371 | Test Acc: 19.59% | Test Loss: 2.2264\n",
      "[L3_LR0.2_B128] Epoch 8 | Train Acc: 18.93% | Train Loss: 2.2372 | Test Acc: 19.59% | Test Loss: 2.2246\n",
      "[L3_LR0.2_B128] Epoch 9 | Train Acc: 18.92% | Train Loss: 2.2372 | Test Acc: 19.59% | Test Loss: 2.2247\n",
      "[L3_LR0.2_B128] Epoch 10 | Train Acc: 18.92% | Train Loss: 2.2372 | Test Acc: 19.59% | Test Loss: 2.2241\n",
      "\n",
      "===== Training MODEL: L=3, LR=0.2, B=256 =====\n",
      "[L3_LR0.2_B256] Epoch 1 | Train Acc: 26.60% | Train Loss: 2.0552 | Test Acc: 54.06% | Test Loss: 1.3824\n",
      "[L3_LR0.2_B256] Epoch 2 | Train Acc: 72.42% | Train Loss: 0.8931 | Test Acc: 76.36% | Test Loss: 0.7583\n",
      "[L3_LR0.2_B256] Epoch 3 | Train Acc: 53.04% | Train Loss: 1.3665 | Test Acc: 19.59% | Test Loss: 2.2241\n",
      "[L3_LR0.2_B256] Epoch 4 | Train Acc: 18.89% | Train Loss: 2.2369 | Test Acc: 19.59% | Test Loss: 2.2246\n",
      "[L3_LR0.2_B256] Epoch 5 | Train Acc: 18.90% | Train Loss: 2.2369 | Test Acc: 19.59% | Test Loss: 2.2243\n",
      "[L3_LR0.2_B256] Epoch 6 | Train Acc: 18.90% | Train Loss: 2.2368 | Test Acc: 19.59% | Test Loss: 2.2246\n",
      "[L3_LR0.2_B256] Epoch 7 | Train Acc: 18.92% | Train Loss: 2.2369 | Test Acc: 19.59% | Test Loss: 2.2243\n",
      "[L3_LR0.2_B256] Epoch 8 | Train Acc: 18.92% | Train Loss: 2.2369 | Test Acc: 19.59% | Test Loss: 2.2242\n",
      "[L3_LR0.2_B256] Epoch 9 | Train Acc: 18.92% | Train Loss: 2.2368 | Test Acc: 19.59% | Test Loss: 2.2241\n",
      "[L3_LR0.2_B256] Epoch 10 | Train Acc: 18.92% | Train Loss: 2.2369 | Test Acc: 19.59% | Test Loss: 2.2248\n",
      "\n",
      "===== Training MODEL: L=3, LR=0.1, B=32 =====\n",
      "[L3_LR0.1_B32] Epoch 1 | Train Acc: 52.02% | Train Loss: 1.4111 | Test Acc: 71.41% | Test Loss: 0.9152\n",
      "[L3_LR0.1_B32] Epoch 2 | Train Acc: 77.05% | Train Loss: 0.7496 | Test Acc: 78.40% | Test Loss: 0.7611\n",
      "[L3_LR0.1_B32] Epoch 3 | Train Acc: 79.14% | Train Loss: 0.6892 | Test Acc: 73.21% | Test Loss: 0.8680\n",
      "[L3_LR0.1_B32] Epoch 4 | Train Acc: 79.98% | Train Loss: 0.6620 | Test Acc: 79.84% | Test Loss: 0.6848\n",
      "[L3_LR0.1_B32] Epoch 5 | Train Acc: 80.66% | Train Loss: 0.6422 | Test Acc: 81.28% | Test Loss: 0.6406\n",
      "[L3_LR0.1_B32] Epoch 6 | Train Acc: 81.05% | Train Loss: 0.6335 | Test Acc: 82.81% | Test Loss: 0.5987\n",
      "[L3_LR0.1_B32] Epoch 7 | Train Acc: 80.71% | Train Loss: 0.6407 | Test Acc: 80.88% | Test Loss: 0.6662\n",
      "[L3_LR0.1_B32] Epoch 8 | Train Acc: 81.31% | Train Loss: 0.6286 | Test Acc: 78.31% | Test Loss: 0.7616\n",
      "[L3_LR0.1_B32] Epoch 9 | Train Acc: 81.14% | Train Loss: 0.6307 | Test Acc: 82.16% | Test Loss: 0.6297\n",
      "[L3_LR0.1_B32] Epoch 10 | Train Acc: 80.65% | Train Loss: 0.6562 | Test Acc: 77.34% | Test Loss: 0.7739\n",
      "\n",
      "===== Training MODEL: L=3, LR=0.1, B=64 =====\n",
      "[L3_LR0.1_B64] Epoch 1 | Train Acc: 52.98% | Train Loss: 1.3828 | Test Acc: 77.24% | Test Loss: 0.7396\n",
      "[L3_LR0.1_B64] Epoch 2 | Train Acc: 69.36% | Train Loss: 0.9480 | Test Acc: 79.31% | Test Loss: 0.6699\n",
      "[L3_LR0.1_B64] Epoch 3 | Train Acc: 81.57% | Train Loss: 0.6072 | Test Acc: 82.66% | Test Loss: 0.5998\n",
      "[L3_LR0.1_B64] Epoch 4 | Train Acc: 83.55% | Train Loss: 0.5445 | Test Acc: 84.27% | Test Loss: 0.5270\n",
      "[L3_LR0.1_B64] Epoch 5 | Train Acc: 84.38% | Train Loss: 0.5183 | Test Acc: 84.83% | Test Loss: 0.5068\n",
      "[L3_LR0.1_B64] Epoch 6 | Train Acc: 85.11% | Train Loss: 0.4993 | Test Acc: 85.51% | Test Loss: 0.4850\n",
      "[L3_LR0.1_B64] Epoch 7 | Train Acc: 85.65% | Train Loss: 0.4771 | Test Acc: 85.75% | Test Loss: 0.4962\n",
      "[L3_LR0.1_B64] Epoch 8 | Train Acc: 86.09% | Train Loss: 0.4632 | Test Acc: 85.12% | Test Loss: 0.5093\n",
      "[L3_LR0.1_B64] Epoch 9 | Train Acc: 86.27% | Train Loss: 0.4527 | Test Acc: 85.64% | Test Loss: 0.5054\n",
      "[L3_LR0.1_B64] Epoch 10 | Train Acc: 86.58% | Train Loss: 0.4445 | Test Acc: 85.96% | Test Loss: 0.5011\n",
      "\n",
      "===== Training MODEL: L=3, LR=0.1, B=128 =====\n",
      "[L3_LR0.1_B128] Epoch 1 | Train Acc: 35.61% | Train Loss: 1.8334 | Test Acc: 74.41% | Test Loss: 0.8443\n",
      "[L3_LR0.1_B128] Epoch 2 | Train Acc: 82.65% | Train Loss: 0.5784 | Test Acc: 80.53% | Test Loss: 0.6420\n",
      "[L3_LR0.1_B128] Epoch 3 | Train Acc: 85.86% | Train Loss: 0.4722 | Test Acc: 85.69% | Test Loss: 0.4910\n",
      "[L3_LR0.1_B128] Epoch 4 | Train Acc: 87.27% | Train Loss: 0.4206 | Test Acc: 88.33% | Test Loss: 0.3961\n",
      "[L3_LR0.1_B128] Epoch 5 | Train Acc: 88.29% | Train Loss: 0.3895 | Test Acc: 87.32% | Test Loss: 0.4266\n",
      "[L3_LR0.1_B128] Epoch 6 | Train Acc: 89.03% | Train Loss: 0.3635 | Test Acc: 88.10% | Test Loss: 0.4022\n",
      "[L3_LR0.1_B128] Epoch 7 | Train Acc: 89.64% | Train Loss: 0.3422 | Test Acc: 88.64% | Test Loss: 0.3945\n",
      "[L3_LR0.1_B128] Epoch 8 | Train Acc: 90.26% | Train Loss: 0.3230 | Test Acc: 88.83% | Test Loss: 0.3976\n",
      "[L3_LR0.1_B128] Epoch 9 | Train Acc: 90.57% | Train Loss: 0.3136 | Test Acc: 88.83% | Test Loss: 0.3955\n",
      "[L3_LR0.1_B128] Epoch 10 | Train Acc: 90.79% | Train Loss: 0.3042 | Test Acc: 89.23% | Test Loss: 0.3797\n",
      "\n",
      "===== Training MODEL: L=3, LR=0.1, B=256 =====\n",
      "[L3_LR0.1_B256] Epoch 1 | Train Acc: 18.78% | Train Loss: 2.2401 | Test Acc: 19.59% | Test Loss: 2.2225\n",
      "[L3_LR0.1_B256] Epoch 2 | Train Acc: 23.03% | Train Loss: 2.1414 | Test Acc: 49.44% | Test Loss: 1.5607\n",
      "[L3_LR0.1_B256] Epoch 3 | Train Acc: 70.87% | Train Loss: 0.9569 | Test Acc: 78.79% | Test Loss: 0.7160\n",
      "[L3_LR0.1_B256] Epoch 4 | Train Acc: 83.45% | Train Loss: 0.5564 | Test Acc: 83.60% | Test Loss: 0.5403\n",
      "[L3_LR0.1_B256] Epoch 5 | Train Acc: 86.34% | Train Loss: 0.4584 | Test Acc: 80.02% | Test Loss: 0.6471\n",
      "[L3_LR0.1_B256] Epoch 6 | Train Acc: 87.89% | Train Loss: 0.4073 | Test Acc: 87.05% | Test Loss: 0.4463\n",
      "[L3_LR0.1_B256] Epoch 7 | Train Acc: 88.93% | Train Loss: 0.3710 | Test Acc: 85.78% | Test Loss: 0.4690\n",
      "[L3_LR0.1_B256] Epoch 8 | Train Acc: 89.63% | Train Loss: 0.3479 | Test Acc: 88.85% | Test Loss: 0.3937\n",
      "[L3_LR0.1_B256] Epoch 9 | Train Acc: 90.25% | Train Loss: 0.3279 | Test Acc: 88.96% | Test Loss: 0.3946\n",
      "[L3_LR0.1_B256] Epoch 10 | Train Acc: 90.78% | Train Loss: 0.3093 | Test Acc: 89.18% | Test Loss: 0.3807\n",
      "\n",
      "===== Training MODEL: L=3, LR=0.01, B=32 =====\n",
      "[L3_LR0.01_B32] Epoch 1 | Train Acc: 18.88% | Train Loss: 2.2394 | Test Acc: 19.59% | Test Loss: 2.2233\n",
      "[L3_LR0.01_B32] Epoch 2 | Train Acc: 45.98% | Train Loss: 1.5798 | Test Acc: 69.81% | Test Loss: 0.9698\n",
      "[L3_LR0.01_B32] Epoch 3 | Train Acc: 81.45% | Train Loss: 0.6187 | Test Acc: 84.11% | Test Loss: 0.5244\n",
      "[L3_LR0.01_B32] Epoch 4 | Train Acc: 86.01% | Train Loss: 0.4684 | Test Acc: 87.32% | Test Loss: 0.4260\n",
      "[L3_LR0.01_B32] Epoch 5 | Train Acc: 87.87% | Train Loss: 0.4101 | Test Acc: 87.99% | Test Loss: 0.4052\n",
      "[L3_LR0.01_B32] Epoch 6 | Train Acc: 88.78% | Train Loss: 0.3748 | Test Acc: 89.13% | Test Loss: 0.3756\n",
      "[L3_LR0.01_B32] Epoch 7 | Train Acc: 89.59% | Train Loss: 0.3484 | Test Acc: 89.69% | Test Loss: 0.3562\n",
      "[L3_LR0.01_B32] Epoch 8 | Train Acc: 90.07% | Train Loss: 0.3318 | Test Acc: 89.68% | Test Loss: 0.3618\n",
      "[L3_LR0.01_B32] Epoch 9 | Train Acc: 90.57% | Train Loss: 0.3152 | Test Acc: 89.89% | Test Loss: 0.3491\n",
      "[L3_LR0.01_B32] Epoch 10 | Train Acc: 91.07% | Train Loss: 0.3021 | Test Acc: 90.34% | Test Loss: 0.3397\n",
      "\n",
      "===== Training MODEL: L=3, LR=0.01, B=64 =====\n",
      "[L3_LR0.01_B64] Epoch 1 | Train Acc: 18.28% | Train Loss: 2.2456 | Test Acc: 19.59% | Test Loss: 2.2244\n",
      "[L3_LR0.01_B64] Epoch 2 | Train Acc: 18.92% | Train Loss: 2.2358 | Test Acc: 19.59% | Test Loss: 2.2200\n",
      "[L3_LR0.01_B64] Epoch 3 | Train Acc: 32.80% | Train Loss: 1.9146 | Test Acc: 64.17% | Test Loss: 1.1618\n",
      "[L3_LR0.01_B64] Epoch 4 | Train Acc: 73.92% | Train Loss: 0.8647 | Test Acc: 80.57% | Test Loss: 0.6664\n",
      "[L3_LR0.01_B64] Epoch 5 | Train Acc: 82.72% | Train Loss: 0.5856 | Test Acc: 84.14% | Test Loss: 0.5379\n",
      "[L3_LR0.01_B64] Epoch 6 | Train Acc: 85.36% | Train Loss: 0.4966 | Test Acc: 86.06% | Test Loss: 0.4731\n",
      "[L3_LR0.01_B64] Epoch 7 | Train Acc: 86.69% | Train Loss: 0.4478 | Test Acc: 86.97% | Test Loss: 0.4371\n",
      "[L3_LR0.01_B64] Epoch 8 | Train Acc: 87.82% | Train Loss: 0.4114 | Test Acc: 88.28% | Test Loss: 0.4024\n",
      "[L3_LR0.01_B64] Epoch 9 | Train Acc: 88.45% | Train Loss: 0.3868 | Test Acc: 88.63% | Test Loss: 0.3878\n",
      "[L3_LR0.01_B64] Epoch 10 | Train Acc: 89.16% | Train Loss: 0.3661 | Test Acc: 88.87% | Test Loss: 0.3800\n",
      "\n",
      "===== Training MODEL: L=3, LR=0.01, B=128 =====\n",
      "[L3_LR0.01_B128] Epoch 1 | Train Acc: 17.82% | Train Loss: 2.2526 | Test Acc: 19.59% | Test Loss: 2.2272\n",
      "[L3_LR0.01_B128] Epoch 2 | Train Acc: 18.92% | Train Loss: 2.2383 | Test Acc: 19.59% | Test Loss: 2.2264\n",
      "[L3_LR0.01_B128] Epoch 3 | Train Acc: 18.92% | Train Loss: 2.2375 | Test Acc: 19.59% | Test Loss: 2.2240\n",
      "[L3_LR0.01_B128] Epoch 4 | Train Acc: 18.92% | Train Loss: 2.2366 | Test Acc: 19.59% | Test Loss: 2.2233\n",
      "[L3_LR0.01_B128] Epoch 5 | Train Acc: 18.92% | Train Loss: 2.2336 | Test Acc: 19.59% | Test Loss: 2.2138\n",
      "[L3_LR0.01_B128] Epoch 6 | Train Acc: 25.16% | Train Loss: 2.1044 | Test Acc: 42.92% | Test Loss: 1.6814\n",
      "[L3_LR0.01_B128] Epoch 7 | Train Acc: 59.92% | Train Loss: 1.2731 | Test Acc: 69.69% | Test Loss: 1.0100\n",
      "[L3_LR0.01_B128] Epoch 8 | Train Acc: 73.55% | Train Loss: 0.8829 | Test Acc: 76.02% | Test Loss: 0.8331\n",
      "[L3_LR0.01_B128] Epoch 9 | Train Acc: 77.19% | Train Loss: 0.7664 | Test Acc: 78.94% | Test Loss: 0.7391\n",
      "[L3_LR0.01_B128] Epoch 10 | Train Acc: 79.76% | Train Loss: 0.6793 | Test Acc: 80.82% | Test Loss: 0.6693\n",
      "\n",
      "===== Training MODEL: L=3, LR=0.01, B=256 =====\n",
      "[L3_LR0.01_B256] Epoch 1 | Train Acc: 17.41% | Train Loss: 2.2568 | Test Acc: 19.59% | Test Loss: 2.2302\n",
      "[L3_LR0.01_B256] Epoch 2 | Train Acc: 18.92% | Train Loss: 2.2400 | Test Acc: 19.59% | Test Loss: 2.2269\n",
      "[L3_LR0.01_B256] Epoch 3 | Train Acc: 18.92% | Train Loss: 2.2386 | Test Acc: 19.59% | Test Loss: 2.2247\n",
      "[L3_LR0.01_B256] Epoch 4 | Train Acc: 18.92% | Train Loss: 2.2376 | Test Acc: 19.59% | Test Loss: 2.2254\n",
      "[L3_LR0.01_B256] Epoch 5 | Train Acc: 18.92% | Train Loss: 2.2363 | Test Acc: 19.59% | Test Loss: 2.2215\n",
      "[L3_LR0.01_B256] Epoch 6 | Train Acc: 18.92% | Train Loss: 2.2333 | Test Acc: 19.59% | Test Loss: 2.2178\n",
      "[L3_LR0.01_B256] Epoch 7 | Train Acc: 18.96% | Train Loss: 2.2237 | Test Acc: 19.66% | Test Loss: 2.1975\n",
      "[L3_LR0.01_B256] Epoch 8 | Train Acc: 23.27% | Train Loss: 2.1464 | Test Acc: 34.20% | Test Loss: 1.9811\n",
      "[L3_LR0.01_B256] Epoch 9 | Train Acc: 49.08% | Train Loss: 1.5823 | Test Acc: 62.06% | Test Loss: 1.2425\n",
      "[L3_LR0.01_B256] Epoch 10 | Train Acc: 69.75% | Train Loss: 1.0163 | Test Acc: 64.91% | Test Loss: 1.1739\n",
      "\n",
      "=== FERTIG: 3 Convolutional Layer ===\n"
     ]
    }
   ],
   "execution_count": 125
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 4 Layer",
   "id": "81ebb00bea9e9d73"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-11-30T09:11:23.598870Z"
    }
   },
   "cell_type": "code",
   "source": [
    "NUM_CONV_LAYERS = 4\n",
    "for lr in learning_rates:\n",
    "    for bs in batch_sizes:\n",
    "        print(f\"\\n===== Training MODEL: L={NUM_CONV_LAYERS}, LR={lr}, B={bs} =====\")\n",
    "\n",
    "        train_loader_exp = DataLoader(train_dataset, batch_size=bs, shuffle=True)\n",
    "\n",
    "        model = SVHNC5NN(num_conv_layers=NUM_CONV_LAYERS)\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.5)\n",
    "\n",
    "        exp_name = f\"L{NUM_CONV_LAYERS}_LR{lr}_B{bs}\"\n",
    "\n",
    "        trained_model, train_loss, test_loss, train_acc, test_acc = run_training(\n",
    "            model, optimizer, train_loader_exp, test_loader,\n",
    "            n_epochs=EPOCHS,\n",
    "            exp_info=exp_name\n",
    "        )\n",
    "\n",
    "        # Modelle & Ergebnisse speichern\n",
    "        ALL_MODELS[exp_name] = trained_model\n",
    "        ALL_RESULTS[exp_name] = {\n",
    "            \"train_loss\": train_loss,\n",
    "            \"test_loss\": test_loss,\n",
    "            \"train_acc\": train_acc,\n",
    "            \"test_acc\": test_acc,\n",
    "            \"layers\": NUM_CONV_LAYERS,\n",
    "            \"lr\": lr,\n",
    "            \"batch\": bs\n",
    "        }\n",
    "\n",
    "print(f\"\\n=== FERTIG: {NUM_CONV_LAYERS} Convolutional Layer ===\")\n"
   ],
   "id": "8bbbcf734640a30f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Training MODEL: L=4, LR=0.2, B=32 =====\n",
      "[L4_LR0.2_B32] Epoch 1 | Train Acc: 19.02% | Train Loss: 2.2382 | Test Acc: 19.59% | Test Loss: 2.2234\n",
      "[L4_LR0.2_B32] Epoch 2 | Train Acc: 18.90% | Train Loss: 2.2394 | Test Acc: 19.59% | Test Loss: 2.2264\n",
      "[L4_LR0.2_B32] Epoch 3 | Train Acc: 18.89% | Train Loss: 2.2392 | Test Acc: 19.59% | Test Loss: 2.2262\n",
      "[L4_LR0.2_B32] Epoch 4 | Train Acc: 18.86% | Train Loss: 2.2392 | Test Acc: 19.59% | Test Loss: 2.2254\n",
      "[L4_LR0.2_B32] Epoch 5 | Train Acc: 18.87% | Train Loss: 2.2392 | Test Acc: 19.59% | Test Loss: 2.2320\n",
      "[L4_LR0.2_B32] Epoch 6 | Train Acc: 18.90% | Train Loss: 2.2392 | Test Acc: 19.59% | Test Loss: 2.2264\n",
      "[L4_LR0.2_B32] Epoch 7 | Train Acc: 18.90% | Train Loss: 2.2391 | Test Acc: 19.59% | Test Loss: 2.2250\n",
      "[L4_LR0.2_B32] Epoch 8 | Train Acc: 18.90% | Train Loss: 2.2393 | Test Acc: 19.59% | Test Loss: 2.2271\n",
      "[L4_LR0.2_B32] Epoch 9 | Train Acc: 18.87% | Train Loss: 2.2393 | Test Acc: 19.59% | Test Loss: 2.2283\n",
      "[L4_LR0.2_B32] Epoch 10 | Train Acc: 18.90% | Train Loss: 2.2394 | Test Acc: 19.59% | Test Loss: 2.2282\n",
      "\n",
      "===== Training MODEL: L=4, LR=0.2, B=64 =====\n",
      "[L4_LR0.2_B64] Epoch 1 | Train Acc: 18.84% | Train Loss: 2.2385 | Test Acc: 19.59% | Test Loss: 2.2250\n",
      "[L4_LR0.2_B64] Epoch 2 | Train Acc: 22.45% | Train Loss: nan | Test Acc: 6.70% | Test Loss: nan\n",
      "[L4_LR0.2_B64] Epoch 3 | Train Acc: 6.75% | Train Loss: nan | Test Acc: 6.70% | Test Loss: nan\n",
      "[L4_LR0.2_B64] Epoch 4 | Train Acc: 6.75% | Train Loss: nan | Test Acc: 6.70% | Test Loss: nan\n",
      "[L4_LR0.2_B64] Epoch 5 | Train Acc: 6.75% | Train Loss: nan | Test Acc: 6.70% | Test Loss: nan\n",
      "[L4_LR0.2_B64] Epoch 6 | Train Acc: 6.75% | Train Loss: nan | Test Acc: 6.70% | Test Loss: nan\n",
      "[L4_LR0.2_B64] Epoch 7 | Train Acc: 6.75% | Train Loss: nan | Test Acc: 6.70% | Test Loss: nan\n",
      "[L4_LR0.2_B64] Epoch 8 | Train Acc: 6.75% | Train Loss: nan | Test Acc: 6.70% | Test Loss: nan\n",
      "[L4_LR0.2_B64] Epoch 9 | Train Acc: 6.75% | Train Loss: nan | Test Acc: 6.70% | Test Loss: nan\n",
      "[L4_LR0.2_B64] Epoch 10 | Train Acc: 6.75% | Train Loss: nan | Test Acc: 6.70% | Test Loss: nan\n",
      "\n",
      "===== Training MODEL: L=4, LR=0.2, B=128 =====\n",
      "[L4_LR0.2_B128] Epoch 1 | Train Acc: 18.84% | Train Loss: 2.2386 | Test Acc: 19.59% | Test Loss: 2.2267\n",
      "[L4_LR0.2_B128] Epoch 2 | Train Acc: 35.73% | Train Loss: 1.8262 | Test Acc: 62.52% | Test Loss: 1.1581\n",
      "[L4_LR0.2_B128] Epoch 3 | Train Acc: 35.73% | Train Loss: 52383738.7186 | Test Acc: 19.59% | Test Loss: 2.2250\n",
      "[L4_LR0.2_B128] Epoch 4 | Train Acc: 18.92% | Train Loss: 2.2371 | Test Acc: 19.59% | Test Loss: 2.2254\n",
      "[L4_LR0.2_B128] Epoch 5 | Train Acc: 18.92% | Train Loss: 2.2372 | Test Acc: 19.59% | Test Loss: 2.2249\n",
      "[L4_LR0.2_B128] Epoch 6 | Train Acc: 18.92% | Train Loss: 2.2372 | Test Acc: 19.59% | Test Loss: 2.2240\n",
      "[L4_LR0.2_B128] Epoch 7 | Train Acc: 18.92% | Train Loss: 2.2373 | Test Acc: 19.59% | Test Loss: 2.2242\n",
      "[L4_LR0.2_B128] Epoch 8 | Train Acc: 18.92% | Train Loss: 2.2372 | Test Acc: 19.59% | Test Loss: 2.2239\n",
      "[L4_LR0.2_B128] Epoch 9 | Train Acc: 18.92% | Train Loss: 2.2371 | Test Acc: 19.59% | Test Loss: 2.2249\n",
      "[L4_LR0.2_B128] Epoch 10 | Train Acc: 18.92% | Train Loss: 2.2372 | Test Acc: 19.59% | Test Loss: 2.2240\n",
      "\n",
      "===== Training MODEL: L=4, LR=0.2, B=256 =====\n",
      "[L4_LR0.2_B256] Epoch 1 | Train Acc: 18.92% | Train Loss: 2.2382 | Test Acc: 19.59% | Test Loss: 2.2242\n",
      "[L4_LR0.2_B256] Epoch 2 | Train Acc: 31.74% | Train Loss: 1.9338 | Test Acc: 69.33% | Test Loss: 1.0292\n",
      "[L4_LR0.2_B256] Epoch 3 | Train Acc: 77.37% | Train Loss: 0.7421 | Test Acc: 80.41% | Test Loss: 0.6607\n",
      "[L4_LR0.2_B256] Epoch 4 | Train Acc: 83.22% | Train Loss: 0.5539 | Test Acc: 82.16% | Test Loss: 0.5990\n",
      "[L4_LR0.2_B256] Epoch 5 | Train Acc: 85.20% | Train Loss: 0.4894 | Test Acc: 82.30% | Test Loss: 0.6246\n",
      "[L4_LR0.2_B256] Epoch 6 | Train Acc: 86.38% | Train Loss: 0.4538 | Test Acc: 86.03% | Test Loss: 0.4782\n",
      "[L4_LR0.2_B256] Epoch 7 | Train Acc: 87.18% | Train Loss: 0.4241 | Test Acc: 86.61% | Test Loss: 0.4536\n",
      "[L4_LR0.2_B256] Epoch 8 | Train Acc: 87.78% | Train Loss: 0.4064 | Test Acc: 82.56% | Test Loss: 0.5672\n",
      "[L4_LR0.2_B256] Epoch 9 | Train Acc: 88.26% | Train Loss: 0.3894 | Test Acc: 87.88% | Test Loss: 0.4194\n",
      "[L4_LR0.2_B256] Epoch 10 | Train Acc: 88.88% | Train Loss: 0.3726 | Test Acc: 87.02% | Test Loss: 0.4323\n",
      "\n",
      "===== Training MODEL: L=4, LR=0.1, B=32 =====\n",
      "[L4_LR0.1_B32] Epoch 1 | Train Acc: 33.39% | Train Loss: 1.8841 | Test Acc: 71.78% | Test Loss: 0.8927\n",
      "[L4_LR0.1_B32] Epoch 2 | Train Acc: 75.36% | Train Loss: 0.8050 | Test Acc: 80.76% | Test Loss: 0.6850\n",
      "[L4_LR0.1_B32] Epoch 3 | Train Acc: 76.34% | Train Loss: 0.7812 | Test Acc: 74.79% | Test Loss: 0.8632\n",
      "[L4_LR0.1_B32] Epoch 4 | Train Acc: 76.34% | Train Loss: 0.7853 | Test Acc: 43.76% | Test Loss: 1.7136\n",
      "[L4_LR0.1_B32] Epoch 5 | Train Acc: 76.63% | Train Loss: 0.7787 | Test Acc: 77.90% | Test Loss: 0.7459\n",
      "[L4_LR0.1_B32] Epoch 6 | Train Acc: 77.63% | Train Loss: 0.7523 | Test Acc: 66.92% | Test Loss: 1.0891\n",
      "[L4_LR0.1_B32] Epoch 7 | Train Acc: 75.60% | Train Loss: 0.8197 | Test Acc: 78.82% | Test Loss: 0.7320\n",
      "[L4_LR0.1_B32] Epoch 8 | Train Acc: 75.73% | Train Loss: 0.8186 | Test Acc: 77.65% | Test Loss: 0.7468\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 5 Layer",
   "id": "feba23719668f6a1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "NUM_CONV_LAYERS = 5\n",
    "for lr in learning_rates:\n",
    "    for bs in batch_sizes:\n",
    "        print(f\"\\n===== Training MODEL: L={NUM_CONV_LAYERS}, LR={lr}, B={bs} =====\")\n",
    "\n",
    "        train_loader_exp = DataLoader(train_dataset, batch_size=bs, shuffle=True)\n",
    "\n",
    "        model = SVHNC5NN(num_conv_layers=NUM_CONV_LAYERS)\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.5)\n",
    "\n",
    "        exp_name = f\"L{NUM_CONV_LAYERS}_LR{lr}_B{bs}\"\n",
    "\n",
    "        trained_model, train_loss, test_loss, train_acc, test_acc = run_training(\n",
    "            model, optimizer, train_loader_exp, test_loader,\n",
    "            n_epochs=EPOCHS,\n",
    "            exp_info=exp_name\n",
    "        )\n",
    "\n",
    "        # Modelle & Ergebnisse speichern\n",
    "        ALL_MODELS[exp_name] = trained_model\n",
    "        ALL_RESULTS[exp_name] = {\n",
    "            \"train_loss\": train_loss,\n",
    "            \"test_loss\": test_loss,\n",
    "            \"train_acc\": train_acc,\n",
    "            \"test_acc\": test_acc,\n",
    "            \"layers\": NUM_CONV_LAYERS,\n",
    "            \"lr\": lr,\n",
    "            \"batch\": bs\n",
    "        }\n",
    "\n",
    "print(f\"\\n=== FERTIG: {NUM_CONV_LAYERS} Convolutional Layer ===\")\n"
   ],
   "id": "6a01c2cab67e28e4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Visualisierung\n",
    "\n",
    "In diesem Abschnitt wollen wir die eben durchlaufenen Modelle miteinander vergleichen. Dafür schauen wir uns zunächst einmal alle Ergebnisse an."
   ],
   "id": "f9bef1135bd0c311"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T08:40:20.201199Z",
     "start_time": "2025-11-30T08:40:20.196343Z"
    }
   },
   "cell_type": "code",
   "source": "print(ALL_RESULTS)",
   "id": "f02e7c563c04323e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'L1_LR0.2_B32': {'train_loss': [2.2398405278645352, 2.239284572355014, 2.2391663970262767, 2.239199931702557, 2.2391920594144405, 2.2391592385326042, 2.23916620503711, 2.239310094514376, 2.2391904469703587, 2.2392153224408458], 'test_loss': [2.225345448034998, 2.226479540885264, 2.224783997632421, 2.2274268551259664, 2.228039511743322, 2.2263252118209715, 2.227063960946991, 2.2305244851420096, 2.2263534205342337, 2.2301431944858243], 'train_acc': [18.85144081794231, 18.89375759313103, 18.870551619640445, 18.911503337565012, 18.888297364074422, 18.87328173416875, 18.85144081794231, 18.89375759313103, 18.866456447847987, 18.89921782218764], 'test_acc': [19.587430854333128, 19.587430854333128, 19.587430854333128, 19.587430854333128, 19.587430854333128, 19.587430854333128, 19.587430854333128, 19.587430854333128, 19.587430854333128, 19.587430854333128], 'layers': 1, 'lr': 0.2, 'batch': 32}, 'L1_LR0.2_B64': {'train_loss': [2.0526346830002518, 1.6563534434025229, 1.7122249804809502, 1.6595466228824596, 1.640057624684135, 1.64361567141824, 1.604024858783393, 1.5755367837208083, 1.5264906243606735, 1.59596338081313], 'test_loss': [1.483639907602452, 1.8554712372025464, 1.4448722242649596, 1.4740138390933215, 1.3868603057193112, 1.6231096933865445, 1.2995026115998947, 1.6199585128284775, 1.5064795880355788, 1.5910651745353657], 'train_acc': [27.43219078040324, 44.817560096646055, 42.3986786245683, 44.682419427494985, 45.43729609457117, 45.11241246570294, 47.12314181579917, 47.998143522120756, 50.382898562594704, 47.158633304667134], 'test_acc': [52.07052858020897, 38.256760909649664, 55.316533497234175, 53.795328826060235, 57.986324523663185, 49.56591886908421, 61.20159803318992, 47.99861708666257, 51.663337430854334, 48.997387830362634], 'layers': 1, 'lr': 0.2, 'batch': 64}, 'L1_LR0.2_B128': {'train_loss': [2.0439416059872353, 1.6146600890862914, 1.5178789308626135, 1.2598067686908188, 1.180471305317848, 1.1411847039921508, 1.124133934777054, 1.100990452758705, 1.0906960169140691, 1.0631483483767221], 'test_loss': [1.427465538049521, 1.5770559291863047, 1.4696082290726786, 1.1800604157456607, 1.1025702540099291, 1.0789771017005418, 1.1215627559797854, 0.9868497989187962, 1.085656310051002, 1.3095966014088336], 'train_acc': [27.98367391512074, 45.66935582947705, 49.08472910438593, 59.76630219637714, 62.41587834609662, 63.63760459751287, 64.23276956468324, 64.99310646081604, 65.43538501440135, 66.1288341045907], 'test_acc': [55.427934849416104, 49.44299323909035, 54.50983405039951, 64.07498463429626, 68.50799016594961, 68.36585740626921, 66.279963122311, 71.30070682237246, 67.55915795943454, 60.92117393976644], 'layers': 1, 'lr': 0.2, 'batch': 128}, 'L1_LR0.2_B256': {'train_loss': [1.7952875628011116, 1.1722330195366746, 1.0573417288720017, 1.00019312054047, 0.966769249907374, 0.8992517113609126, 0.8567764538494722, 0.8156749739447017, 0.7841909251699206, 0.7876157731174575], 'test_loss': [1.2973514861978486, 1.1225728748323878, 1.0675223505269844, 1.1286039747338978, 1.0377588703651406, 0.9123821021152114, 0.9288550580874058, 0.9214102004888824, 1.0397354549033992, 1.0379279790367169], 'train_acc': [37.97589308871507, 62.671144054493084, 66.81136273666681, 68.67876107402705, 69.95372455874524, 72.48454072648347, 74.14035518790013, 75.48630165035424, 76.4131755327136, 76.34082749771353], 'test_acc': [58.63552550706822, 65.69606637984019, 67.85494775660726, 66.3759987707437, 69.09572833435772, 73.43269821757836, 74.18561770129072, 73.45574677320221, 70.78211432083589, 70.19437615242778], 'layers': 1, 'lr': 0.2, 'batch': 256}, 'L1_LR0.1_B32': {'train_loss': [1.8989908705017142, 1.8829500029954307, 1.7424271165643601, 1.5726697402574075, 1.5621100166501807, 1.6383424781308946, 1.3371922904922386, 1.3047585480864758, 1.2910086854879195, 1.3426080904742876], 'test_loss': [1.5311944024578954, 2.2258925974259145, 1.2767498919622402, 1.4934165186140533, 1.695037109103024, 1.3294022996219008, 1.2159672864170696, 1.1926492653379575, 1.1037501916609882, 1.4966258788738942], 'train_acc': [33.39749102474849, 34.47179109163629, 40.04258978664155, 47.37977258145979, 47.80976561966774, 44.90628881881595, 57.107170645808594, 58.5500361740175, 59.00050507118774, 56.91879274335558], 'test_acc': [51.22541487400123, 19.60279655808236, 60.97495390288875, 55.15519360786724, 44.36462814996927, 59.618930547019055, 63.77919483712354, 64.07882606023355, 68.26213890596189, 52.581438229870926], 'layers': 1, 'lr': 0.1, 'batch': 32}, 'L1_LR0.1_B64': {'train_loss': [1.476041746323083, 1.0449849439880918, 0.9866670829522362, 0.9481915515003901, 0.922870692683846, 0.9033430675692388, 0.8766738180837105, 0.8602173720450904, 0.8322786403502626, 0.8366229068943773], 'test_loss': [1.1165672722372748, 1.143238296886859, 0.8629793133117263, 0.8404481331849288, 0.8576563258953599, 0.9103946772981584, 0.8761298988373938, 0.8661388784518766, 0.8693456950184753, 0.9627490559456607], 'train_acc': [50.821081944387565, 67.58398514817696, 69.45138348553722, 70.92018510176501, 71.50852478261463, 72.33847959921918, 73.25989325252195, 73.87007384959799, 74.69456843714593, 74.48844479025895], 'test_acc': [65.93423478795329, 65.09680393362017, 75.26889981561156, 76.11785494775661, 75.16133988936693, 74.73878303626306, 75.37261831591887, 75.61078672403197, 75.30731407498463, 72.2418561770129], 'layers': 1, 'lr': 0.1, 'batch': 64}, 'L1_LR0.1_B128': {'train_loss': [1.8777912497473255, 1.1356262887966129, 0.9776778605126605, 0.8667206008010195, 0.7923079394837902, 0.7418473304169902, 0.7096210081247583, 0.6815156298045817, 0.6704578273218931, 0.6590637510365086], 'test_loss': [1.1474590023090008, 0.9700570934450399, 0.8895713205929541, 0.8256771390102506, 0.7376330651310814, 0.7342072317185545, 0.7445279049155312, 0.7261913196397253, 0.7707213194148996, 0.7268050736859304], 'train_acc': [34.79940483503283, 64.00343994430567, 69.43500279836739, 73.7840752419564, 76.30124083705311, 77.74001119346957, 78.76380414158373, 79.57601321375432, 80.05105314167929, 80.36092114064185], 'test_acc': [65.25046097111247, 71.09326982175784, 74.5159803318992, 76.37523048555624, 79.63660110633067, 79.80562384757222, 79.58282114320836, 79.92470805162877, 78.79917025199754, 80.44714197910264], 'layers': 1, 'lr': 0.1, 'batch': 128}, 'L1_LR0.1_B256': {'train_loss': [1.9098526745380051, 1.025017428824804, 0.8534023216782984, 0.7829317446821326, 0.7386523066371932, 0.6985090112857673, 0.6712869268925172, 0.6553806585524308, 0.6359710204802582, 0.6209382320932175], 'test_loss': [1.3345545613113619, 0.9011202674008764, 0.8423789138067113, 1.0243379332642653, 0.8087097393931534, 0.7558587688423771, 0.8225449078201734, 0.7202984999496657, 0.7358113986697018, 0.795428424725594], 'train_acc': [33.93668864408862, 69.18383226176338, 74.86383553790081, 76.94281775120466, 78.39387362299848, 79.37398473865979, 80.19984438347188, 80.80592980875548, 81.39017431781264, 81.69048691592613], 'test_acc': [58.043945912722805, 73.87830362630608, 75.5762138905962, 70.66303011677935, 76.72864167178857, 78.74154886293792, 76.52120467117393, 80.17440073755378, 79.56361401352181, 77.8080823601721], 'layers': 1, 'lr': 0.1, 'batch': 256}, 'L1_LR0.01_B32': {'train_loss': [1.9342857609201074, 0.9634404828437875, 0.7864457836570244, 0.7085310565041651, 0.6639377322105768, 0.6274744580417613, 0.6037748675614256, 0.5817322189688743, 0.5656781161461369, 0.5527534068073386], 'test_loss': [1.2747976822433988, 0.8416006951956347, 0.7428166267691614, 0.707062459277755, 0.6687057897878997, 0.667432022043438, 0.6439565950987451, 0.6521174433265058, 0.6389426410601193, 0.6599461487926584], 'train_acc': [32.79004054220074, 71.3979551442183, 77.07795842035573, 79.21017786696152, 80.85507187026496, 81.85019861583193, 82.60780539743642, 83.20843059366341, 83.6943909797016, 83.93191094366409], 'test_acc': [61.84695759065765, 75.54164105716042, 78.83374308543331, 80.0437922556853, 81.21158574062692, 81.4958512599877, 82.27950215119853, 82.23724646588813, 82.84419176398279, 82.25645359557468], 'layers': 1, 'lr': 0.01, 'batch': 32}, 'L1_LR0.01_B64': {'train_loss': [2.2367250586173744, 1.690075288643026, 1.0235758741799195, 0.826715920973119, 0.7282457510978526, 0.6708746816822249, 0.632282771337647, 0.604064018663178, 0.5847716398818575, 0.5672200666311724], 'test_loss': [2.199187685246107, 1.18509003657392, 0.908388909930163, 0.7852139656867583, 0.7311904872203796, 0.7160168966048323, 0.6856764755002839, 0.6712043038452046, 0.6837060156231947, 0.6543351370788603], 'train_acc': [18.773632553885637, 44.23877581664551, 69.69709379308462, 75.81664550827907, 78.98357836111225, 80.7881840643215, 81.7136928894167, 82.66786791705913, 83.33674597649372, 83.76946912922998], 'test_acc': [19.687307928703135, 64.83174554394591, 73.19837123540258, 77.58912108174555, 79.55593116164721, 80.03995082974801, 81.26920712968654, 81.70712968653964, 81.33835279655808, 82.21419791026429], 'layers': 1, 'lr': 0.01, 'batch': 64}, 'L1_LR0.01_B128': {'train_loss': [2.2448905301555877, 2.2153553618571387, 1.9534326975450116, 1.332635361130229, 1.0637273725671876, 0.9519485163386312, 0.880040327951204, 0.8303987663457143, 0.7855065741984484, 0.7514095307145031], 'test_loss': [2.222089590730298, 2.158535822813835, 1.657265810858053, 1.110484111140532, 0.9865220908966252, 0.8869055861260309, 0.8580400070385034, 0.8167615709087932, 0.7892594951460826, 0.7760458397000717], 'train_acc': [18.92378885294238, 20.193292108603956, 34.15646286361713, 59.04555196090476, 68.58457212280055, 72.23337018987947, 74.58809397054206, 75.98318249450564, 77.40284204922396, 78.53720463573447], 'test_acc': [19.587430854333128, 21.55039950829748, 42.3440381069453, 66.57191149354641, 70.45175169022741, 74.1740934234788, 74.92701290719116, 76.3060848186847, 77.62369391518132, 77.60448678549477], 'layers': 1, 'lr': 0.01, 'batch': 128}, 'L1_LR0.01_B256': {'train_loss': [2.2457596245429334, 2.2340565911397996, 2.2091617127647676, 2.0727248022824707, 1.7968837726431384, 1.5014047347589814, 1.291197598023622, 1.165331141507741, 1.0854460818258178, 1.0250717859517358], 'test_loss': [2.2282822159952964, 2.2170708415401297, 2.1670841092189983, 2.0270061563258532, 1.635457893053886, 1.4319067624132547, 1.2428411801460701, 1.269356955614424, 1.2062144691253924, 1.1526279147593024], 'train_acc': [18.754521752187504, 18.921058738414075, 19.184514790395458, 26.512142184364635, 39.370981612678655, 52.28578838882291, 60.83514203420833, 65.3998935255334, 68.36752801780035, 70.12299165950012], 'test_acc': [19.587430854333128, 19.587430854333128, 20.282728948985863, 27.085894283958204, 49.29317762753534, 54.778733866011066, 62.20421020282729, 61.435925015365704, 63.940534726490476, 64.84711124769514], 'layers': 1, 'lr': 0.01, 'batch': 256}, 'L2_LR0.2_B32': {'train_loss': [2.2395166671934974, 2.2389108878459614, 2.2388412871775345, 2.2392813337619404, 2.2391401587552306, 2.239404615870382, 2.2393240051971732, 2.239281271648851, 2.236841731769443, 2.2392080076897125], 'test_loss': [2.2279174745485837, 2.224797972155702, 2.22619472930383, 2.2293802409649777, 2.227720864066773, 2.223926064128442, 2.2256264384714606, 2.22473166101757, 2.226736751596255, 2.2265817044919887], 'train_acc': [18.884202192281965, 18.86782150511214, 18.926518967470685, 18.85553598973477, 18.87874196322536, 18.87874196322536, 18.903312993980098, 18.891027478602727, 18.987946544357534, 18.87874196322536], 'test_acc': [19.587430854333128, 19.587430854333128, 19.587430854333128, 19.587430854333128, 19.587430854333128, 19.587430854333128, 19.587430854333128, 19.587430854333128, 19.587430854333128, 19.587430854333128], 'layers': 2, 'lr': 0.2, 'batch': 32}, 'L2_LR0.2_B64': {'train_loss': [1.767924857832481, 1.0779770065512888, 0.9569190796840226, 0.9087913973380237, 0.8910414842777067, 0.8624188494346768, 0.8398705971990033, 0.8017405681863832, 0.7989430077410493, 0.7746696347176971], 'test_loss': [1.0702408404165642, 1.024042200587905, 1.0307822719848294, 0.833776076072993, 0.8864980378470394, 0.8444019567768634, 0.8382259414616581, 0.816489846228674, 0.7372548811334587, 0.8055820728857677], 'train_acc': [38.773086530979974, 66.28308557543988, 70.51885826610426, 72.20060881553981, 72.79031355365358, 73.44827115497495, 74.47069904582497, 75.80845516469415, 75.95042112016598, 76.55650654544958], 'test_acc': [67.21342962507683, 68.2160417947142, 68.18915181315305, 74.3738475722188, 73.82452366318377, 74.10110633066995, 75.04609711124769, 75.86432083589429, 78.52642901044868, 76.65565457897972], 'layers': 2, 'lr': 0.2, 'batch': 64}, 'L2_LR0.2_B128': {'train_loss': [1.6405788265646781, 0.7684511610812259, 0.6419321566928765, 0.5742672004394203, 0.5328458076040394, 0.5112364657647028, 0.48766582340755893, 0.47985492853703915, 0.4487908095463518, 0.4549174223499506], 'test_loss': [0.8934154420376703, 0.8023953771913572, 0.6325575746343997, 0.8411120267169345, 0.5606474386964316, 0.5673944332076496, 0.5653814588797628, 0.5769288476874803, 0.5624444404114369, 0.5707808352718342], 'train_acc': [43.58900855890904, 76.86227937261968, 80.9711017377179, 82.81392904432342, 84.0288300094189, 84.70999358423086, 85.40753784621265, 85.7433419331941, 86.61288341045906, 86.15968439876053], 'test_acc': [73.49031960663798, 76.83620159803318, 82.16041794714198, 75.26505838967425, 84.36923786109404, 84.43070067609096, 84.47295636140136, 83.9582052858021, 84.95313460356485, 83.71235402581438], 'layers': 2, 'lr': 0.2, 'batch': 128}, 'L2_LR0.2_B256': {'train_loss': [2.17355775143328, 0.9891443631165293, 0.6324155098616671, 0.5390800462056872, 0.49050175869084256, 0.46227553756422834, 0.4341139568400809, 0.40765241431172405, 0.3934791365314624, 0.3795859946003921], 'test_loss': [1.8630511913111991, 0.8558157081609865, 0.74366744794066, 0.6176259662010366, 0.5166110846259218, 0.8462863585373047, 0.5306358388324567, 0.5019871629889053, 0.4682677919394265, 0.4957890625592017], 'train_acc': [21.76174290511487, 68.9517725268575, 81.14582906752938, 83.8459123360225, 85.6546132110242, 86.18971565857187, 86.97052841366695, 87.83733977640362, 88.226381096687, 88.38882291112112], 'test_acc': [42.70897357098955, 74.43146896127843, 78.04240934234788, 82.54071911493547, 85.37953288260603, 73.40580823601721, 84.89935464044254, 86.53580208973571, 86.94683466502765, 86.51275353411187], 'layers': 2, 'lr': 0.2, 'batch': 256}, 'L2_LR0.1_B32': {'train_loss': [1.4451494988560578, 0.9417628029444435, 0.8660686641763925, 0.7913564401950356, 0.7402207821638914, 0.7128895306701292, 0.6929509200872042, 0.6747726157667863, 0.6704580558710489, 0.6561499407047419], 'test_loss': [1.1931662975679385, 1.372540849663395, 0.7821272703204774, 0.7710121832673508, 0.7191838228270255, 0.7611996690847423, 0.8791214376413243, 0.6632161728062363, 0.7185428649571752, 0.6880312646981317], 'train_acc': [51.67424273448271, 70.98843796497263, 73.25170290893703, 75.8384864245055, 77.37281078941261, 78.27238352648894, 79.08049742686705, 79.85721501016968, 79.8790559263961, 80.3936825149815], 'test_acc': [64.99308543331284, 56.13859864781807, 76.6479717271051, 77.44698832206515, 79.30239704978489, 77.86570374923171, 73.04471419791027, 80.88890596189306, 81.09634296250768, 80.74293177627536], 'layers': 2, 'lr': 0.1, 'batch': 32}, 'L2_LR0.1_B64': {'train_loss': [1.5592220402551353, 0.6552436873781555, 0.5468620327510771, 0.494918755847777, 0.46506069245531045, 0.44512920179109566, 0.42482318130619934, 0.40876248470915816, 0.40089875932597835, 0.38782766806924235], 'test_loss': [0.7370741046420182, 0.6733781301176613, 0.5930174382537439, 0.5034753391181462, 0.5478716585705012, 0.5771259624824688, 0.49386329407624546, 0.4905761828727347, 0.4823020969934821, 0.5113711119357545], 'train_acc': [46.3109327436286, 80.58752064649111, 83.73397764036201, 85.20414431385396, 86.15968439876053, 86.70297718989312, 87.10293896828972, 87.7376905961205, 87.87692643706403, 88.25368224197005], 'test_acc': [78.56484326982176, 80.65073755377996, 83.06315304240934, 85.97111247695145, 84.28472649047326, 82.7980946527351, 86.39751075599263, 86.70482483097726, 87.32713583282114, 86.48970497848802], 'layers': 2, 'lr': 0.1, 'batch': 64}, 'L2_LR0.1_B128': {'train_loss': [2.2393437530002527, 1.8911813394553096, 0.8125881204101363, 0.6670202076901304, 0.5923549286683603, 0.5446621292057942, 0.5102277079106569, 0.4847106306112416, 0.465406810883647, 0.44499682646795446], 'test_loss': [2.22442280330318, 1.0279090330508418, 0.7436771445942569, 0.6494824546352075, 0.6261157525706745, 0.6248942513588974, 0.5677072733695623, 0.5721062278190712, 0.5345987537995979, 0.5454274008775241], 'train_acc': [18.832330016244182, 33.75786614248468, 75.37709706922206, 79.80943800592435, 82.35799991809657, 83.92508565734333, 84.82192827989134, 85.60683620677888, 86.11736762357181, 86.74256385055354], 'test_acc': [19.587430854333128, 70.03687768899816, 78.08466502765826, 81.19237861094038, 82.69053472649047, 82.42547633681623, 83.93899815611555, 83.78149969268593, 85.11063306699447, 84.89167178856792], 'layers': 2, 'lr': 0.1, 'batch': 128}, 'L2_LR0.1_B256': {'train_loss': [2.1785552803440353, 0.942870823862362, 0.6137705420709622, 0.5197635977625926, 0.45889212914815136, 0.4231225410766058, 0.3998891776395208, 0.37535301684317834, 0.35704709754324315, 0.34046918394355435], 'test_loss': [1.5818082061221281, 0.6954355237303735, 0.6318792202169351, 0.6074799459333718, 0.48794800823425033, 0.5195161418527933, 0.518740448837421, 0.42575188886921467, 0.45765129162185236, 0.46380398540168577], 'train_acc': [21.62796729322795, 71.26281447506723, 82.0399415755491, 84.66358163724968, 86.5978677805534, 87.60664509876189, 88.15130294715863, 88.94303616036693, 89.50134458140519, 89.93679784866976], 'test_acc': [48.8859864781807, 80.50476336816226, 82.37553779963122, 82.62523048555624, 86.4820221266134, 84.66118623232944, 84.81868469575906, 87.79963122311001, 86.74323909035034, 86.95067609096496], 'layers': 2, 'lr': 0.1, 'batch': 256}, 'L2_LR0.01_B32': {'train_loss': [2.139614249264489, 1.05985576352364, 0.6250713216499248, 0.5395096074639494, 0.48481157878963893, 0.44943496727156795, 0.4217274029766496, 0.39415126941197137, 0.3771850716921752, 0.3635418441382618], 'test_loss': [1.5562788364560456, 0.7013362267666531, 0.559930120719014, 0.5107714255931705, 0.4794400903844511, 0.46439197051356595, 0.43099844274377264, 0.4210660183583584, 0.4228978366044378, 0.4461000522148704], 'train_acc': [23.558158264739205, 67.76690282157337, 81.80242161158661, 84.03019506668305, 85.5918205768732, 86.63608938394965, 87.46604420055421, 88.18133420696998, 88.69459573829123, 89.15325497904637], 'test_acc': [51.81315304240934, 80.4279348494161, 83.58942839582053, 85.18362015980333, 85.63306699446835, 86.9237861094038, 87.42701290719116, 87.88030116779349, 87.89950829748003, 87.39628149969269], 'layers': 2, 'lr': 0.01, 'batch': 32}, 'L2_LR0.01_B64': {'train_loss': [2.2405302589657654, 1.7739995165135616, 0.8697058687734781, 0.6726918657695603, 0.5928703914556713, 0.5418609795506324, 0.5026867864827002, 0.47500513180052356, 0.4546137941056616, 0.4333510607375637], 'test_loss': [2.219089388481004, 1.0402758593345318, 0.7315121445101906, 0.6396899711301741, 0.5694264541320003, 0.5396982836225096, 0.5254545654339078, 0.5105770371336841, 0.48767314645213883, 0.4858750708643908], 'train_acc': [18.866456447847987, 39.573010087773184, 74.81742359091965, 80.76907326262337, 83.07602003904064, 84.52980602536276, 85.45531485045798, 86.18016025772282, 86.72891327791201, 87.2981421570635], 'test_acc': [19.587430854333128, 70.34419176398279, 79.82098955132145, 82.35248924400737, 84.12338660110633, 84.77642901044868, 85.05301167793485, 85.54855562384758, 86.05178242163491, 86.06714812538414], 'layers': 2, 'lr': 0.01, 'batch': 64}, 'L2_LR0.01_B128': {'train_loss': [2.245143362434246, 2.23609324442933, 2.2021401841073174, 1.6791441346102685, 1.0011911633618171, 0.792708627774762, 0.6836532515931067, 0.619877477319319, 0.5791662599307283, 0.5503654659045817], 'test_loss': [2.2260752278159717, 2.2190752885205365, 2.091983963392524, 1.169878275686052, 0.8544325455611076, 0.7129453837908186, 0.6289702286438922, 0.5968992816720481, 0.5684906737014569, 0.5354190943573762], 'train_acc': [18.47058984124384, 18.921058738414075, 20.205577623981327, 44.57731001815526, 70.29635393204745, 77.12437036733691, 80.40050780130227, 82.17781235922847, 83.52102870715426, 84.19263688111717], 'test_acc': [19.587430854333128, 19.587430854333128, 26.321450522433928, 65.18899815611555, 75.66456668715428, 79.75184388444991, 82.60218192993239, 83.35510141364475, 84.22326367547633, 85.2681315304241], 'layers': 2, 'lr': 0.01, 'batch': 128}, 'L2_LR0.01_B256': {'train_loss': [2.2549701139457183, 2.2402245794322275, 2.2372066260363774, 2.23196642986964, 2.211020709265508, 2.03594583935479, 1.452800480433897, 1.0026871789489007, 0.8356507535425476, 0.7499682075796015], 'test_loss': [2.229573540256737, 2.2274943306611665, 2.2223756077396555, 2.2129315359869395, 2.164039447692984, 1.780845544348484, 1.1286849035539728, 0.8762901893186658, 0.7942882875540392, 0.716574584331407], 'train_acc': [17.470002866620256, 18.921058738414075, 18.922423795678228, 18.987946544357534, 20.706553639925193, 30.62642477851946, 54.630956768636445, 70.83555155138758, 76.03368961327928, 78.87300872271592], 'test_acc': [19.587430854333128, 19.587430854333128, 19.587430854333128, 19.764136447449292, 23.839889366933004, 40.3119237861094, 66.18776889981561, 75.74523663183774, 76.96296865396435, 80.64305470190534], 'layers': 2, 'lr': 0.01, 'batch': 256}, 'L3_LR0.2_B32': {'train_loss': [2.1801227022809146, 2.239284584471704, 2.2394625643201587, 2.2393172066144365, 2.2394321517858935, 2.239320669271896, 2.2389990804021513, 2.239325526364142, 2.239049323310687, 2.2392037588256954], 'test_loss': [2.2266982025212134, 2.2288316745441534, 2.228157551946564, 2.226500866086335, 2.2254434294252214, 2.2290047157301665, 2.224790981377131, 2.2288586177485956, 2.22722809931656, 2.226748112747108], 'train_acc': [21.64025280860532, 18.854170932470616, 18.896487707659336, 18.89921782218764, 18.88010702048951, 18.912868394829164, 18.884202192281965, 18.881472077753664, 18.885567249546117, 18.871916676904597], 'test_acc': [19.587430854333128, 19.587430854333128, 19.587430854333128, 19.587430854333128, 19.587430854333128, 19.587430854333128, 19.587430854333128, 19.587430854333128, 19.587430854333128, 19.587430854333128], 'layers': 3, 'lr': 0.2, 'batch': 32}, 'L3_LR0.2_B64': {'train_loss': [1.4995603842887408, 0.8207575448321865, 0.7557243338162628, 0.7217062112601837, 0.699806861752011, 0.7033989581213209, 0.7014562592081429, 0.6878420108260339, 0.6815350510436597, 0.6618426032113667], 'test_loss': [0.8770237698024522, 0.7527943627191015, 0.7321344503758506, 0.678490316787232, 0.7111536820167842, 0.6798521398252406, 0.7581329412011919, 0.6897137247863985, 0.741327097317743, 1.149634773773728], 'train_acc': [48.84447902589513, 75.08497481469348, 77.27862183818611, 78.09356102488499, 79.09414799950858, 78.86891355092347, 78.81294620309322, 79.45452311724476, 79.5596325265845, 80.14797220743411], 'test_acc': [74.92701290719116, 78.61478180700676, 79.1679471419791, 81.28841425937308, 79.49062692071297, 80.72372464658882, 78.57252612169637, 80.17055931161647, 78.46880762138906, 64.2478488014751], 'layers': 3, 'lr': 0.2, 'batch': 64}, 'L3_LR0.2_B128': {'train_loss': [1.7566007650531377, 0.7982185637211952, 0.661230995722852, 0.6647315950565648, 2.2391578495914986, 2.2370576332520113, 2.2370765904988272, 2.237201730893559, 2.2371913553421705, 2.2372272401909385], 'test_loss': [0.867210860844397, 0.6647886703112554, 0.7779551315102463, 2.2324504230970534, 2.2243572846467172, 2.2265755862052554, 2.2264138982570016, 2.2246045570983135, 2.2247295608239153, 2.2240652991132497], 'train_acc': [38.84406950871589, 75.39757292818433, 79.78350191790545, 79.34395347884843, 18.865091390583835, 18.916963566621618, 18.921058738414075, 18.926518967470685, 18.916963566621618, 18.91832862388577], 'test_acc': [74.28549477566072, 80.36647203441917, 75.12292562999386, 19.587430854333128, 19.587430854333128, 19.587430854333128, 19.587430854333128, 19.587430854333128, 19.587430854333128, 19.587430854333128], 'layers': 3, 'lr': 0.2, 'batch': 128}, 'L3_LR0.2_B256': {'train_loss': [2.0551645844763495, 0.893136299760407, 1.3664558536550058, 2.236904131275418, 2.236935200737373, 2.23679225638472, 2.2368943719112613, 2.2369139547119032, 2.236833891125317, 2.2368855583584275], 'test_loss': [1.382368630841237, 0.7582662592069019, 2.2240710712754073, 2.224587504674704, 2.2242680897211544, 2.224626010081192, 2.2242608146573697, 2.224177184101989, 2.2241087912340873, 2.2247915977191983], 'train_acc': [26.598140792006223, 72.42174809233248, 53.03793494137079, 18.89375759313103, 18.895122650395184, 18.901947936715946, 18.921058738414075, 18.921058738414075, 18.921058738414075, 18.922423795678228], 'test_acc': [54.06038721573448, 76.36370620774431, 19.587430854333128, 19.587430854333128, 19.587430854333128, 19.587430854333128, 19.587430854333128, 19.587430854333128, 19.587430854333128, 19.587430854333128], 'layers': 3, 'lr': 0.2, 'batch': 256}, 'L3_LR0.1_B32': {'train_loss': [1.4111372364335915, 0.7495843160993655, 0.689246101009157, 0.6619965531385451, 0.642160426570057, 0.6335008394197931, 0.6407448752246149, 0.628596765021069, 0.6307433963310957, 0.6561524311857675], 'test_loss': [0.9151756501153854, 0.7611096542381258, 0.8680321514496572, 0.6847787397217003, 0.6405895492308112, 0.5987495873409616, 0.6661565323218878, 0.7615998777496207, 0.6297378472264882, 0.773877636596478], 'train_acc': [52.02233233684153, 77.05475244686515, 79.14465511828222, 79.97734004941508, 80.6625987960195, 81.05300517356703, 80.70764568573652, 81.31236605375595, 81.14173389573692, 80.65440845243458], 'test_acc': [71.40826674861708, 78.39966195451751, 73.2098955132145, 79.840196681008, 81.27688998156115, 82.81346035648433, 80.87738168408113, 78.30746773202213, 82.16425937307929, 77.33942839582053], 'layers': 3, 'lr': 0.1, 'batch': 32}, 'L3_LR0.1_B64': {'train_loss': [1.3827796438193132, 0.9480399631683566, 0.6071767324666233, 0.5445302557603198, 0.5183291012321046, 0.4993408597502435, 0.47711441586751413, 0.46316125583972184, 0.4526560225360251, 0.4444709203089055], 'test_loss': [0.7396125815437847, 0.669910443437268, 0.599775525676596, 0.5269730814630148, 0.5068262621861114, 0.4849726096867196, 0.4962348894392997, 0.509288232569762, 0.5053516972555877, 0.5010950257240371], 'train_acc': [52.98469770806885, 69.36401982063147, 81.56763176215242, 83.5483298524373, 84.38237984083432, 85.11132041989161, 85.6518830964959, 86.08733636376046, 86.27298415168516, 86.58148709338357], 'test_acc': [77.24339274738783, 79.30623847572218, 82.65596189305471, 84.26551936078673, 84.8340503995083, 85.50629993853718, 85.7521511985249, 85.1221573448064, 85.64459127228027, 85.95958819913952], 'layers': 3, 'lr': 0.1, 'batch': 64}, 'L3_LR0.1_B128': {'train_loss': [1.8333661459795303, 0.5784080783133929, 0.4721909979674897, 0.4205518697265039, 0.38949538180847526, 0.3635286544209032, 0.34217577746744465, 0.3229788486470539, 0.31364858298198073, 0.3041728405276028], 'test_loss': [0.8442760964588806, 0.6420195986320435, 0.4910162482880051, 0.39612771177775447, 0.4266273027266471, 0.40220378047128946, 0.39451215651112975, 0.39762283605194265, 0.3955260695822047, 0.3797114973392088], 'train_acc': [35.6088837926751, 82.6542173444176, 85.85937180064704, 87.26538078272384, 88.29326890263046, 89.03449499706512, 89.64331053687702, 90.25758630574552, 90.57291453376469, 90.7899586387649], 'test_acc': [74.41226183159189, 80.53165334972341, 85.69452980946528, 88.3259065765212, 87.31561155500921, 88.10310387215735, 88.64090350338046, 88.82913337430854, 88.83297480024585, 89.23248309772588], 'layers': 3, 'lr': 0.1, 'batch': 128}, 'L3_LR0.1_B256': {'train_loss': [2.240062038733495, 2.1414055309133317, 0.9568803194434666, 0.5563877417233547, 0.45844626696824436, 0.40734445370935823, 0.37103572103183974, 0.34794720282762204, 0.3278768746964457, 0.3093099347459959], 'test_loss': [2.2224534807867555, 1.5607153349444407, 0.7160064839261693, 0.5402558950756544, 0.6470846882171256, 0.4462603564707868, 0.46895158605921394, 0.39373039598470827, 0.3946165401252707, 0.3807305332408335], 'train_acc': [18.779092782942243, 23.031246160776444, 70.86694786846309, 83.4514107866825, 86.33850690036446, 87.8864818379131, 88.93484581678202, 89.6296599642355, 90.24530079036816, 90.78313335244414], 'test_acc': [19.587430854333128, 49.44299323909035, 78.78764597418562, 83.60479409956976, 80.01690227412415, 87.04671173939767, 85.78288260602335, 88.85218192993239, 88.96358328211431, 89.17870313460357], 'layers': 3, 'lr': 0.1, 'batch': 256}, 'L3_LR0.01_B32': {'train_loss': [2.2394317527910657, 1.5798450816261347, 0.6186540975524155, 0.46837064229299863, 0.4101485925713206, 0.37483369090994445, 0.34836763485938443, 0.3318096548768398, 0.31520983690968624, 0.30208679384116255], 'test_loss': [2.2232748555052697, 0.9698321117384711, 0.5244428391609004, 0.4259542886765368, 0.40524651770512604, 0.3756107192832264, 0.35617083913721626, 0.3617894607742066, 0.3490611096064445, 0.33966664304085575], 'train_acc': [18.87874196322536, 45.981953942967905, 81.45160189469948, 86.01362327149624, 87.86737103621498, 88.78195940319696, 89.58597813178262, 90.06920840329252, 90.56881936197223, 91.0670652633878], 'test_acc': [19.587430854333128, 69.8140749846343, 84.1080208973571, 87.31561155500921, 87.99170251997542, 89.12876459741857, 89.68961278426552, 89.67808850645359, 89.88552550706822, 90.33881376767056], 'layers': 3, 'lr': 0.01, 'batch': 32}, 'L3_LR0.01_B64': {'train_loss': [2.2456099436425236, 2.235762775198974, 1.9145999704409755, 0.8646629147013626, 0.585644250798011, 0.4966147550648784, 0.4478265847627606, 0.41143569300570043, 0.3867745265558496, 0.36605052735427107], 'test_loss': [2.2244490003322044, 2.220036016334107, 1.1617939382809837, 0.6664493663514693, 0.5378933537570553, 0.4730592109194253, 0.4371300020061099, 0.402398833502416, 0.38783572256088844, 0.3799500028158011], 'train_acc': [18.276751709734224, 18.921058738414075, 32.796865828521504, 73.92194602563578, 82.7170099785686, 85.35976084196732, 86.6947868463082, 87.81959403196964, 88.44888543074381, 89.16417543715959], 'test_acc': [19.587430854333128, 19.587430854333128, 64.17486170866626, 80.57390903503381, 84.13875230485556, 86.06330669944684, 86.9660417947142, 88.28365089121081, 88.62553779963122, 88.86754763368162], 'layers': 3, 'lr': 0.01, 'batch': 64}, 'L3_LR0.01_B128': {'train_loss': [2.252601911741843, 2.2383126876980297, 2.2375482185087057, 2.236562714363288, 2.233552321830907, 2.104402609268645, 1.2731301027181565, 0.8828550218569227, 0.7664268140451477, 0.6793064641993521], 'test_loss': [2.227229692192992, 2.2263897234630057, 2.2239568275986232, 2.223345459350136, 2.2138025844177736, 1.6814305968422354, 1.009952978261351, 0.833083894770059, 0.7391459833425323, 0.6692558827397277], 'train_acc': [17.82082258350738, 18.921058738414075, 18.921058738414075, 18.921058738414075, 18.921058738414075, 25.155275263797318, 59.921918724490496, 73.55201550705053, 77.18989311601622, 79.76302605894317], 'test_acc': [19.587430854333128, 19.587430854333128, 19.587430854333128, 19.587430854333128, 19.587430854333128, 42.924093423478794, 69.69499078057775, 76.02181929932391, 78.94130301167793, 80.82360172095882], 'layers': 3, 'lr': 0.01, 'batch': 128}, 'L3_LR0.01_B256': {'train_loss': [2.2568326387498217, 2.2400342836468137, 2.238550377091919, 2.2375592036503518, 2.236290774409252, 2.233329945974122, 2.2237423057434564, 2.1463539785133126, 1.5823365846325874, 1.016262616909359], 'test_loss': [2.230180842640507, 2.226869644969244, 2.2247279365881867, 2.225393837774613, 2.221529962097712, 2.2178216144653207, 2.1974604005233305, 1.9810524678479402, 1.2425278865565679, 1.173905170176318], 'train_acc': [17.407210232469254, 18.921058738414075, 18.921058738414075, 18.921058738414075, 18.921058738414075, 18.921058738414075, 18.963375513602795, 23.26740106747478, 49.08472910438593, 69.74760091185826], 'test_acc': [19.587430854333128, 19.587430854333128, 19.587430854333128, 19.587430854333128, 19.587430854333128, 19.587430854333128, 19.65657652120467, 34.20021511985249, 62.05823601720959, 64.91241548862938], 'layers': 3, 'lr': 0.01, 'batch': 256}}\n"
     ]
    }
   ],
   "execution_count": 127
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Sinnvoll wäre natürlich ein Dataframe, der die Anzahl der Layer, die learning Rates und die Batch-Sizes sowie die Ergebnisse der letzten Epoche eines jeden Experiments darstellt.",
   "id": "14e572bc0003e51d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T08:45:00.112714Z",
     "start_time": "2025-11-30T08:45:00.095998Z"
    }
   },
   "cell_type": "code",
   "source": [
    "rows = []\n",
    "for exp_name, res in ALL_RESULTS.items():\n",
    "    rows.append({\n",
    "        \"layers\": res[\"layers\"],\n",
    "        \"learning_rate\": res[\"lr\"],\n",
    "        \"batch_size\": res[\"batch\"],\n",
    "        \"train_loss_last\": res[\"train_loss\"][-1],\n",
    "        \"train_acc_last\": res[\"train_acc\"][-1],\n",
    "        \"test_loss_last\": res[\"test_loss\"][-1],\n",
    "        \"test_acc_last\": res[\"test_acc\"][-1]\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "df"
   ],
   "id": "4a21de3a93948488",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "    layers  learning_rate  batch_size  train_loss_last  train_acc_last  \\\n",
       "0        1           0.20          32         2.239215       18.899218   \n",
       "1        1           0.20          64         1.595963       47.158633   \n",
       "2        1           0.20         128         1.063148       66.128834   \n",
       "3        1           0.20         256         0.787616       76.340827   \n",
       "4        1           0.10          32         1.342608       56.918793   \n",
       "5        1           0.10          64         0.836623       74.488445   \n",
       "6        1           0.10         128         0.659064       80.360921   \n",
       "7        1           0.10         256         0.620938       81.690487   \n",
       "8        1           0.01          32         0.552753       83.931911   \n",
       "9        1           0.01          64         0.567220       83.769469   \n",
       "10       1           0.01         128         0.751410       78.537205   \n",
       "11       1           0.01         256         1.025072       70.122992   \n",
       "12       2           0.20          32         2.239208       18.878742   \n",
       "13       2           0.20          64         0.774670       76.556507   \n",
       "14       2           0.20         128         0.454917       86.159684   \n",
       "15       2           0.20         256         0.379586       88.388823   \n",
       "16       2           0.10          32         0.656150       80.393683   \n",
       "17       2           0.10          64         0.387828       88.253682   \n",
       "18       2           0.10         128         0.444997       86.742564   \n",
       "19       2           0.10         256         0.340469       89.936798   \n",
       "20       2           0.01          32         0.363542       89.153255   \n",
       "21       2           0.01          64         0.433351       87.298142   \n",
       "22       2           0.01         128         0.550365       84.192637   \n",
       "23       2           0.01         256         0.749968       78.873009   \n",
       "24       3           0.20          32         2.239204       18.871917   \n",
       "25       3           0.20          64         0.661843       80.147972   \n",
       "26       3           0.20         128         2.237227       18.918329   \n",
       "27       3           0.20         256         2.236886       18.922424   \n",
       "28       3           0.10          32         0.656152       80.654408   \n",
       "29       3           0.10          64         0.444471       86.581487   \n",
       "30       3           0.10         128         0.304173       90.789959   \n",
       "31       3           0.10         256         0.309310       90.783133   \n",
       "32       3           0.01          32         0.302087       91.067065   \n",
       "33       3           0.01          64         0.366051       89.164175   \n",
       "34       3           0.01         128         0.679306       79.763026   \n",
       "35       3           0.01         256         1.016263       69.747601   \n",
       "\n",
       "    test_loss_last  test_acc_last  \n",
       "0         2.230143      19.587431  \n",
       "1         1.591065      48.997388  \n",
       "2         1.309597      60.921174  \n",
       "3         1.037928      70.194376  \n",
       "4         1.496626      52.581438  \n",
       "5         0.962749      72.241856  \n",
       "6         0.726805      80.447142  \n",
       "7         0.795428      77.808082  \n",
       "8         0.659946      82.256454  \n",
       "9         0.654335      82.214198  \n",
       "10        0.776046      77.604487  \n",
       "11        1.152628      64.847111  \n",
       "12        2.226582      19.587431  \n",
       "13        0.805582      76.655655  \n",
       "14        0.570781      83.712354  \n",
       "15        0.495789      86.512754  \n",
       "16        0.688031      80.742932  \n",
       "17        0.511371      86.489705  \n",
       "18        0.545427      84.891672  \n",
       "19        0.463804      86.950676  \n",
       "20        0.446100      87.396281  \n",
       "21        0.485875      86.067148  \n",
       "22        0.535419      85.268132  \n",
       "23        0.716575      80.643055  \n",
       "24        2.226748      19.587431  \n",
       "25        1.149635      64.247849  \n",
       "26        2.224065      19.587431  \n",
       "27        2.224792      19.587431  \n",
       "28        0.773878      77.339428  \n",
       "29        0.501095      85.959588  \n",
       "30        0.379711      89.232483  \n",
       "31        0.380731      89.178703  \n",
       "32        0.339667      90.338814  \n",
       "33        0.379950      88.867548  \n",
       "34        0.669256      80.823602  \n",
       "35        1.173905      64.912415  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>layers</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>train_loss_last</th>\n",
       "      <th>train_acc_last</th>\n",
       "      <th>test_loss_last</th>\n",
       "      <th>test_acc_last</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.20</td>\n",
       "      <td>32</td>\n",
       "      <td>2.239215</td>\n",
       "      <td>18.899218</td>\n",
       "      <td>2.230143</td>\n",
       "      <td>19.587431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.20</td>\n",
       "      <td>64</td>\n",
       "      <td>1.595963</td>\n",
       "      <td>47.158633</td>\n",
       "      <td>1.591065</td>\n",
       "      <td>48.997388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0.20</td>\n",
       "      <td>128</td>\n",
       "      <td>1.063148</td>\n",
       "      <td>66.128834</td>\n",
       "      <td>1.309597</td>\n",
       "      <td>60.921174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0.20</td>\n",
       "      <td>256</td>\n",
       "      <td>0.787616</td>\n",
       "      <td>76.340827</td>\n",
       "      <td>1.037928</td>\n",
       "      <td>70.194376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0.10</td>\n",
       "      <td>32</td>\n",
       "      <td>1.342608</td>\n",
       "      <td>56.918793</td>\n",
       "      <td>1.496626</td>\n",
       "      <td>52.581438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>0.10</td>\n",
       "      <td>64</td>\n",
       "      <td>0.836623</td>\n",
       "      <td>74.488445</td>\n",
       "      <td>0.962749</td>\n",
       "      <td>72.241856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>0.10</td>\n",
       "      <td>128</td>\n",
       "      <td>0.659064</td>\n",
       "      <td>80.360921</td>\n",
       "      <td>0.726805</td>\n",
       "      <td>80.447142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>0.10</td>\n",
       "      <td>256</td>\n",
       "      <td>0.620938</td>\n",
       "      <td>81.690487</td>\n",
       "      <td>0.795428</td>\n",
       "      <td>77.808082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>0.01</td>\n",
       "      <td>32</td>\n",
       "      <td>0.552753</td>\n",
       "      <td>83.931911</td>\n",
       "      <td>0.659946</td>\n",
       "      <td>82.256454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>0.01</td>\n",
       "      <td>64</td>\n",
       "      <td>0.567220</td>\n",
       "      <td>83.769469</td>\n",
       "      <td>0.654335</td>\n",
       "      <td>82.214198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>0.01</td>\n",
       "      <td>128</td>\n",
       "      <td>0.751410</td>\n",
       "      <td>78.537205</td>\n",
       "      <td>0.776046</td>\n",
       "      <td>77.604487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>0.01</td>\n",
       "      <td>256</td>\n",
       "      <td>1.025072</td>\n",
       "      <td>70.122992</td>\n",
       "      <td>1.152628</td>\n",
       "      <td>64.847111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2</td>\n",
       "      <td>0.20</td>\n",
       "      <td>32</td>\n",
       "      <td>2.239208</td>\n",
       "      <td>18.878742</td>\n",
       "      <td>2.226582</td>\n",
       "      <td>19.587431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2</td>\n",
       "      <td>0.20</td>\n",
       "      <td>64</td>\n",
       "      <td>0.774670</td>\n",
       "      <td>76.556507</td>\n",
       "      <td>0.805582</td>\n",
       "      <td>76.655655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2</td>\n",
       "      <td>0.20</td>\n",
       "      <td>128</td>\n",
       "      <td>0.454917</td>\n",
       "      <td>86.159684</td>\n",
       "      <td>0.570781</td>\n",
       "      <td>83.712354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2</td>\n",
       "      <td>0.20</td>\n",
       "      <td>256</td>\n",
       "      <td>0.379586</td>\n",
       "      <td>88.388823</td>\n",
       "      <td>0.495789</td>\n",
       "      <td>86.512754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2</td>\n",
       "      <td>0.10</td>\n",
       "      <td>32</td>\n",
       "      <td>0.656150</td>\n",
       "      <td>80.393683</td>\n",
       "      <td>0.688031</td>\n",
       "      <td>80.742932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2</td>\n",
       "      <td>0.10</td>\n",
       "      <td>64</td>\n",
       "      <td>0.387828</td>\n",
       "      <td>88.253682</td>\n",
       "      <td>0.511371</td>\n",
       "      <td>86.489705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2</td>\n",
       "      <td>0.10</td>\n",
       "      <td>128</td>\n",
       "      <td>0.444997</td>\n",
       "      <td>86.742564</td>\n",
       "      <td>0.545427</td>\n",
       "      <td>84.891672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2</td>\n",
       "      <td>0.10</td>\n",
       "      <td>256</td>\n",
       "      <td>0.340469</td>\n",
       "      <td>89.936798</td>\n",
       "      <td>0.463804</td>\n",
       "      <td>86.950676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2</td>\n",
       "      <td>0.01</td>\n",
       "      <td>32</td>\n",
       "      <td>0.363542</td>\n",
       "      <td>89.153255</td>\n",
       "      <td>0.446100</td>\n",
       "      <td>87.396281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2</td>\n",
       "      <td>0.01</td>\n",
       "      <td>64</td>\n",
       "      <td>0.433351</td>\n",
       "      <td>87.298142</td>\n",
       "      <td>0.485875</td>\n",
       "      <td>86.067148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2</td>\n",
       "      <td>0.01</td>\n",
       "      <td>128</td>\n",
       "      <td>0.550365</td>\n",
       "      <td>84.192637</td>\n",
       "      <td>0.535419</td>\n",
       "      <td>85.268132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2</td>\n",
       "      <td>0.01</td>\n",
       "      <td>256</td>\n",
       "      <td>0.749968</td>\n",
       "      <td>78.873009</td>\n",
       "      <td>0.716575</td>\n",
       "      <td>80.643055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>3</td>\n",
       "      <td>0.20</td>\n",
       "      <td>32</td>\n",
       "      <td>2.239204</td>\n",
       "      <td>18.871917</td>\n",
       "      <td>2.226748</td>\n",
       "      <td>19.587431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>3</td>\n",
       "      <td>0.20</td>\n",
       "      <td>64</td>\n",
       "      <td>0.661843</td>\n",
       "      <td>80.147972</td>\n",
       "      <td>1.149635</td>\n",
       "      <td>64.247849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>3</td>\n",
       "      <td>0.20</td>\n",
       "      <td>128</td>\n",
       "      <td>2.237227</td>\n",
       "      <td>18.918329</td>\n",
       "      <td>2.224065</td>\n",
       "      <td>19.587431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>3</td>\n",
       "      <td>0.20</td>\n",
       "      <td>256</td>\n",
       "      <td>2.236886</td>\n",
       "      <td>18.922424</td>\n",
       "      <td>2.224792</td>\n",
       "      <td>19.587431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>3</td>\n",
       "      <td>0.10</td>\n",
       "      <td>32</td>\n",
       "      <td>0.656152</td>\n",
       "      <td>80.654408</td>\n",
       "      <td>0.773878</td>\n",
       "      <td>77.339428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>3</td>\n",
       "      <td>0.10</td>\n",
       "      <td>64</td>\n",
       "      <td>0.444471</td>\n",
       "      <td>86.581487</td>\n",
       "      <td>0.501095</td>\n",
       "      <td>85.959588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>3</td>\n",
       "      <td>0.10</td>\n",
       "      <td>128</td>\n",
       "      <td>0.304173</td>\n",
       "      <td>90.789959</td>\n",
       "      <td>0.379711</td>\n",
       "      <td>89.232483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>3</td>\n",
       "      <td>0.10</td>\n",
       "      <td>256</td>\n",
       "      <td>0.309310</td>\n",
       "      <td>90.783133</td>\n",
       "      <td>0.380731</td>\n",
       "      <td>89.178703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>3</td>\n",
       "      <td>0.01</td>\n",
       "      <td>32</td>\n",
       "      <td>0.302087</td>\n",
       "      <td>91.067065</td>\n",
       "      <td>0.339667</td>\n",
       "      <td>90.338814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>3</td>\n",
       "      <td>0.01</td>\n",
       "      <td>64</td>\n",
       "      <td>0.366051</td>\n",
       "      <td>89.164175</td>\n",
       "      <td>0.379950</td>\n",
       "      <td>88.867548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>3</td>\n",
       "      <td>0.01</td>\n",
       "      <td>128</td>\n",
       "      <td>0.679306</td>\n",
       "      <td>79.763026</td>\n",
       "      <td>0.669256</td>\n",
       "      <td>80.823602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>3</td>\n",
       "      <td>0.01</td>\n",
       "      <td>256</td>\n",
       "      <td>1.016263</td>\n",
       "      <td>69.747601</td>\n",
       "      <td>1.173905</td>\n",
       "      <td>64.912415</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 131
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "im kommenden wollen wir alle Modelle nach deren Gleichheit gruppieren, um diese besser zu vergleichen",
   "id": "35b96960b75ee5ef"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T08:54:08.367047Z",
     "start_time": "2025-11-30T08:54:08.358899Z"
    }
   },
   "cell_type": "code",
   "source": [
    "MODELS_BY_LAYER = {}\n",
    "RESULTS_BY_LAYER = {}\n",
    "\n",
    "MODELS_BY_LR = {}\n",
    "RESULTS_BY_LR = {}\n",
    "\n",
    "MODELS_BY_BATCH = {}\n",
    "RESULTS_BY_BATCH = {}\n"
   ],
   "id": "53c969e0e44a2723",
   "outputs": [],
   "execution_count": 132
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T08:54:56.740292Z",
     "start_time": "2025-11-30T08:54:56.733063Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for exp_name, result in ALL_RESULTS.items():\n",
    "\n",
    "    layer = result[\"layers\"]\n",
    "    lr = result[\"lr\"]\n",
    "    bs = result[\"batch\"]\n",
    "\n",
    "    # ===== GROUP BY LAYER =====\n",
    "    if layer not in MODELS_BY_LAYER:\n",
    "        MODELS_BY_LAYER[layer] = {}\n",
    "        RESULTS_BY_LAYER[layer] = {}\n",
    "\n",
    "    MODELS_BY_LAYER[layer][exp_name] = ALL_MODELS[exp_name]\n",
    "    RESULTS_BY_LAYER[layer][exp_name] = result\n",
    "\n",
    "    # ===== GROUP BY LEARNING RATE =====\n",
    "    if lr not in MODELS_BY_LR:\n",
    "        MODELS_BY_LR[lr] = {}\n",
    "        RESULTS_BY_LR[lr] = {}\n",
    "\n",
    "    MODELS_BY_LR[lr][exp_name] = ALL_MODELS[exp_name]\n",
    "    RESULTS_BY_LR[lr][exp_name] = result\n",
    "\n",
    "    # ===== GROUP BY BATCH SIZE =====\n",
    "    if bs not in MODELS_BY_BATCH:\n",
    "        MODELS_BY_BATCH[bs] = {}\n",
    "        RESULTS_BY_BATCH[bs] = {}\n",
    "\n",
    "    MODELS_BY_BATCH[bs][exp_name] = ALL_MODELS[exp_name]\n",
    "    RESULTS_BY_BATCH[bs][exp_name] = result\n"
   ],
   "id": "212a69fff7f818ed",
   "outputs": [],
   "execution_count": 133
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "um jetzt z.B. alle Ergebnisse aller Modelle mit 3 Layern zu bekommen führt man folgendes aus",
   "id": "8bec7a353b3ad6ef"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T08:55:07.979204Z",
     "start_time": "2025-11-30T08:55:07.970944Z"
    }
   },
   "cell_type": "code",
   "source": "RESULTS_BY_LAYER[3]",
   "id": "1d675e00f31eb9c4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'L3_LR0.2_B32': {'train_loss': [2.1801227022809146,\n",
       "   2.239284584471704,\n",
       "   2.2394625643201587,\n",
       "   2.2393172066144365,\n",
       "   2.2394321517858935,\n",
       "   2.239320669271896,\n",
       "   2.2389990804021513,\n",
       "   2.239325526364142,\n",
       "   2.239049323310687,\n",
       "   2.2392037588256954],\n",
       "  'test_loss': [2.2266982025212134,\n",
       "   2.2288316745441534,\n",
       "   2.228157551946564,\n",
       "   2.226500866086335,\n",
       "   2.2254434294252214,\n",
       "   2.2290047157301665,\n",
       "   2.224790981377131,\n",
       "   2.2288586177485956,\n",
       "   2.22722809931656,\n",
       "   2.226748112747108],\n",
       "  'train_acc': [21.64025280860532,\n",
       "   18.854170932470616,\n",
       "   18.896487707659336,\n",
       "   18.89921782218764,\n",
       "   18.88010702048951,\n",
       "   18.912868394829164,\n",
       "   18.884202192281965,\n",
       "   18.881472077753664,\n",
       "   18.885567249546117,\n",
       "   18.871916676904597],\n",
       "  'test_acc': [19.587430854333128,\n",
       "   19.587430854333128,\n",
       "   19.587430854333128,\n",
       "   19.587430854333128,\n",
       "   19.587430854333128,\n",
       "   19.587430854333128,\n",
       "   19.587430854333128,\n",
       "   19.587430854333128,\n",
       "   19.587430854333128,\n",
       "   19.587430854333128],\n",
       "  'layers': 3,\n",
       "  'lr': 0.2,\n",
       "  'batch': 32},\n",
       " 'L3_LR0.2_B64': {'train_loss': [1.4995603842887408,\n",
       "   0.8207575448321865,\n",
       "   0.7557243338162628,\n",
       "   0.7217062112601837,\n",
       "   0.699806861752011,\n",
       "   0.7033989581213209,\n",
       "   0.7014562592081429,\n",
       "   0.6878420108260339,\n",
       "   0.6815350510436597,\n",
       "   0.6618426032113667],\n",
       "  'test_loss': [0.8770237698024522,\n",
       "   0.7527943627191015,\n",
       "   0.7321344503758506,\n",
       "   0.678490316787232,\n",
       "   0.7111536820167842,\n",
       "   0.6798521398252406,\n",
       "   0.7581329412011919,\n",
       "   0.6897137247863985,\n",
       "   0.741327097317743,\n",
       "   1.149634773773728],\n",
       "  'train_acc': [48.84447902589513,\n",
       "   75.08497481469348,\n",
       "   77.27862183818611,\n",
       "   78.09356102488499,\n",
       "   79.09414799950858,\n",
       "   78.86891355092347,\n",
       "   78.81294620309322,\n",
       "   79.45452311724476,\n",
       "   79.5596325265845,\n",
       "   80.14797220743411],\n",
       "  'test_acc': [74.92701290719116,\n",
       "   78.61478180700676,\n",
       "   79.1679471419791,\n",
       "   81.28841425937308,\n",
       "   79.49062692071297,\n",
       "   80.72372464658882,\n",
       "   78.57252612169637,\n",
       "   80.17055931161647,\n",
       "   78.46880762138906,\n",
       "   64.2478488014751],\n",
       "  'layers': 3,\n",
       "  'lr': 0.2,\n",
       "  'batch': 64},\n",
       " 'L3_LR0.2_B128': {'train_loss': [1.7566007650531377,\n",
       "   0.7982185637211952,\n",
       "   0.661230995722852,\n",
       "   0.6647315950565648,\n",
       "   2.2391578495914986,\n",
       "   2.2370576332520113,\n",
       "   2.2370765904988272,\n",
       "   2.237201730893559,\n",
       "   2.2371913553421705,\n",
       "   2.2372272401909385],\n",
       "  'test_loss': [0.867210860844397,\n",
       "   0.6647886703112554,\n",
       "   0.7779551315102463,\n",
       "   2.2324504230970534,\n",
       "   2.2243572846467172,\n",
       "   2.2265755862052554,\n",
       "   2.2264138982570016,\n",
       "   2.2246045570983135,\n",
       "   2.2247295608239153,\n",
       "   2.2240652991132497],\n",
       "  'train_acc': [38.84406950871589,\n",
       "   75.39757292818433,\n",
       "   79.78350191790545,\n",
       "   79.34395347884843,\n",
       "   18.865091390583835,\n",
       "   18.916963566621618,\n",
       "   18.921058738414075,\n",
       "   18.926518967470685,\n",
       "   18.916963566621618,\n",
       "   18.91832862388577],\n",
       "  'test_acc': [74.28549477566072,\n",
       "   80.36647203441917,\n",
       "   75.12292562999386,\n",
       "   19.587430854333128,\n",
       "   19.587430854333128,\n",
       "   19.587430854333128,\n",
       "   19.587430854333128,\n",
       "   19.587430854333128,\n",
       "   19.587430854333128,\n",
       "   19.587430854333128],\n",
       "  'layers': 3,\n",
       "  'lr': 0.2,\n",
       "  'batch': 128},\n",
       " 'L3_LR0.2_B256': {'train_loss': [2.0551645844763495,\n",
       "   0.893136299760407,\n",
       "   1.3664558536550058,\n",
       "   2.236904131275418,\n",
       "   2.236935200737373,\n",
       "   2.23679225638472,\n",
       "   2.2368943719112613,\n",
       "   2.2369139547119032,\n",
       "   2.236833891125317,\n",
       "   2.2368855583584275],\n",
       "  'test_loss': [1.382368630841237,\n",
       "   0.7582662592069019,\n",
       "   2.2240710712754073,\n",
       "   2.224587504674704,\n",
       "   2.2242680897211544,\n",
       "   2.224626010081192,\n",
       "   2.2242608146573697,\n",
       "   2.224177184101989,\n",
       "   2.2241087912340873,\n",
       "   2.2247915977191983],\n",
       "  'train_acc': [26.598140792006223,\n",
       "   72.42174809233248,\n",
       "   53.03793494137079,\n",
       "   18.89375759313103,\n",
       "   18.895122650395184,\n",
       "   18.901947936715946,\n",
       "   18.921058738414075,\n",
       "   18.921058738414075,\n",
       "   18.921058738414075,\n",
       "   18.922423795678228],\n",
       "  'test_acc': [54.06038721573448,\n",
       "   76.36370620774431,\n",
       "   19.587430854333128,\n",
       "   19.587430854333128,\n",
       "   19.587430854333128,\n",
       "   19.587430854333128,\n",
       "   19.587430854333128,\n",
       "   19.587430854333128,\n",
       "   19.587430854333128,\n",
       "   19.587430854333128],\n",
       "  'layers': 3,\n",
       "  'lr': 0.2,\n",
       "  'batch': 256},\n",
       " 'L3_LR0.1_B32': {'train_loss': [1.4111372364335915,\n",
       "   0.7495843160993655,\n",
       "   0.689246101009157,\n",
       "   0.6619965531385451,\n",
       "   0.642160426570057,\n",
       "   0.6335008394197931,\n",
       "   0.6407448752246149,\n",
       "   0.628596765021069,\n",
       "   0.6307433963310957,\n",
       "   0.6561524311857675],\n",
       "  'test_loss': [0.9151756501153854,\n",
       "   0.7611096542381258,\n",
       "   0.8680321514496572,\n",
       "   0.6847787397217003,\n",
       "   0.6405895492308112,\n",
       "   0.5987495873409616,\n",
       "   0.6661565323218878,\n",
       "   0.7615998777496207,\n",
       "   0.6297378472264882,\n",
       "   0.773877636596478],\n",
       "  'train_acc': [52.02233233684153,\n",
       "   77.05475244686515,\n",
       "   79.14465511828222,\n",
       "   79.97734004941508,\n",
       "   80.6625987960195,\n",
       "   81.05300517356703,\n",
       "   80.70764568573652,\n",
       "   81.31236605375595,\n",
       "   81.14173389573692,\n",
       "   80.65440845243458],\n",
       "  'test_acc': [71.40826674861708,\n",
       "   78.39966195451751,\n",
       "   73.2098955132145,\n",
       "   79.840196681008,\n",
       "   81.27688998156115,\n",
       "   82.81346035648433,\n",
       "   80.87738168408113,\n",
       "   78.30746773202213,\n",
       "   82.16425937307929,\n",
       "   77.33942839582053],\n",
       "  'layers': 3,\n",
       "  'lr': 0.1,\n",
       "  'batch': 32},\n",
       " 'L3_LR0.1_B64': {'train_loss': [1.3827796438193132,\n",
       "   0.9480399631683566,\n",
       "   0.6071767324666233,\n",
       "   0.5445302557603198,\n",
       "   0.5183291012321046,\n",
       "   0.4993408597502435,\n",
       "   0.47711441586751413,\n",
       "   0.46316125583972184,\n",
       "   0.4526560225360251,\n",
       "   0.4444709203089055],\n",
       "  'test_loss': [0.7396125815437847,\n",
       "   0.669910443437268,\n",
       "   0.599775525676596,\n",
       "   0.5269730814630148,\n",
       "   0.5068262621861114,\n",
       "   0.4849726096867196,\n",
       "   0.4962348894392997,\n",
       "   0.509288232569762,\n",
       "   0.5053516972555877,\n",
       "   0.5010950257240371],\n",
       "  'train_acc': [52.98469770806885,\n",
       "   69.36401982063147,\n",
       "   81.56763176215242,\n",
       "   83.5483298524373,\n",
       "   84.38237984083432,\n",
       "   85.11132041989161,\n",
       "   85.6518830964959,\n",
       "   86.08733636376046,\n",
       "   86.27298415168516,\n",
       "   86.58148709338357],\n",
       "  'test_acc': [77.24339274738783,\n",
       "   79.30623847572218,\n",
       "   82.65596189305471,\n",
       "   84.26551936078673,\n",
       "   84.8340503995083,\n",
       "   85.50629993853718,\n",
       "   85.7521511985249,\n",
       "   85.1221573448064,\n",
       "   85.64459127228027,\n",
       "   85.95958819913952],\n",
       "  'layers': 3,\n",
       "  'lr': 0.1,\n",
       "  'batch': 64},\n",
       " 'L3_LR0.1_B128': {'train_loss': [1.8333661459795303,\n",
       "   0.5784080783133929,\n",
       "   0.4721909979674897,\n",
       "   0.4205518697265039,\n",
       "   0.38949538180847526,\n",
       "   0.3635286544209032,\n",
       "   0.34217577746744465,\n",
       "   0.3229788486470539,\n",
       "   0.31364858298198073,\n",
       "   0.3041728405276028],\n",
       "  'test_loss': [0.8442760964588806,\n",
       "   0.6420195986320435,\n",
       "   0.4910162482880051,\n",
       "   0.39612771177775447,\n",
       "   0.4266273027266471,\n",
       "   0.40220378047128946,\n",
       "   0.39451215651112975,\n",
       "   0.39762283605194265,\n",
       "   0.3955260695822047,\n",
       "   0.3797114973392088],\n",
       "  'train_acc': [35.6088837926751,\n",
       "   82.6542173444176,\n",
       "   85.85937180064704,\n",
       "   87.26538078272384,\n",
       "   88.29326890263046,\n",
       "   89.03449499706512,\n",
       "   89.64331053687702,\n",
       "   90.25758630574552,\n",
       "   90.57291453376469,\n",
       "   90.7899586387649],\n",
       "  'test_acc': [74.41226183159189,\n",
       "   80.53165334972341,\n",
       "   85.69452980946528,\n",
       "   88.3259065765212,\n",
       "   87.31561155500921,\n",
       "   88.10310387215735,\n",
       "   88.64090350338046,\n",
       "   88.82913337430854,\n",
       "   88.83297480024585,\n",
       "   89.23248309772588],\n",
       "  'layers': 3,\n",
       "  'lr': 0.1,\n",
       "  'batch': 128},\n",
       " 'L3_LR0.1_B256': {'train_loss': [2.240062038733495,\n",
       "   2.1414055309133317,\n",
       "   0.9568803194434666,\n",
       "   0.5563877417233547,\n",
       "   0.45844626696824436,\n",
       "   0.40734445370935823,\n",
       "   0.37103572103183974,\n",
       "   0.34794720282762204,\n",
       "   0.3278768746964457,\n",
       "   0.3093099347459959],\n",
       "  'test_loss': [2.2224534807867555,\n",
       "   1.5607153349444407,\n",
       "   0.7160064839261693,\n",
       "   0.5402558950756544,\n",
       "   0.6470846882171256,\n",
       "   0.4462603564707868,\n",
       "   0.46895158605921394,\n",
       "   0.39373039598470827,\n",
       "   0.3946165401252707,\n",
       "   0.3807305332408335],\n",
       "  'train_acc': [18.779092782942243,\n",
       "   23.031246160776444,\n",
       "   70.86694786846309,\n",
       "   83.4514107866825,\n",
       "   86.33850690036446,\n",
       "   87.8864818379131,\n",
       "   88.93484581678202,\n",
       "   89.6296599642355,\n",
       "   90.24530079036816,\n",
       "   90.78313335244414],\n",
       "  'test_acc': [19.587430854333128,\n",
       "   49.44299323909035,\n",
       "   78.78764597418562,\n",
       "   83.60479409956976,\n",
       "   80.01690227412415,\n",
       "   87.04671173939767,\n",
       "   85.78288260602335,\n",
       "   88.85218192993239,\n",
       "   88.96358328211431,\n",
       "   89.17870313460357],\n",
       "  'layers': 3,\n",
       "  'lr': 0.1,\n",
       "  'batch': 256},\n",
       " 'L3_LR0.01_B32': {'train_loss': [2.2394317527910657,\n",
       "   1.5798450816261347,\n",
       "   0.6186540975524155,\n",
       "   0.46837064229299863,\n",
       "   0.4101485925713206,\n",
       "   0.37483369090994445,\n",
       "   0.34836763485938443,\n",
       "   0.3318096548768398,\n",
       "   0.31520983690968624,\n",
       "   0.30208679384116255],\n",
       "  'test_loss': [2.2232748555052697,\n",
       "   0.9698321117384711,\n",
       "   0.5244428391609004,\n",
       "   0.4259542886765368,\n",
       "   0.40524651770512604,\n",
       "   0.3756107192832264,\n",
       "   0.35617083913721626,\n",
       "   0.3617894607742066,\n",
       "   0.3490611096064445,\n",
       "   0.33966664304085575],\n",
       "  'train_acc': [18.87874196322536,\n",
       "   45.981953942967905,\n",
       "   81.45160189469948,\n",
       "   86.01362327149624,\n",
       "   87.86737103621498,\n",
       "   88.78195940319696,\n",
       "   89.58597813178262,\n",
       "   90.06920840329252,\n",
       "   90.56881936197223,\n",
       "   91.0670652633878],\n",
       "  'test_acc': [19.587430854333128,\n",
       "   69.8140749846343,\n",
       "   84.1080208973571,\n",
       "   87.31561155500921,\n",
       "   87.99170251997542,\n",
       "   89.12876459741857,\n",
       "   89.68961278426552,\n",
       "   89.67808850645359,\n",
       "   89.88552550706822,\n",
       "   90.33881376767056],\n",
       "  'layers': 3,\n",
       "  'lr': 0.01,\n",
       "  'batch': 32},\n",
       " 'L3_LR0.01_B64': {'train_loss': [2.2456099436425236,\n",
       "   2.235762775198974,\n",
       "   1.9145999704409755,\n",
       "   0.8646629147013626,\n",
       "   0.585644250798011,\n",
       "   0.4966147550648784,\n",
       "   0.4478265847627606,\n",
       "   0.41143569300570043,\n",
       "   0.3867745265558496,\n",
       "   0.36605052735427107],\n",
       "  'test_loss': [2.2244490003322044,\n",
       "   2.220036016334107,\n",
       "   1.1617939382809837,\n",
       "   0.6664493663514693,\n",
       "   0.5378933537570553,\n",
       "   0.4730592109194253,\n",
       "   0.4371300020061099,\n",
       "   0.402398833502416,\n",
       "   0.38783572256088844,\n",
       "   0.3799500028158011],\n",
       "  'train_acc': [18.276751709734224,\n",
       "   18.921058738414075,\n",
       "   32.796865828521504,\n",
       "   73.92194602563578,\n",
       "   82.7170099785686,\n",
       "   85.35976084196732,\n",
       "   86.6947868463082,\n",
       "   87.81959403196964,\n",
       "   88.44888543074381,\n",
       "   89.16417543715959],\n",
       "  'test_acc': [19.587430854333128,\n",
       "   19.587430854333128,\n",
       "   64.17486170866626,\n",
       "   80.57390903503381,\n",
       "   84.13875230485556,\n",
       "   86.06330669944684,\n",
       "   86.9660417947142,\n",
       "   88.28365089121081,\n",
       "   88.62553779963122,\n",
       "   88.86754763368162],\n",
       "  'layers': 3,\n",
       "  'lr': 0.01,\n",
       "  'batch': 64},\n",
       " 'L3_LR0.01_B128': {'train_loss': [2.252601911741843,\n",
       "   2.2383126876980297,\n",
       "   2.2375482185087057,\n",
       "   2.236562714363288,\n",
       "   2.233552321830907,\n",
       "   2.104402609268645,\n",
       "   1.2731301027181565,\n",
       "   0.8828550218569227,\n",
       "   0.7664268140451477,\n",
       "   0.6793064641993521],\n",
       "  'test_loss': [2.227229692192992,\n",
       "   2.2263897234630057,\n",
       "   2.2239568275986232,\n",
       "   2.223345459350136,\n",
       "   2.2138025844177736,\n",
       "   1.6814305968422354,\n",
       "   1.009952978261351,\n",
       "   0.833083894770059,\n",
       "   0.7391459833425323,\n",
       "   0.6692558827397277],\n",
       "  'train_acc': [17.82082258350738,\n",
       "   18.921058738414075,\n",
       "   18.921058738414075,\n",
       "   18.921058738414075,\n",
       "   18.921058738414075,\n",
       "   25.155275263797318,\n",
       "   59.921918724490496,\n",
       "   73.55201550705053,\n",
       "   77.18989311601622,\n",
       "   79.76302605894317],\n",
       "  'test_acc': [19.587430854333128,\n",
       "   19.587430854333128,\n",
       "   19.587430854333128,\n",
       "   19.587430854333128,\n",
       "   19.587430854333128,\n",
       "   42.924093423478794,\n",
       "   69.69499078057775,\n",
       "   76.02181929932391,\n",
       "   78.94130301167793,\n",
       "   80.82360172095882],\n",
       "  'layers': 3,\n",
       "  'lr': 0.01,\n",
       "  'batch': 128},\n",
       " 'L3_LR0.01_B256': {'train_loss': [2.2568326387498217,\n",
       "   2.2400342836468137,\n",
       "   2.238550377091919,\n",
       "   2.2375592036503518,\n",
       "   2.236290774409252,\n",
       "   2.233329945974122,\n",
       "   2.2237423057434564,\n",
       "   2.1463539785133126,\n",
       "   1.5823365846325874,\n",
       "   1.016262616909359],\n",
       "  'test_loss': [2.230180842640507,\n",
       "   2.226869644969244,\n",
       "   2.2247279365881867,\n",
       "   2.225393837774613,\n",
       "   2.221529962097712,\n",
       "   2.2178216144653207,\n",
       "   2.1974604005233305,\n",
       "   1.9810524678479402,\n",
       "   1.2425278865565679,\n",
       "   1.173905170176318],\n",
       "  'train_acc': [17.407210232469254,\n",
       "   18.921058738414075,\n",
       "   18.921058738414075,\n",
       "   18.921058738414075,\n",
       "   18.921058738414075,\n",
       "   18.921058738414075,\n",
       "   18.963375513602795,\n",
       "   23.26740106747478,\n",
       "   49.08472910438593,\n",
       "   69.74760091185826],\n",
       "  'test_acc': [19.587430854333128,\n",
       "   19.587430854333128,\n",
       "   19.587430854333128,\n",
       "   19.587430854333128,\n",
       "   19.587430854333128,\n",
       "   19.587430854333128,\n",
       "   19.65657652120467,\n",
       "   34.20021511985249,\n",
       "   62.05823601720959,\n",
       "   64.91241548862938],\n",
       "  'layers': 3,\n",
       "  'lr': 0.01,\n",
       "  'batch': 256}}"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 134
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "956394a36537d05f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def plot_accuracy(\n",
    "    RESULTS,\n",
    "    layers=None,       # z.B. [1,2,3]\n",
    "    lrs=None,          # z.B. [0.01]\n",
    "    batches=None,      # z.B. [32,64]\n",
    "    title=\"Accuracy Plot\"\n",
    "):\n",
    "    # Filterfunktion\n",
    "    def match(res):\n",
    "        if layers is not None and res[\"layers\"] not in layers:\n",
    "            return False\n",
    "        if lrs is not None and res[\"lr\"] not in lrs:\n",
    "            return False\n",
    "        if batches is not None and res[\"batch\"] not in batches:\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    # alle passenden Modelle filtern\n",
    "    selected = {name: res for name, res in RESULTS.items() if match(res)}\n",
    "\n",
    "    if len(selected) == 0:\n",
    "        print(\"⚠️ Keine Modelle entsprechen diesem Filter.\")\n",
    "        return\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy (%)\")\n",
    "\n",
    "    # zeichnen\n",
    "    for name, res in selected.items():\n",
    "        train_acc = res[\"train_acc\"]\n",
    "        test_acc = res[\"test_acc\"]\n",
    "\n",
    "        label_train = f\"{name} (train)\"\n",
    "        label_test = f\"{name} (test)\"\n",
    "\n",
    "        plt.plot(train_acc, \"--\", alpha=0.7, label=label_train)\n",
    "        plt.plot(test_acc, alpha=0.9, label=label_test)\n",
    "\n",
    "    plt.legend(fontsize=8, bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ],
   "id": "442c2e06170ab0c2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "914715ed474afc14"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
